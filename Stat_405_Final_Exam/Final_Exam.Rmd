---
title: "Untitled"
output: html_document
date: "2024-12-03"
---

# Module 13

## select() - selecting variables (columns)

```{r}
library(nycflights13)
library(tidyverse)
names(flights)
#you can subset variables like this
select(flights,year,month,day)

#you can selected ranges of variables (columns) without using the column number 
select(flights,year:day)


#and just do this for the compliment
select(flights,!(year:day))


```

selection functions for subsetting exactly what you want

```{r}

#starts_with(): Starts with a prefix.

#ends_with(): Ends with a suffix.

#contains(): Contains a literal string.

#matches(): Matches a regular expression.

#num_range(): Matches a numerical range like x01, x02, x03.

```

examples of how to use these selection functions

```{r}
select(flights, starts_with("dep"))

select(flights, ends_with("time"))

select(flights, contains("time"))

# this is or 
select(flights, starts_with("dep"), contains("time"), year:day)

# this is just the column numbers 
select(flights, c(1,3,5))

#select columns that are numbered but also have a prefix 
select(billboard, num_range("wk", 20:23))


```

## filter() - selecting observations of variables (rows)

```{r}
# use
# & and 
# | or 
# ! not 
```

```{r}
#filter flights departed on january 1st 
#all rows (observations) that satisfy the criteria
filter(flights, month == 1 & day == 1)

#flights that depart on November or December 
filter(flights, month == 11 | month == 12)

# all flights where the destination is IAH 
filter(flights, dest == "IAH")

# all flights with arrival delay less than or equal to 2 hours 
# or departure delay greater than 2 hours 

#only one condition needs to be satisfied to be included
filter(flights, arr_delay <= 120 | dep_delay > 120)
```

## mutate() - apply a transformation to a variable or make new one

```{r}
# from left to right future variables can use previous ones within the same statement 
mutate(flights, gain = arr_delay - dep_delay, hours = air_time/60, gain_per_hour = gain/hours)

```

### transmute()

```{r}
# same thing as filter but all variables other than the new ones just created will be removed
transmute(flights, gain = arr_delay - dep_delay, hours = air_time/60, gain_per_hour = gain/hours)

```

### variants of mutate() and transmute()

```{r}
#mutate_all() and transmute_all() affect every variable

#mutate_at() and transmute_at() affect variables selected with a character vector

#mutate_if() and transmute_if() affect variables selected with a predicate function (a function that returns TRUE or FALSE)
```

```{r}
head(iris,5)
#take the log() of all columns that include "Sepal" in their name and then assaign that to new tibble
mutated <- mutate_at(iris, vars(contains("Sepal")), log)

#you can specify to apply the function only to columns of specified data type 
mutated <- mutate_if(iris, is.numeric, log)
head(mutated)

#transmute would do the exact same thing just removal all other columns other than the ones that were transformed


#across() will apply a function to multiple different selection criteria 

```

## arrange() - reorder rows

```{r}
#reorder rows by numeric aescending value by default 
arrange(flights, year, month, day)

#or descending order
arrange(flights, desc(month))
```

## summarize() and group_by()

```{r}
#this will output a tibble with delay_mean which is the mean of the variable 
# dep_delay aswell output in the tibble delay_sd the standard dev of dep_delay
flights 

summarize(flights, delay_mean = mean(dep_delay, na.rm = TRUE), delay_sd = sd(dep_delay, na.rm = TRUE))
```

group_by() and summarize() together

```{r}
# will produce a group for each unique combination of the variables
by_day <- group_by(flights, year, month, day)

# takes that tibble of groups and evaluates the mean of variable dep_delay within each group 
summarize(by_day, delay = mean(dep_delay, na.rm = TRUE))

```

summarize functions

```{r}
# mean(), median(), sum(), min(), max(), sd(), var(), ... 
```

## pipe-line operator %\>%

```{r}
flights %>% 
  group_by(dest) %>% 
    summarize(count = n(), dist = mean(distance, na.rm = TRUE), delay = mean(arr_delay, na.rm = TRUE)) %>%
      filter(count > 20, dest != "HNL") %>% 
        ggplot(mapping = aes(x = dist, y = delay)) + 
          geom_point(aes(size = count), alpha = 1/3) + 
            geom_smooth(se = FALSE)
```

## more pipe-lines %\>%

```{r}
iris #remember you need to declare it as a tibble 
as_tibble(iris)

iris_grouped <- group_by(iris, Species) 
summarise_all(iris_grouped, mean)

#or you can just 

iris %>%
  group_by(Species) %>%
    summarise_all(mean)
    
#will output a summary of the observation groups where the summary is the mean of all columns within each group that start with Sepal in the variable name 
iris %>% 
  group_by(Species) %>%
    summarise(across(starts_with("Sepal"), mean))

# if you want the output from this for input to the next function in the pipeline to not be subject to previous grouping constraints for whatever reason. 
iris %>%
  group_by(Species) %>%
    summarise(across(starts_with("Sepal"), mean), .groups = "drop")


```

```{r,eval=false eval=FALSE, include=FALSE, r,eval=false}

library(candisc)

# convert data frame to tibble 
HSB <- as_tibble(HSB)
HSB

# summary of data types 
str(HSB)

# the function n() will give you the number of observations of each of the groups passed from the previous function in the pipe-line
HSB %>%
  group_by(race) %>%
    summarise(n=n())

# count() will tell you how many observations there are of each of the possible unique values within the variable
HSB %>% 
  count(race)

# how many observations of each unique combination of variable values 
HSB %>%
  count(race, gender)

```

example of filtering and then summarizing

```{r eval=FALSE, include=FALSE}
#means of all of these variables for all students
HSB %>% 
  summarise(readm = mean(read), writem = mean(write), mathm = mean(math), ssm = mean(ss))

# for each gender 
HSB %>% 
  group_by(gender) %>%
    summarise(readm = mean(read), writem = mean(write), mathm = mean(math), ssm = mean(ss))

# for each gender and race 
HSB %>% 
  group_by(gender,race) %>%
    summarise(readm = mean(read), writem = mean(write), mathm = mean(math), ssm = mean(ss))

# for each gender evaluate the mean of all numeric variables
HSB %>% 
  group_by(gender) %>%
    summarise(across(where(is.numeric), mean, na.rm = TRUE))


# the means of all numeric variables by ses for only asian and hispanic
HSB %>%
  filter(race == "hispanic" | race == "asian") %>%
    group_by(ses) %>%
      summarise(across(where(is.numeric), mean, na.rm = TRUE))

```

```{r}
library(dplyr)
data(starwars)
glimpse(starwars) # similar to str() in Base R

#a. How many humans are contained in starwars overall? (Hint. use count())
starwars %>%
  filter(species == "Human") %>%
    summarise(observations = n())

#b. How many feminine humans are contained in starwars?
starwars %>%
  filter(species == "Human" & gender == "feminine") %>%
    summarise(observations = n())


#c. From which homeworld do the most individuals come from?
starwars %>%
  group_by(homeworld) %>%
    summarise(observations = n()) %>%
      filter(observations == max(observations))

  
#d. What is the mean height of all individuals with orange eyes from the most popular homeworld?
starwars %>%
  filter(eye_color == "orange" & homeworld == "Naboo") %>%
    summarise(mean_height = mean(height))

```

## tibble joins

```{r}
library(tidyverse)
# tribble() creates a tibble
x <- tribble(
  ~key, ~val_x,
     1, "x1",
     2, "x2",
     3, "x3"
)

y <- tribble(
  ~key, ~val_y,
     1, "y1",
     2, "y2",
     4, "y3"
)
```

```{r}

  #  inner_join() keeps observations that appear in both data frames
  #  left_join() keeps all observations in x
  #  right_join() keeps all observations in y
  #  full_join() keeps all observations in x and y

```

inner join - retains all variables which correspond to matching keys of both sets

```{r}
inner_join_1<- x %>% 
  inner_join(y, by = "key")

inner_join_1

#equivalently 
inner_join_2 <- inner_join(x, y, by = "key")
```

left join - retains all variables which correspond to the keys of x

```{r}
left_join <- x %>% 
  left_join(y, by = "key")
left_join

#equivalently 
left_join(x, y, by = "key")
```

right join - retains all variables which correspond to the keys of y

```{r}
right_join <- x %>%
  right_join(y,by = "key")
right_join

#equivalently
right_join(x, y, by = "key")
```

full join - retains all variables which correspond to all keys present in both sets

```{r}
full_join <- x %>%
   full_join(y,by="key")
full_join

#equivalently
full_join(x, y, by = "key")
```

## tibble reshape and separate()

```{r}

 #   pivot_longer() lengthens data, increasing the number of rows and decreasing the #number of columns (i.e., turning columns into rows)
    
    
 #   pivot_wider() widens data, increasing the number of columns and decreasing the #number of rows (i.e., turning rows into columns)
    
    
 #   separate() separates a character column into multiple columns with a regular #expression or numeric locations
# as in a column containing ab-01 becomes two columns ab   01 
   
    
  #   unite() unites multiple columns into one by pasting strings together
      # the reverse of the previous 

```

```{r}
iris <- as_tibble(iris)

iris

# wide to long 
iris %>%
     pivot_longer(cols = -Species, names_to = "Measurements", values_to = "Values")


# seperate 
iris %>%
     pivot_longer(cols = -Species, names_to = "Measurements", values_to = "Values") %>%
          separate(col = Measurements, into = c("Part", "Measure")) #it registered the . as the seperator

# now that you've organized the data this way you can easily produce this complex plot 
iris %>%
  pivot_longer(cols = -Species, names_to = "Measures", values_to = "Values") %>%
  separate(col = Measures, into = c("Part", "Measure")) %>%
  ggplot(aes(x = Species, y = Values, color = Part)) + 
    geom_jitter() + 
    facet_grid(cols = vars(Measure)) + 
    theme_minimal()

```

wide to long

```{r}
table <- as_tibble(table4a)
table

long <- table %>%
  pivot_longer(cols=c("1999","2000"), names_to = "Year", values_to = "cases" )

long
```

```{r}
billboard

long <- billboard %>%
  pivot_longer(cols = wk1:wk76, names_to = "Week", values_to = "Rank", values_drop_na = TRUE)
long
```

long to wide

```{r}
table2 <- as_tibble(table2)

table2

#pivot_wider(data, names_from, values_from)

  #  names_from = A string specifying the name of the column to get the name of the output column
  #  values_from = A string specifying the name of the column to get the cell values from


wide <- table2 %>%
  pivot_wider(names_from = c("type"),values_from = "count")

wide
```

separate

```{r}
table3

#by default any non alphanumeric char will be registered as the seperator

table3 %>%
  separate(rate, into = c("cases","population"))

# or explicitly sepcify the seperator
table3 %>%  separate(rate, into = c("cases", "population"), sep = "/")

#convert from char to the data type it's best suited 
table3 %>%   
  separate(rate, into = c("cases", "population"), convert = TRUE)

# or you can specify by location, posative is left to right and sep = -2 is right to left 
table3 %>%   
  separate(year, into = c("century", "year"), sep = 2)

```

unite

```{r}
table5

table5 %>%
  unite(combined_column, century, year) # wil use _ by default 


table5 %>%
  unite(combined_column, century, year, sep = "") #specify none
```

## module 13 homework

```{r}
#setwd("C:/Users/jake pc/Desktop/Personal_save/Stat_405_Final_Exam")
setwd("/Users/jacobrichards/Desktop/Personal_save/Stat_405_Final_Exam")
bike <- read.csv(file="Bike_Lanes.csv",header=TRUE)
road <- read.csv(file="roads.csv",header=TRUE)
crash <- read.csv(file="crashes.csv",header=TRUE)
wide <- read.csv(file="Bike_Lanes_Wide.csv",header=TRUE)
```

Reshape wide using pivot_longer. Call this data long. Make the key lanetype, and the value\
the_length. Make sure we gather all columns but name, using -name. Note the NAs here

```{r}
wide

long <- wide %>%
    pivot_longer(cols = -name, names_to = "lanytype", values_to = "the_length",values_drop_na = TRUE)
long
```

Replace (using str_replace) any hyphens (-) with a space in crash\$Road. Call this data crash2.\
Table the Road variable.

```{r}
# reassaigns the Road variable with the output of str_replace
crash2 <- crash %>% mutate(Road = str_replace(Road, "-", " "))
table(crash2$Road)
```

Separate the Road column (using separate) into (type and number) in crash2.

Reassign this to\
crash2. Table crash2\$type. Then create a new variable calling it road_hyphen using the unite\
function. Unite the type and number columns using a hyphen (-) and then table road_hyphen.

```{r}
crash2 <- crash2 %>%
     separate(col=Road, into = c("type", "number"))

table(crash2$type)

crash2 <- crash2 %>%
          unite(road_hyphen, type, number, sep = "-")

table(crash2$road_hyphen)
```

Keep rows where the record is not missing type and not missing name and re-assign the output to bike.

```{r}
#in a tibble empty spaces you can index as (" ") 


bike <- bike %>%
          filter(name != " " & type != " ")

bike <- bike %>%
          filter(!is.na(name) & !is.na(type))

```

Summarize and group the data by grouping name and type (i.e, for each type within each name) and take the sum of the length (reassign the sum of the lengths to the length variable). Call this data set sub.

```{r}
sub <- bike %>%
  group_by(name,type) %>%
    summarise(length = sum(length)) #mutate cannot change dimensionality of the tibble, summarize will reduce 
                                    #the tibble to one row for each group 
sub
```

Reshape sub using pivot_wider. Spread the data where the key is type and we want the value in the new columns to be length - the bike lane length. Call this wide2. Look at the column names of wide2 - what are they? (they also have spaces).

```{r}
#pivot_wider(data, names_from, values_from)
#in her problems when she says "key" she means the variable, the thing we are measuring  
wide2 <- sub %>%
            pivot_wider(names_from = type, values_from = length)
```

Join data to retain only complete data, (using an inner_join) e.g., those observations with road lengths and districts. Merge without using by argument, then merge using by = "Road". call the output merged. How many observations are there?

```{r}
merged <- inner_join(road,crash)
merged
```

```{r}
full <- full_join(road,crash)
full
```

```{r}
left <- left_join(road,crash)
left
```

```{r}
right <- right_join(road,crash, by="Road") #in case there are more than 1 common variables but you want to use one as the key instead of the other. 
right
```

# Module 14

## t-tests, Wilcox - INDEPENDENT LONG DATA - PAIRED WIDE DATA

one sample - two sided

```{r}
data(trees)
t.test(trees$Height, mu = 70)
```

one sample - one sided

```{r}
t.test(trees$Height, mu = 70,alternative = "greater") # sample is greater than mu
t.test(trees$Height, mu = 70,alternative = "less")  # sample is less than mu 
```

change the confidence level on the mean confidence interval

```{r}
t.test(trees$Height, mu = 70,conf.level = 0.99)
```

whenever normality cannot be assumed use wilcox rank sum test

the function call is exactly the same as for t-tests

```{r}
wilcox.test(trees$Height, mu = 70)
wilcox.test(trees$Height, mu = 70, alternative = "greater") # sample is greater than mu
wilcox.test(trees$Height, mu = 70, alternative = "less")  # sample is less than mu 
```

## hypothesis test normality assumption verification

```{r}
qqnorm(trees$Height)
qqline(trees$Height, lty = 2)
```

this looks a little suspicious, lets check the Shapiro test

```{r}
shapiro.test(trees$Height)
```

fail to reject the null hypothesis that it's normal.

## two sample t-tests - unpaired - Variance test

H0: the two samples come from distributions with the same mean

Example:

For example, a study was conducted to test whether ‘seeding’ clouds with dimethylsulphate alters the moisture content of clouds. Ten random clouds were ‘seeded’ with a further ten ‘unseeded.’ The dataset can be found in the `atmosphere.txt` data file located in Canvas.

```{r}
atmos <- read.table(file="atmosphere.txt",header=TRUE)
atmos
t.test(atmos$moisture ~ atmos$treatment)
```

if you happen to be sure that the variances of the samples are equal

```{r}
t.test(atmos$moisture ~ atmos$treatment, var.equal = TRUE) #so in this context 
```

to verify assumption that variances are unequal - cannot use this with data that is not normal.

```{r}
var.test(atmos$moisture ~ atmos$treatment)
#H0: the varainces are equal, p-value = 0.4282 --> fail to reject null --> the variances are equal 
```

just the same you can use Wilcox for 2 sample tests for data you cant assume normality on

```{r}
wilcox.test(atmos$moisture ~ atmos$treatment)

```

## Paired t-tests

Paired data are where there are two measurements on the same experimental unit (individual, site, etc.) For example, the `pollution` dataset gives the biodiversity score of aquatic invertebrates collected using kick samples in 17 different rivers. These data are paired because two samples were taken on each river, one upstream of a paper mill and one downstream. Each individual observation pair originated from the same place.

```{r}
pollution <- read.table('pollution.txt', header = TRUE)
str(pollution)

t.test(pollution$down, pollution$up, paired = TRUE)

wilcox.test(pollution$down, pollution$up, paired = TRUE)
```

## Prop test and chisq test

### prop test

advertising campaign, counts of those would and would not buy are taken before and after

are the proportions significantly different?

|                               |                           |                       |
|:-----------------------------:|:-------------------------:|:---------------------:|
|                               | before = **no treatment** | after = **treatment** |
|   would buy = **+ outcome**   |            45             |          71           |
| would not buy = **- outcome** |            35             |          32           |

```{r}
buy <- c(45,71)                     # creates a vector of positive outcomes
total <-c((45 + 35), (71 + 32))   # creates a vector of total numbers
prop.test(buy, total)   
```

same thing but perform a chi-square test - you can also use the chisq.test function on raw un-tabulated data

```{r}
buyers <- matrix(c(45, 35, 71, 32), nrow = 2)
chisq.test(buyers)
```

14.2

## correlation coeficients and plots 

Pearson’s product-moment correlation coefficient between two continuous variables is the default correlation coefficient in the function

these values are only meaningful under the assumption of linear relationships

```{r}
cor(trees$Height, trees$Volume) #select specific variables 

cor(trees) #or produce a matrix of the coef's 

cor(trees, use = "complete.obs") #if theres is an NA 

setwd("~/Desktop/Personal_save/Stat_405_Module_14/Lab_14.2")
times <- read.csv(file="Game_Times.csv",header=TRUE)

# when you need to plot it 
pairs(times[,c(4,5,6,11)])
pairs(times[,c(7,8,9,11)])
pairs(times[,c(10,12,13,11)])
pairs(times[,c(14,11)])

cor(times[,4:14]) #or can just do this and then check that there is actually a linear relationship between the variables
```

to test if these correlation coefficients are statistically significant you need to use.

```{r}
cor.test(trees$Height, trees$Volume)
```

non parametric options

```{r}
cor.test(trees$Height, trees$Volume, method = "spearman")
cor.test(trees$Height, trees$Volume, method = "kendall")
```

## Linear Models

```{r eval=FALSE, include=FALSE}

Bivariate regression	    Y ~ X (continuous)	                lm(Y ~ X) #regular linear regression

One-way ANOVA	            Y ~ X (categorical)	                lm(Y ~ X) #difference between group means 

Two-way ANOVA	            Y ~ X1 (cat) + X2(cat)            	lm(Y ~ X1 + X2) 

ANCOVA	                  Y ~ X1 (cat) + X2(cont)           	lm(Y ~ X1 + X2)

Multiple regression     	Y ~ X1 (cont) + X2(cont)          	lm(Y ~ X1 + X2)

Factorial ANOVA	          Y ~ X1 (cat) * X2(cat)	            lm(Y ~ X1 * X2) or lm(Y ~ X1 + X2 + X1:X2)

```

## linear regression

produce model
```{r}
smoke <- read.table(file="smoking.txt",header=TRUE) 
smoke_lm <- lm(mortality ~ smoking,data=smoke)
summary(smoke_lm)
```

plot with data and correlation coefficient 
```{r}
slope_intercept <- coefficients(smoke_lm)
plot(mortality ~ smoking,data=smoke)
abline(a=slope_intercept[1],slope_intercept[2])
cor(y=smoke$mortality, x=smoke$smoking)
```

summary of model
```{r}
summary(smoke_lm) #check for significance of coeficient's 
anova(smoke_lm) #this is a condensed form of an anova table, signifigant p-value of F statistic - model is signifigant
```

### prediction and confidence interval

```{r}
predicted_smoke_120 <- predict(smoke_lm, newdata = data.frame(smoking = 120))
predicted_smoke_120

CI_predicted_smoke_120 <- predict(smoke_lm, newdata = data.frame(smoking = 120), interval = "confidence", level = 0.98)
CI_predicted_smoke_120
```

**Plot the model**

```{r}
ggplot(mapping = aes(x = smoking, y = mortality), data = smoke) + geom_point() + geom_smooth(method = "lm", se = TRUE)
```

**1.) residuals by the response values produced by the model (fitted values)**

```{r}
smoke_res <- resid(smoke_lm) 
smoke_fit <- fitted(smoke_lm) # this will extract the response values produced by the model

ggplot(mapping = aes(x = smoke_fit, y = smoke_res)) +
    geom_point() +
    geom_hline(yintercept = 0, colour = "red", linetype = "dashed")
```

**2.) residuals by known predictor values**

```{r}
ggplot(mapping = aes(x = smoke$smoking, y = smoke_res)) +
    geom_point() +
    geom_hline(yintercept = 0, colour = "red", linetype = "dashed")
```

No patterns, looks randomly and and evenly distributed. **check**

**3.) normality of residuals**

```{r}
qqnorm(smoke_res)
qqline(smoke_res) 
```

**4.) Studentized Residuals vs. Predictor observation number**

```{r}
plot(rstudent(smoke_lm) ~ seq(1,length(smoke$smoking),by=1))
```

more diagnostic plots

```{r}
par(mfrow = c(2,2))
plot(smoke_lm)
```

the first 2 are same as before

bottom left is first one but by a different scale

bottom right plots the points which have the greatest impact on the linear models coefficients

We may see that there is an extreme outlier point that weakens out model for the normal range of values that we would like to predict, it would benefit our model to just remove the point from the data points we produce the model from

```{r}
library(ggfortify)
autoplot(smoke_lm, which = 1:6, ncol = 2, label.size = 3)
```

in the bottom right plot, the y axis cooks distance is the influence the point has on the model, the leverage is how far the value is away the mean of the predictor

so this one is probably hurting the model because it's far outside the data center (less accurate) and has high impact

```{r}
smoke_lm2 <- update(smoke_lm, subset = -2)
summary(smoke_lm2)
```

## anova

linear model with categorical variable as predictor will compute anova test

```{r}
# this will evaluate an anova test 
smoke_anova_lm <- lm(mortality ~ risk.group, data=smoke)

# this will print the anova table of the test 
anova(smoke_anova_lm)
```

reject the null hypothesis that the group means are equal.

this will give us more information on the results of this anova test.

```{r}
summary(smoke_anova_lm)
```

```{r eval=FALSE, include=FALSE}
Coefficients:
  
  Coefficients:
                 Estimate Std. Error t value Pr(>|t|)    
(Intercept)        135.00       5.25  25.713  < 2e-16 ***
risk.grouplow      -57.83       8.02  -7.211 3.16e-07 ***
risk.groupmedium   -27.55       6.90  -3.992 0.000615 ***
  
  
  
  
  
                 Estimate Std. Error t value Pr(>|t|)    
(Intercept)        135.00       5.25  25.713  < 2e-16 *** #this is the first level of the factor, in this case risk.grouphigh (alpahebtical)
  # estimated mean mortality index for risk group high is 135,
  
   < 2e-16 *** #this is testing the null hypotheis that the mean mortality index of the high risk group is equal to zero. --> reject h0, not zero
  
  
risk.grouplow      -57.83       8.02  -7.211 3.16e-07 *** # these estimate values are the estimated mean differences of it and the high risk group (whaterver one gets assaigned to the intercept) 
  # as in risk.grouplow - risk.grouphigh = -57.83 ---> 135 - 57.83 = 77.17  
  
  3.16e-07 *** # this is testing  H0: abs(risk.grouplow - risk.grouphigh) = 0 , 2 sided --> reject h0: they are different
  
  
risk.groupmedium   -27.55       6.90  -3.992 0.000615 *** # risk.groupmedium - risk.grouphigh = -27.55 --> 135 - 27.55 = 107.45
  
  0.000615 *** # this is testing  H0: abs(risk.groupmedium - risk.grouphigh) = 0 , 2 sided --> reject h0: they are different
  

```

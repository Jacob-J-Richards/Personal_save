---
title: "Untitled"
output: html_document
date: "2024-12-03"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE)
```

# MODULE 2

## data inputting

```{r eval=FALSE}
DATA jansales;
INPUT @1 Item $10. @18 Amount comma6.; 
DATALINES;
trucks           1,382
jeeps            1,235
Landrovers       2,391
;
RUN;
```

has \@ for start position, does not need colon

```{r, eval=FALSE}
DATA january_sales; 
  INPUT Item : $12. Amount : COMMA5.; 
  DATALINES; 
    Trucks 1,382 
    Vans 1,235 
    Sedans 2,391 
    SportUtility 987 
; 
RUN;
```

does not have starting postion, and is of variable length, stop once 12 chars read or until blank reached. Same for second variable

```{r, eval=FALSE}
DATA wghtclub;
  INPUT IDno comma5. Name $ 6-24 Team $ Strtwght Endwght;
  Loss = Strtwght-Endwght; /#Create a variable called Loss by computing the difference of Strtwght and Endwght*/
  CARDS;
1,023 David Shaw         red 189 165
1,049 Amelia Serrano     yellow 145 124
1,219 Alan Nance         red 210 192
1,246 Ravi Sinha         yellow 194 177
1,078 Ashley McKnight    red 127 118
;
RUN;
```

of same length, does not need colon

## delimited input data

```{r, eval=FALSE}
DATA SCORES1;   
  LENGTH Team $ 14;   
  ***INFILE DATALINES DLM=',';***
INPUT ...
DATALINES;
      Joe,11,32,76,Red Racers
; 
```

## data that is comma delimeted and contained within quotes

```{r, eval=FALSE}
DATA HTWT;
INFILE DATALINES DSD;
INPUT ID
GENDER  $
AGE
HEIGHT
WEIGHT;
DATALINES;
1,"M", 23, 68.5, 155
2, F,, 61.2, 99
3, M, 55,70, 202
;
RUN;
PROC PRINT DATA=HTWT;
RUN;
```

if datalines are delimited by something

## complex data inputting

```{r, eval=FALSE}
data factory;
input @1 factory_number 2. state $ 6-7 ID $ 1-7 @8 quantity 2. @10 price dollar7.;

datalines;
13AB2NY44 $123
22XXXCT88 $1,033
37123TX11$22,999
;

proc print data=factory; 
title "Jacob Richards Factory Data";
```

```{r, eval=FALSE}
data scores; 
infile datalines missover;
input SSN $ 1-11 score_1-score_8 2. @37 score_9 score_10;


datalines;
123-45-6789 100 98 96 95 92 88 95 98100 90
344-56-7234 69 79 82 65 88 78 78 92 66 77
898-23-1234 80 82 86 92 78 88 84 85 83
;

proc print data=scores; 
title "Jacob Richards Scores";
VAR SSN Score_1-Score_8 score_9 score_10;  
RUN; 
```

the 2. after score_1-score_8 is only have the last input variable have the 2. stopping condition

then because of the missover it reads a blank for the missing score when the score 10 was formatted as list input

## missing values in data input

```{r, eval=FALSE}
  DATA NEW;
  INFILE DATALINES MISSOVER;
  INPUT ID INIT $ GENDER : $1. AGE VAULT P_BAR;
  DATALINES;
3 LJ . . .  6.7
5 MK . 15 8.1 7.2
7 FR F  
9 BV M 11
;
RUN;
```

will assign missing values to missing variables at the end of datelines such as this example it assigns missing to missing values in last 2 lines

## dates

unless specifies start and length, use colon before the date format

```{r, eval=FALSE}
data mice_contagion;
input number date_of_birth : date9. date_of_contraction : date9. date_of_death : date9. group $;
...
datalines;
1 23MAY1990 23JUN1990 28JUN1990 A
2 21MAY1990 27JUN1990 05JUL1990 A
;
```

the colons are required for date informants in list input because dates are inherently variable length data types.

05JUL1990 would read in the same as 5JUL1990

```{r, eval=FALSE}
DATA HOSPITAL;
   INPUT @1   ID            $3.     /*This is called formatted input */
         @4   DOB     MMDDYY10.
         @14  ADMIT   MMDDYY10.
         @24  DISCHRG MMDDYY10.
         @34  DX             1.
         @35  FEE            5.;
/#A column pointer (@)first tells the program which column to start reading, 
follow by the variable name and a specification of what type of data we are reading, 
called an informat. All of our formats and informats end with periods#/

   LENGTH_STAY = DISCHRG-ADMIT + 1;
   AGE = ADMIT - DOB;
DATALINES;
00110/21/194612/12/200412/14/20048 8000
00205/01/198007/08/200408/08/2004412000
00301/01/196001/01/200401/04/20043 9000
00406/23/199811/11/200412/25/2004715123
;
```

## Basic Programming

```{r, eval=FALSE}
DATA IQ_AND_TEST_SCORES;
	INPUT ID 1-3 IQ 4-6 MATH 7-9 SCIENCE 10-12;
	overall=((IQ + MATH + SCIENCE)/3)/500;
	if IQ ge 0 and IQ lt 100 then group='1';
	else if IQ ge 101 and IQ le 140 then group='2';
	else if IQ gt 140 then group='3';
	
	DATALINES;
001128550590
002102490501
003140670690
004115510510
;

proc sort data=IQ_AND_TEST_SCORES;
	by IQ;

proc freq data=iq_and_test_scores;
	tables group;

proc print data=iq_and_test_scores;
	title "Jacob Richards data set in IQ order";
	var IQ;
run;
```

basic programming

```{r, eval=FALSE}
data labor_expenses;
	input employeeID 1-3 @4 salary job_class;
	bonus_jobclass1_calculation=salary*.10;
	bonus_jobclass2_calculation=salary*.15;
	bonus_jobclass3_calculation=salary*.20;

	if job_class=1 then
		bonus=bonus_jobclass1_calculation;
	else if job_class=2 then
		bonus=bonus_jobclass2_calculation;
	else if job_class=3 then
		bonus=bonus_jobclass3_calculation;
	new_salary=salary + bonus;
	datalines;
137  28000  1
214  98000    3
199 150000  3
355   57000   2
;

proc print data=labor_expenses;
	title "Jacob Richards Labor Expenses";
	var employeeID salary job_class bonus new_salary;

```

# Module 3

## reading in txt and setting variables manually

```{r, eval=FALSE}
data adults;
infile '/home/u63989204/Midterm 1/Module 3/Module_3_labs/adults(1).txt' firstobs=2;
input weight bloodp; 
run;

proc print data=adults; 
run;
```

### csv read in, delimeter set, line of first obs set

```{r, eval=FALSE}
DATA HW4_2;
Infile '/home/u63989204/Module 4/Module_4_Homework/exchange rates.csv' DLM=',' FIRSTOBS=2;
Input Date : MMDDYY10. USD JPY EUR GBP AUD BRL CAD CNY DKK
HKD INR MYR MXN NOK NZD KRW SEK SGD LKR ZAR TWD THB;
Year=Year(Date);
run;

PROC Means data=HW4_2;
BY Year;
output out=AVG mean=; #prevents any other statistics from being calculated;
run;
```

## reading in data from spreadsheet

```{r, eval=FALSE}
proc import datafile = "/home/u63989204/Midterm 1/Module 3/Module_3_labs/Game_Times (1).xlsx" 
DBMS=XLSX  
out = imported_data_sheet replace;  
getnames=yes;  
options validvarname=v7;  
run;

proc sort data=imported_data_sheet; 
by descending Time_of_Game;


proc print data=imported_data_sheet (obs=1);  
format Date worddate.14;
var Date HomeTeam AwayTeam Time_of_Game; 
```

## reading in data from csv

```{r, eval=FALSE}
PROC IMPORT DATAFILE='/home/u63989204/Midterm 1/Module 4/Module_4_Homework/NBAPPG2008.csv'
 OUT=NBA
 DBMS=CSV REPLACE;
 getnames=yes;  
 options validvarname=v7;
run;
 
proc univariate data=NBA normaltest;
var G MIN PTS FGM FGA FGP FTM FTA FTP _3PM _3PA _3PP ORB DRB TRB AST STL BLK TO PF;
ods output TestsForNormality=normaltest;
run;
```

## programming lag (like diff() in R)

```{r, eval=FALSE}

DATA PATIENTS;
   INPUT @1  ID          $3.
         @4  DATE   MMDDYY8.
         @12 HR           3.
         @15 SBP          3.
         @18 DBP          3.
         @21 DX           3.
         @24 DOCFEE       4.
         @28 LABFEE       4.;
   FORMAT DATE MMDDYY10.;
DATALINES;
0071021198307012008001400400150
0071201198307213009002000500200
0090903198306611007013700300000
0050705198307414008201300900000
0050115198208018009601402001500
0050618198207017008401400800400
0050703198306414008401400800200
;

PROC SORT DATA=PATIENTS;
   BY ID DATE;   #Sort the data set in patient-date order
RUN;

DATA DIFFERENCE;
   SET PATIENTS;
   BY ID;
   DIFF_HR = HR - LAG(HR); 
   DIFF_SBP = SBP - LAG(SBP);
   DIFF_DBP = DBP - LAG(DBP);

   IF NOT FIRST.ID THEN OUTPUT; 
RUN;

PROC PRINT DATA = DIFFERENCE; 
RUN;
```

## more lag() function usage

```{r, eval=FALSE}
DATA CLINICAL;
LENGTH PATIENT VISIT DATE_VISIT 8;
RETAIN DATE_VISIT WEIGHT;
DO PATIENT = 1 TO 25;
IF RANUNI(135) LT .5 THEN GENDER = 'Female';
ELSE GENDER = 'Male';
X = RANUNI(135);
IF X LT .33 THEN GROUP = 'A';
ELSE IF X LT .66 THEN GROUP = 'B';
ELSE GROUP = 'C';
DO VISIT = 1 TO INT(RANUNI(135)*5);
IF VISIT = 1 THEN DO;
DATE_VISIT = INT(RANUNI(135)*100) + 15800;
WEIGHT = INT(RANNOR(135)*10 + 150);
END;
ELSE DO;
DATE_VISIT = DATE_VISIT + VISIT*(10 + INT(RANUNI(135)*50));
WEIGHT = WEIGHT + INT(RANNOR(135)*10);
END;
OUTPUT;
IF RANUNI(135) LT .2 THEN LEAVE;
END;
END;
DROP X;
FORMAT DATE_VISIT DATE9.;
RUN;



*^ given code from lab above; 
data difference_by_visit;
	set clinical;
	by patient;
	diference_weight_from_previous = weight - lag(weight);   
*current observed weight minus previous observed weight for same patient;
*just as diff in R;
	
	if not first.patient then output;
*specifies to only compute this when patient observation n>1 is read;
run;

proc print data = difference_by_visit;
```

## programming with dates and formating date input and output

```{r, eval=FALSE}
data people;
input ID @5 DOB MMDDYY8. @13 start_date MMDDYY8. @21 end_date MMDDYY8. @29 sales 5.;

Age_work_started_years = (abs(DOB) + start_date)/365;

Length_at_work_years = (end_date - start_date)/365;

sales_per_year_of_work = sales/Length_at_work_years;

sales_per_year_rounded = round(sales_per_year_of_work, 10);

datalines; 
001 1021194611121980122819887343
002 0913195502021980020419880123
005 0606194003121981031220040000
003 0705194411151980111320009544
; 
run;

proc print data=people;
title "Jacob Richards data people";
format DOB MMDDYY10.; 
format Age_work_started_years 8.; #this just rounds off the decimal places;
format sales_per_year_rounded dollar6.;
var ID DOB Age_work_started_years Length_at_work_years sales_per_year_rounded;
run;
```

if you want to hardcode a date you do this

```{r, eval=FALSE}
age_actual = ('15JAN2005'D - DOB)/365.25;
age_today = ('13SEP2024'D - DOB)/365.25;
```

where dob is an input argument

## date options

```{r,eval=FALSE}
3.2.7 has full list of functions dealing with dates such as 
AGE = ROUND(YRDIF(DOB,ADMIT, ‘ACTUAL’),.1); 

```

if you want to round or find weekday, month, or year of a date

```{r, eval=FALSE}
age = (abs(DOB) + ADMIT)/365.25;
age = round(age,1);

day = weekday(ADMIT);
month = month(ADMIT);
```

## more dates input, output, programming

```{r, eval=FALSE}
data mice_contagion;
input number date_of_birth : date9. date_of_contraction : date9. date_of_death : date9. group $;
birth_to_disease_time = date_of_contraction - date_of_birth;
disease_to_death_time = date_of_death - date_of_contraction;
age_at_death = date_of_death - date_of_birth;

*the colon is needed for date informats in a list because the length of a date 
is inherently variable;


datalines;
1 23MAY1990 23JUN1990 28JUN1990 A
2 21MAY1990 27JUN1990 05JUL1990 A
3 23MAY1990 25JUN1990 01JUL1990 A
4 27MAY1990 07JUL1990 15JUL1990 A
5 22MAY1990 29JUN1990 22JUL1990 B
6 26MAY1990 03JUL1990 03AUG1990 B
7 24MAY1990 01JUL1990 29JUL1990 B
8 29MAY1990 15JUL1990 18AUG1990 B
;


proc means data = mice_contagion mean std stderr;    

title "mean, standard deviation, and standard error for birth to disease, disease to death, and age time in days for each group";
class group;
*compute these statistics for objects of each group seperatly;
var birth_to_disease_time   disease_to_death_time   age_at_death;
*compute these statistics of each of these variables of each group;

proc print data=mice_contagion;
var birth_to_disease_time   disease_to_death_time   age_at_death;
```

## reapeating data input patterns

```{r, eval=FALSE}
DATA BLOOD;
LENGTH GROUP $ 1;
INPUT ID GROUP $ TIME WBC RBC @@;
DATALINES;
1 A 1 8000 4.5 1 A 2 8200 4.8 1 A 3 8400 5.2
1 A 4 8300 5.3 1 A 5 8400 5.5
2 A 1 7800 4.9 2 A 2 7900 5.0
3 B 1 8200 5.4 3 B 2 8300 5.4 3 B 3 8300 5.2
3 B 4 8200 4.9 3 B 5 8300 5.0
4 B 1 8600 5.5
5 A 1 7900 5.2 5 A 2 8000 5.2 5 A 3 8200 5.4
5 A 4 8400 5.5
;
```

the \@\@ has the data input repeat by these 5 variables regardless of line location, simply left to right list style

# Module 4

## means table, evaluating mean of variable by numerical value category

```{r, eval=FALSE}
DATA HW4_2;
Infile '/home/u63989204/Midterm 1/Module 4/Module_4_Homework/exchange rates.csv' DLM=',' FIRSTOBS=2;
Input Date : MMDDYY10. USD JPY EUR GBP AUD BRL CAD CNY DKK
HKD INR MYR MXN NOK NZD KRW SEK SGD LKR ZAR TWD
THB;
Year=Year(Date);
run;

PROC Means data=HW4_2;
BY Year;
output out=AVG mean=;
run;

proc print data=AVG;
var year USD JPY EUR GBP AUD BRL CAD CNY DKK HKD INR
MYR MXN NOK NZD KRW SEK SGD LKR ZAR TWD THB;
run;
```

## normality test, inputted data

```{r, eval=FALSE}
data test_one;
input score;
datalines;
82
91
100
68
87
73
78
80
65
84
116
76
97
100
105
77
;

proc univariate data=test_one normal;
var score;
run;

##for all 4 normality tests, the p-value is above 0.05 and we therefore fail
#to reject the null hypothesis that the data set is normally distributed;
```

Null hypothesis: variable is normally distributed —-\> (p-value \< 0.05) —\> reject null hypothesis —\> variable is not normally distributed.

## Normality test - csv read in

```{r, eval=FALSE}
PROC IMPORT DATAFILE='/home/u63989204/Module 4/Module_4_Homework/NBAPPG2008.csv'
 OUT=NBA
 DBMS=CSV REPLACE;
 getnames=yes;  
 options validvarname=v7;
run;

#Test for normality;
proc univariate data=NBA normaltest;
var G MIN PTS FGM FGA FGP FTM FTA FTP _3PM _3PA _3PP ORB DRB TRB AST STL BLK TO PF;
ods output TestsForNormality=normaltest;
run;

#create 0/1 variable for rejection of hypothesis;
data normaltest;
set normaltest;
reject=(pValue<0.05);
run;

proc print data = normaltest;
where reject=0;
run;
# the table produced is all of the normality tests which fail to reject null, so all variables that have all 4 tests fail to reject null are normal variables, the variables with 2 or 3 tests present in this resulting table we identified as having tests which rejected the null 
```

scroll all the way down to the table of the variable table, reject = 0 (fail to reject) —-\> normally distributed

Null hypothesis: variable is normally distributed —\> (p-value \< 0.05) —\> reject null hypothesis —\> variable is not normally distributed.

```{r, eval=FALSE}
proc univariate data=NBA Normal;
var _3PM _3PA DRB;
run;
```

these variables have at least one test with a p-value less than 0.05 —\> reject null hypothesis —\> variables are not normally distributed

## conifdence interval for mean of computed values

```{r, eval=FALSE}
data clinic; 
input id 1-3 gender $ 4-4 race $ 5-5 HR 6-8 SBP 9-11 DBP 12-14 N_PROC 15-16;

AVE_BP = DBP + 1/3*(abs(SBP - DBP));

datalines;
001MW08013008010
002FW08811007205
003MB05018810002
004FB   10806801
005MW06812208204
006FB101   07404
007FW07810406603
008MW04811207006
009FB07719011009
010FB06616410610
;

*when you copy and paste it omits more than 1 space which is what i assumed happened 
beetween the textbook and the data to copy in the lab instructions 
because in the textbook the data is clearly aligned.; 

proc means data=clinic n clm mean std median alpha=.05;
var SBP DBP AVE_BP;
run;

*includes confidence limits for the mean;
```

## ALL DESCRIPTIVE STATISTICS - proc univarate - proc freq - proc means

```{r, eval=FALSE}
*SAS Tutorial 4.1; 

*First, write a code to read in the Crews.csv dataset;

PROC IMPORT DATAFILE='/home/u63989204/Midterm 1/Module 4/tutorials/Crews.csv'
            OUT=crew
            DBMS=CSV
            REPLACE;
run;

proc print data = crew;
run;


PROC MEANS DATA = crew MAXDEC=1; /*PROC MEANS Statement Computes descriptive statistics for variables.*/
  VAR Salary; /*Identifies Salary as the analysis variable*/
  CLASS JobCode; /*JobCode defines the subgroup combinations for the analysis */
  TITLE "Salary by Job Code"; /* create title in the output*/
RUN;

PROC MEANS DATA = crew /*Mean precedure specifies the input data set.*/
  MIN Q1 MEDIAN Q3 MAX /* Five-Number Summary */
  QRANGE               /* Interquartile Range */
  CLM                  /* 95% Confidence Interval */
  T PRT                /* T-Test of Mean Salary=0 */
  MAXDEC=0;            /*specifies the number of decimal places for printed statistics */
  VAR Salary;
RUN;


DM 'clear log; clear output';  /*Clear log and output*/

DATA student;   /*Define a data set call Student*/
   INFILE DATALINES; /*specifies that the input data immediately follows the DATALINES statement in the DATA step*/
   INPUT gender $ age marstat $ credits state $; /*List input 5 variables:*/ 
   			    /* gender, marstat and state are character variables*/
                           /* age and credits are numeric variables */ 
DATALINES;										   
F 23 S 13 MN
F 21 S 10 WI
F 22 S 09 MN
F 35 M 08 MN
F 22 M 11 MN
F 25 S 12 WI
M 20 S 13 MN
M 26 M 11 WI
M 27 S 9 MN
M 23 S 14 IA
M 21 S 12 MN
M 29 M 10 MN
;
RUN;
TITLE 'Student Data';

PROC PRINT DATA=student;              /*Print the data set called student*/
 VAR gender age marstat credits state ; /*Display the variables in the order  from left to right*/
RUN;

PROC PRINT DATA=student;
RUN;

PROC FREQ DATA = student; /*Use to compute frequencies*/
  TABLES gender / NOCUM NOPERCENT; /*Provide a table of frequencies for the variable GENDER omit both cumulative statistics and percentages.*/
RUN;

PROC UNIVARIATE DATA =student; /*PROC UNIVARIATE provides more descriptive statistics such as mean, stdv, variance, min, max...*/
  VAR credits; /*Identifies Credits as the analysis variable*/
  BY gender; /*You can specify a BY statement with PROC UNIVARIATE to obtain separate analyses for each BY group. */
RUN;

PROC UNIVARIATE DATA = student MU0=20 12; /*The following statement tests the hypothesis MU=20 for the first variable and the hypothesis MU=12 for the second variable. */
  VAR age credits;  /*specifies the analysis variables and their order in the results.*/
RUN; 

PROC UNIVARIATE DATA = student CIBASIC(ALPHA=0.1)  ; /*Requests confidence limits for the mean, standard deviation, and variance. The option ALPHA= specifies the level of significance for confidence intervals*/
  VAR CREDITS;
RUN;

PROC UNIVARIATE DATA = student
    NEXTROBS=6; /*specifies the number of extreme observations that PROC UNIVARIATE lists in the table of extreme observations.*/
  VAR credits;
  ID gender state; /*The ID statement specifies one or more variables to include in the table of extreme observations */
RUN;


```

### proc means - confidence interval for means - standard dev and median

```{r,eval=FALSE}
proc means data=clinic n clm mean std median alpha=.05;
var SBP DBP AVE_BP;
run;
```

the confidence interval is set on the mean by clm set before mean in the statement, followed by alpha being set.

will compute for each of these variables

### proc means by category

```{r,eval=FALSE}
PROC MEANS DATA = crew MAXDEC=1; # MAXDEC=1; WILL ONLY OUTPUT TO 1 DECIMAL PLACE
  VAR Salary; /#Identifies Salary as the analysis variable#/
  CLASS JobCode; /#JobCode defines the subgroup combinations for the analysis #/
  TITLE "Salary by Job Code"; /# create title in the output#/
RUN;
```

### proc means 5 number summary

```{r,eval=FALSE}
PROC MEANS DATA = crew /#Mean precedure specifies the input data set.#/
  MIN Q1 MEDIAN Q3 MAX /# Five-Number Summary #/
  QRANGE               /# Interquartile Range #/
  CLM                  /# 95% Confidence Interval #/
  T PRT                /# T-Test of Mean Salary=0 #/
  MAXDEC=0;            /#specifies the number of decimal places for printed statistics #/
  VAR Salary;
RUN;
```

### proc univarate for variable by stratification

```{r,eval=FALSE}
PROC UNIVARIATE DATA =student; /#PROC UNIVARIATE provides more descriptive statistics such as mean, stdv, variance, min, max...#/
  VAR credits; /#Identifies Credits as the analysis variable#/
  BY gender; /#You can specify a BY statement with PROC UNIVARIATE to obtain separate analyses for each BY group. #/
RUN;
```

### proc univarate mean hypothesis test, (one sample t-test)

```{r,eval=FALSE}
PROC UNIVARIATE DATA = student MU0=20 12; /#The following statement tests the hypothesis MU=20 for the first variable and the hypothesis MU=12 for the second variable. #/
  VAR age credits;  /#specifies the analysis variables and their order in the results.#/
RUN; 
```

### proc univarate confidence intervals for mean, standard dev, and variance

```{r,eval=FALSE}
PROC UNIVARIATE DATA = student CIBASIC(ALPHA=0.1)  ; /#Requests confidence limits for the mean, standard deviation, and variance. The option ALPHA= specifies the level of significance for confidence intervals#/
  VAR CREDITS;
RUN;
```

### proc univarate finds extreme observations

```{r,eval=FALSE}
PROC UNIVARIATE DATA = student
    NEXTROBS=6; /#specifies the number of extreme observations that PROC UNIVARIATE lists in the table of extreme observations.#/
  VAR credits;
  ID gender state; /#The ID statement specifies one or more variables to include in the table of extreme observations #/
RUN;
```

### simple frequency table

```{r,eval=FALSE}
PROC FREQ DATA = student; /*Use to compute frequencies*/
  TABLES gender / NOCUM NOPERCENT; /*Provide a table of frequencies for the variable GENDER omit both cumulative statistics and percentages.*/
RUN;
```

# Module 5 - SAS

## odds ratio

**Case vs Control studies - select individuals with +/- outcomes then look back for effect of risk factors**

odds ratio test for independence of variables just like the chi-square specifically for contingency tables.

Question: Is there evidence that the results of the entrance exam are related to race? Analysis: Perform a statistical test of association between row and column variables. Null Hypothesis: Exam results and race of examinee are independent. Alternative Hypothesis: Exam results and race of examinee are not independent.

we're testing the same null hypothesis as a chi-square test, that the variables are independent.

the first row and the first column are the exposure and outcome categories we are testing for association.

```{r,eval=FALSE}
data entrance_exam;
input race $ RESULTS $ count;
datalines;
2-white PASS  5
2-white FAIL 3
1-other PASS 4
1-other FAIL 6
;

PROC FREQ DATA=entrance_exam;
     TITLE "Mantel-Haenszel Chi-square Test";
     TABLES race*RESULTS/all; /#ALL option use with the TABLES statement 
#requests tests and measures of association produced by CHISQ, MEASURES, and CMH options#/
  WEIGHT count;
RUN;

```

1.) check Chi-square for overall association between predictor and response

+--------------------------------------------------------+---------+-----------+----------+
| **Statistic**                                          | **DF**  | **Value** | **Prob** |
+:=======================================================+========:+==========:+=========:+
| **WARNING: 50% of the cells have expected counts less\ |         |           |          |
| than 5. Chi-Square may not be a valid test.**          |         |           |          |
+--------------------------------------------------------+---------+-----------+----------+
| **Chi-Square**                                         | 1       | 0.9000    | 0.3428   |
+--------------------------------------------------------+---------+-----------+----------+

p-value = 0.34 —\> fail to reject null that the variables are independent

2.) check odds ratio and confidence interval for odds ratio

```{r,eval=FALSE}

Odds Ratio and Relative Risks
Statistic               Value       95% Confidence Limits
Odds Ratio              2.5000      0.3701    16.8884
Relative Risk (Col 1)   1.6000      0.5725    4.4719
Relative Risk (Col 2)   0.6400      0.2526    1.6216

```

odds ratio contains 1 within it's upper and lower bounds —\> odds ratio is not greater than 1 with 95% confidence —\> fail to reject null hypothesis that the variables are independent.

## odds ratio with set confidence level

(Use SAS) A random sample of 90 adults is classified according to gender and the number of hours of television watched during a week: Use a 0.01 level of significance and test the hypothesis that the time spent watching television is independent of whether the viewer is male or female.

same thing here it's asking if the categorical variables are independent.

```{r,eval=FALSE}
data tv;
input gender $ GT_or_LT $ count; 
datalines; 
2-M 1-More 15   # we think that it will be higher for females so we put a 1 in front of the variable so that it will print in the first
2-M 2-Less 27   # row of the contigency table and align with the first column 1-More
1-F 1-More 29
1-F 2-Less 19
;

PROC FREQ DATA=tv; 
    TITLE "Jacob Richards - Mantel-Haenszel Chi-square Test with Odds Ratio"; 
    TABLES gender*GT_or_LT / CHISQ CMH OR CL ALPHA=0.01;  
    WEIGHT count;
RUN;
```

| **Statistic**  | **DF** | **Value** | **Prob** |
|:---------------|-------:|----------:|---------:|
| **Chi-Square** |      1 |    5.4702 |   0.0193 |

fail to reject the null hypothesis that the variables are independent

```{r,eval=FALSE}
Odds Ratio and Relative Risks
Statistic               Value       99% Confidence Limits
Odds Ratio              2.7474      0.8918    8.4641
Relative Risk (Col 1)   1.6917      0.9171    3.1206
Relative Risk (Col 2)   0.6157      0.3565    1.0636

```

odds ratio confidence interval at alpha = 0.01 contains 1 —\> insufficient evidence to conclude that odds ratio does not equal 1 —\> fail to reject null that variables are independent.

## Risk Ratio

**Cohort studies - select individuals with +/- risk factors then follow through time to see if outcome occurs**

*A randomized clinical trial compared aspirin to placebo for the prevention of heart attacks (Mis) and strokes. Out of a total of 1,000 subjects on aspirin, there were 80 heart attacks and 65 strokes;* out of a total of 2,000 subjects on placebo, there were 240 heart attacks and 165 strokes. is there a significant benefit for aspirin therapy for heart attacks and strokes? What is the RR for aspirin use for each of these two outcomes? (Careful here, you are given the total number of subjects in each group and the number of complications.);

rather than analyse the relative risk of having a heart complications or not analyse the risk ratio of treatment (yes or no) to having a heart attack (Y/N) and risk ratio of treatment (yes or no) to having a stroke (Y/N)

HEART ATTACK RR EVALUATION

```{r,eval=FALSE}
DATA heart_attacks;
	INPUT treatment $ heart_attack $ COUNT;
	DATALINES;
asprin YES 80 
asprin NO 920
placebo YES 240
placebo NO 1760
;

PROC FREQ DATA=heart_attacks;
	TITLE "realtive risk: treatment on heart attacks";
	TABLES treatment*heart_attack /CMH;
	WEIGHT COUNT;
RUN;
```

HEART ATTACKS - results

```{r,eval=FALSE}
Common Odds Ratio and Relative Risks
Statistic               Method            Value       95% Confidence Limits
Odds Ratio              Mantel-Haenszel   1.5682      1.2028    2.0446
                        Logit             1.5682      1.2028    2.0446
Relative Risk (Col 1)   Mantel-Haenszel   1.0455      1.0202    1.0713
                        Logit             1.0455      1.0202    1.0713
Relative Risk (Col 2)   Mantel-Haenszel   0.6667      0.5237    0.8487
                        Logit             0.6667      0.5237    0.8487


```

Column 1 given you are in row 1 (Asprin) your risk of not having a heart attack is greater than 1 for both upper and lower bounds

Column 2 given you are in row 1 (Asprin) your risk of having a heart attack is less than 1 for both upper and lower bounds

Therefore, there is a signifigant benefit of asprin thearapy for heart attacks

STROKES – RESULTS

```{r,eval=FALSE}
Common Odds Ratio and Relative Risks
Statistic               Method            Value       95% Confidence Limits
Odds Ratio              Mantel-Haenszel   1.2934      0.9605    1.7418
                        Logit             1.2934      0.9605    1.7418
Relative Risk (Col 1)   Mantel-Haenszel   1.0191      0.9979    1.0407
                        Logit             1.0191      0.9979    1.0407
Relative Risk (Col 2)   Mantel-Haenszel   0.7879      0.5974    1.0391
                        Logit             0.7879      0.5974    1.0391

```

Column 1 given you are in row 1 (Asprin) your risk of not having a stroke is not greater than 1 for the lower bound

Column 2 given you are in row 1 (Asprin) your risk of having a heart attack is not less than 1 for the upper bound

Therefore, there is a not significant benefit of asprin thearapy for strokes

## stratified odds ratio

The relationship between office temperature and head colds is tested for smokers and nonsmokers. Since smoking is assumed to be a confounding factor in this relationship, use a Mantel-Haenszel chi-square for stratified tables to analyze the two tables here:

```{r,eval=FALSE}
Smokers
          Colds   No Colds
Poor      20      100
Good      15      150

Nonsmokers
          Colds   No Colds
Poor      30      100
Good      25      200

```

```{r,eval=FALSE}
data colds;
	input smoker $ cold $ Tempature $ count;
	datalines;
yes 1-yes 1-poor 20
yes 1-yes 2-good 15
yes 2-no 1-poor 100
yes 2-no 2-good 150 
no 1-yes 1-poor 30 
no 1-yes 2-good 25
no 2-no 1-poor 100
no 2-no 2-good 200
;

PROC FREQ DATA=colds;
	TITLE "Jacob Richards - Mantel-Haenszel Chi-square Test";
	TABLES smoker*tempature*cold/ALL;
	WEIGHT COUNT;
RUN;
```

first check if the odds ratio for the stratification categories are significantly different

```{r,eval=FALSE}

Breslow Day Test for Homogeneity of Odds Ratios
Statistic: Value
Chi-Square: 0.1501
Degrees of Freedom (DF): 1
P-value (Pr > ChiSq): 0.6985

```

fail to reject the null hypothesis that the odds ratios for the categories are not different

so we can analyse the table all together

test if the odds ratios for both categories are equal to 1

```{r,eval=FALSE}
Cochran-Mantel-Haenszel Statistics (Based on Table Scores)
Statistic                         Alternative Hypothesis         DF    Value     Prob
1                                 Nonzero Correlation            1     12.4770   0.0004
2                                 Row Mean Scores Differ         1     12.4770   0.0004
3                                 General Association            1     12.4770   0.0004

```

reject null —-\> the odds ratio for both categories is not 1.

check the value of the common odds ratio

```{r,eval=FALSE}
Common Odds Ratio and Relative Risks
Statistic               Method            Value       95% Confidence Limits
Odds Ratio              Mantel-Haenszel   2.2289      1.4185    3.5024
                        Logit             2.2318      1.4205    3.5064
Relative Risk (Col 1)   Mantel-Haenszel   1.9775      1.3474    2.9021
                        Logit             1.9822      1.3508    2.9087
Relative Risk (Col 2)   Mantel-Haenszel   0.8891      0.8283    0.9544
                        Logit             0.8936      0.8334    0.9582
```

upper and lower bounds are greater than 1 —\> with 95% the risk ratio is greater than 1 -\> there is association between poor temp and cold

## Stratified odds ratio example (gender)

```{r, eval=FALSE}
DATA ABILITY;
      INPUT GENDER $ RESULTS $ SLEEP $ COUNT;
DATALINES;
BOYS FAIL 1-LOW 20
BOYS FAIL 2-HIGH 15
BOYS PASS 1-LOW 100
BOYS PASS 2-HIGH 150
GIRLS FAIL 1-LOW 30
GIRLS FAIL 2-HIGH 25
GIRLS PASS 1-LOW 100
GIRLS PASS 2-HIGH 200
;
PROC FREQ DATA=ABILITY;
      TITLE "Mantel-Haenszel Chi-square Test";
      TABLES GENDER*SLEEP*RESULTS/ALL; /*ALL option use with the TABLES statement requests tests and measures of association produced by CHISQ, MEASURES, and CMH options*/
   WEIGHT COUNT;
RUN;
```

Breslow-Day Test for Homogeneity of Odds Ratios

Are the odds ratios of the stratification categories significantly different

```{r,eval=FALSE}
Breslow-Day Test for
Homogeneity of Odds Ratios
Chi-Square	0.1501
DF	1
Pr > ChiSq	0.6985
```

fail to reject the null hypothesis that the odds ratios for boys and girls are not significantly different.

Cochran-Mantel-Haenszel Statistics

Are the odds ratios of the stratification categories equal to 1?

```{r,eval=FALSE}
Cochran-Mantel-Haenszel Statistics (Based on Table Scores)
Statistic	Alternative Hypothesis	DF	Value	  Prob
1	Nonzero Correlation           	1  12.4770	0.0004
```

reject null hypothesis that the odds ratio across the stratification categories are equal to 1. The variables are associated.

What is the common odds ratio and it's confidence interval

```{r,eval=FALSE}
Common Odds Ratio and Relative Risks
Statistic	      Method	    Value	    95% Confidence Limits
Odds Ratio	Mantel-Haenszel	2.2289    	1.4185	3.5024
```

**Conclusion:**

-   The odds ratio is 2.22 and its confidence interval is 1.4185, 3.5024 We can say that we are 95% confident that the true population odds ratio is in the interval 1.4185 to 3.5024.

-   We see that the lower and upper limits are above 1; thus, we can say that the odds of having exposure (low sleep) for the case group (failed test) are statistically significantly greater than the odds for the control group (passed test).

## Chi-Square given counts

Can we conclude from these data at the 0.01 level of significance that the occurrence of these types of crime is dependent on the city district?

As this is clearly not a 2x2 table --\> chi-square no odds ratio

```{r,eval=FALSE}
data crimes; 
input district $ crime $ count; 
datalines; 
1 1-assault 162
1 2-Burglary 118
1 3-Larceny 451
1 4-Homicide 18
2 1-assault 310
2 2-Burglary 196
2 3-Larceny 996
2 4-Homicide 25
3 1-assault 258
3 2-Burglary 193
3 3-Larceny 458
3 4-Homicide 10
4 1-assault 280
4 2-Burglary 175
4 3-Larceny 390
4 4-Homicide 19
;

PROC FREQ DATA=crimes;
      TITLE "Jacob Richards -  Chi-square Test";
      TABLES district*crime/all; 
   WEIGHT COUNT;
RUN;
```

| **Statistic**  | **DF** | **Value** | **Prob** |
|:--------------:|:------:|:---------:|:--------:|
| **Chi-Square** |   9    | 124.5297  | \<.0001  |

reject null hypothesis that the variables are independent.

## Chi square given raw data

Ex:

```{r,eval=FALSE}
4 categories of predictor, 3 categories of response --> chi square test 
data structure:
  col1               col2
predictor_1       responce_2
predictor_2       responce_1
....


```

if given the raw data, use proc freq to find the frequencies of the variable category occurrences and run the chi square from that

```{r,eval=FALSE}

proc import datafile="/home/u63989204/Midterm 1/Module 5/Module_5_Homework/Homework 5 data.xlsx"
dbms=xlsx 
out = hw_five replace;
getnames=yes;
option validvarname=v7;
run;

proc freq data=hw_five;
    tables GL*opinion/all;
run;
```

or just convert it to cvs

```{r, eval=FALSE}
DATA CLIMATE;
INFILE 'your file location/Homework 5 data.csv' DLM = ',' FIRSTOBS=2;
INPUT GL $ OPINION $;
RUN;

PROC FREQ DATA=CLIMATE;
TITLE "Relationship between grade level and opinion";
TABLES GL*OPINION/CHISQ;
RUN;
```

## chi square for trend

```{r,eval=FALSE}
DATA TREND;
INPUT Alcohol $ Mal $ COUNT;
DATALINES;
0 N 17066
1 N 14464
2 N 788
3 N 126
4 N 37
0 Y 48
1 Y 38
2 Y 5
3 Y 1
4 Y 1
;
PROC FREQ DATA=TREND;
      TITLE "Chi-square Test for Trend";
   TABLES Alcohol*Mal / CHISQ;
   WEIGHT COUNT;
RUN;
```

RESULTS:

```{r,eval=FALSE}
# to test if there is a significant linear trend in proportions, use the “Mantel-Haenszel Chi-square."

Statistics for Table of Alcohol by Mal
  Statistic 	             DF 	Value   	Prob
Mantel-Haenszel Chi-Square 	1 	1.8278 	0.1764
```

## McNemar Test For Paired Data

Paired design: the same subject responds to a question under two different conditions (a dichotomous characteristic)

Ex: a subject is asked their opinion of smoking before and after an anti smoking ad

100 people, asked before and after given data table

```{r,eval=FALSE}
         	After
 	        -	  +
Before	-	32	15
 	      +	30	23
```

such that, 32 people were negative before and negative after.

```{r,eval=FALSE}
DATA MCNEMAR;
LENGTH AFTER BEFORE $ 1;
INPUT AFTER $ BEFORE $ COUNT;
FORMAT BEFORE AFTER $OPINION.;
DATALINES;
N N 32
N P 30
P N 15
P P 23
;
PROC FREQ DATA=MCNEMAR;
TITLE "McNemar's Test for Paired Samples";
TABLES BEFORE*AFTER / AGREE ;
WEIGHT COUNT;
RUN;
```

```{r,eval=FALSE}
      McNemar's Test
Chi-Square	DF	Pr > ChiSq
5.0000.  	1	     0.0253

therefore reject the null hypothesis that the variables are independent. 

```

## chi-square on un-tabulated input

```{r, eval=FALSE}
DATA GENDER;
   INPUT SEX $ PASS $;
DATALINES;
M YES         
F NO
F YES
M NO
F YES
M NO
M YES
F YES
F NO
M NO
M NO
F NO
M YES
M YES
M YES
F YES
M NO
;


PROC FREQ DATA=GENDER;
   TABLES SEX*PASS / CHISQ;
RUN;
```

# Module 6

## t-tests

## Independent vs Paired t-tests

```{r, eval=FALSE}
*Perform a paired t-test comparing before and after values.
Analyze these data as if the values were not paired. How do the p-values compare?;

data south_beach;
do subject = 1 to 12;
	input weight_before weight_after @@;
	output;
end;
datalines; 
300 290 350 331 190 200 400 395 244 240 321 300
330 332 250 242 190 185 160 158 260 256 240 220
;
run;

*the paired t-test produces a qq-difference plot that demonstraights normality of 
the data set. 

*paired t-test, 2-sided test 
H0: mu_after = mu_before
H1: mu_after != mu_before;

PROC TTEST DATA=south_beach;
title "Jacob Richards - 2 sided test";
PAIRED weight_before*weight_after;
RUN;

* The results of the t sided paired t-test were:
DF 	t Value 	Pr > |t|
11 	 -2.69 	     0.0212

The resuting p value is 0.0212 which is less than 0.05, therfore it is statistically signifigant 
We therefore reject the null hypothesis and tentativly conclude that the group means are not equal. 


*un-paired, independent 2-sided t-test;


DATA south_beach_independent;
 SET south_beach;
 time = 'BEFORE';
 weight = weight_before;
 OUTPUT;
 time = 'AFTER';
 weight = weight_after;
 OUTPUT;
 KEEP time weight;
RUN;


proc ttest data=south_beach_independent;
class time;
var weight;
run;

*the results of the independent t-test produces qq plots for both data sets and 
demonstraights normal distribution for both of them. 

*The results of the independent t-test were very different

Equality of Variances
 F Value 	Pr > F
 1.09 	    0.8940
 
*Pr > F is 0.8940 which is greater than 0.05 --> pooled results 
Method 	Variances 	DF 	t Value 	Pr > |t|
Pooled 	Equal 	    22 	 -0.25 	     0.8064

The P value obtained is 0.8064 which is far greater than 0.05 and therefore the test is statistically insignifigant 
we therfore fail to reject the null hypothesis that the group means are equal. 


when performing the paired t test the p value obtained was 0.0212 vs the independent t test p value was 0.8064

This actually makes perfect sense, as within a normal distribution of weights and a sample size of 12, such a small difference in 
means (about 7) would be not be unlikley at all. However if these were the same subjects, a difference in weight such as this 
would very unlikely be a coicidence. 
```

**paired**

1.) (Use SAS) Consider the dataset “Homework 6 data.xlsx”. It consists of 5 randomly selected student’s scores on Test 1 and Test 2 in my introductory statistics course. We want to answer 2 questions: a. First, we want to see if there was a difference in the two tests. b. Second, we want to see if there was improvement over the course of the semester. (Note: You will need to choose the correct tests to answer parts (a) and (b).);

**SAS will automatically produce qq-plots for both samples which you will use to verify normality**

the QQ plot this paired t-test produces verifyies the normalcy of the data sets.;

```{r,eval=FALSE}
proc import datafile="/home/u63989204/Midterm 1/Module 6/Module_6_Homework/Homework 6 data.xlsx"
dbms=xlsx 
out = hw_six replace;
getnames=yes;
option validvarname=v7;
run;
```

a\. First, we want to see if there was a difference in the two tests.;

```{r,eval=FALSE}
PROC TTEST DATA=hw_six;
title "Jacob Richards - two sided t-test";
PAIRED Test_2*Test_1; #first group - second group
RUN;
```

| **DF** | **t Value** | **Pr \> \|t\|** |
|-------:|------------:|----------------:|
|      4 |        2.96 |          0.0414 |

Reject the null hypothesis that the difference of means of tests is equal to zero.

```{r,eval=FALSE}
PROC TTEST DATA=hw_six side=U;
title "Jacob Richards - Upper sides t-test";
PAIRED test_2*test_1; #first group - second group 
RUN;
```

upper tailed test

| **DF** | **t Value** | **Pr \> t** |
|-------:|------------:|------------:|
|      4 |        2.96 |      0.0207 |

reject null differences are equal —\> conclude test 2 was better

**confidence interval for mean difference of test 2 - test 1**

this came from the two sided function call

```{r,eval=FALSE}
Mean	      95% CL Mean	  Std Dev	95% CL Std Dev
9.8000	0.6167	18.9833	7.3959	4.4312	21.2527
```

with 95% confidence the mean difference of test 2 - test 1 on the upper and lower bound is greater than 0 —\> more evidnce that test 2 was better than test 1

## ANOVA - SNK - data input

-   Continuous dependent variable.

-   Categorical independent variable that classifies observations into two or more groups.

```{r,eval=FALSE}
data balls;
input brand_age $ bounces;
datalines;
WNew 67 
WNew 72
WNew 74
WNew 82
WNew 81 
WOld 46
WOld 44
WOld 45
WOld 51
WOld 43 
PNEW 75
PNEW 76
PNEW 80
PNEW 72 
PNEW 73
POld 63
POld 62
POld 66
POld 62
POld 60
;
run;

PROC ANOVA DATA=balls;
  TITLE "Analysis of balls data";
  CLASS brand_age; 
  MODEL bounces = brand_age;
  MEANS brand_age / SNK ALPHA=0.05;
RUN;
```

RESULTS:

+------------+----------+--------------------+-----------------+-------------+-------------+
| **Source** | **DF**   | **Sum of Squares** | **Mean Square** | **F Value** | **Pr \> F** |
+:==========:+:========:+:==================:+:===============:+:===========:+:===========:+
| **Model**  | 3        | 2910.600000        | 970.200000      | 60.73       | \<.0001     |
+------------+----------+--------------------+-----------------+-------------+-------------+

reject the null hypothesis that all of the means are equal

at least one of the group means is different from the others

**SNK RESULTS:**

the means:

P_new and W_new \> P_old \> W_old

and the means of P_new and W_new are not significantly different

## ANOVA formatted data input (cholesterol medication)

\*Two cholesterol-lowering medications (statins) and a placebo were given to each of 10 volunteers with total cholesterol readings of 240 or higher. After 6 weeks, the following total cholesterol values were recorded Create a SAS data set by reading these data.;

the data is given such that each line is the data samples for each of the three groups

3 groups —\> anova instead of t-test

```{r,eval=FALSE}
data blood_med;
do group = 'Statin A','Statin B','Placebo';
	do subject = 1 to 10;
		input cholesterol @;
		output;
	end;
end;
datalines;
220 190 180 185 210 170 178 200 177 189
160 168 178 200 172 155 159 167 185 199
240 220 246 244 198 238 277 255 190 188
;

PROC ANOVA DATA=blood_med;
   TITLE "Jacob Richards - Analysis of Cholesterol Data";
   CLASS group; 
   MODEL cholesterol = group;
   MEANS group / SNK ALPHA=0.05; 
RUN;

```

**Anova test results**

+------------+----------+--------------------+-----------------+-------------+-------------+
| **Source** | **DF**   | **Sum of Squares** | **Mean Square** | **F Value** | **Pr \> F** |
+:===========+=========:+===================:+================:+============:+============:+
| **Model**  | 2        | 16258.46667        | 8129.23333      | 17.58       | \<.0001     |
+------------+----------+--------------------+-----------------+-------------+-------------+

H0 = The means are equal.

h1 = Not all the means are equal. (at least one mean is different than the others)

The test statistic obtained has a p value of less than 0.0001 which is less than 0.05 and is therefore statistically significant. We therefore reject the null hypothesis and tentatively conclude that at least one of the group means of cholesterol is significantly different than the others.

**SNK test (**Student-Newman-Keuls multiple-comparison test) **results**

the means:

Placebo \> Statin_A and Statin_B

aswell that the means for statin A and Statin B are not significantly different.

## anova - SNK - contrast (study methods)

Four different methods of preparing for a college entrance exam were compared. They are labeled 'A', 'B', 'C', and 'D'. The following exam scores were obtained for each of the four programs:;

\*Compare the four preparation methods. Use a multiple-comparison method of your choice to make pairwise comparisons.

Create two contrasts: One to compare methods A and B to C and D, the other to compare method D to the other three.;

```{r,eval=FALSE}
data study_methods;
do group = 'A','B';
	do student = 1 to 8;
		input score @;
		output;
	end;
end;
do group = 'C','D';
	do student = 1 to 7;
		input score @;
		output;
	end;
end;

datalines;
560 520 530 525 575 527 580 620
565 522 520 530 510 522 600 590 
512 518 555 502 510 520 516 
505 508 512 520 543 523 517
;
```

anova test

```{r, eval=FALSE}
PROC ANOVA DATA=study_methods;
  TITLE "Jacob Richards - Analysis of Reading Data";
  CLASS GROUP;  #this names the classification variables to be used in the model, in this case (A,B,C,D)
  MODEL score = GROUP; # dependent variable = independent variable 
  MEANS GROUP / SNK ALPHA=0.05; #runs test of the means of groups dependent variables which are the scores of each group
RUN;
```

anova results

+------------+----------+--------------------+-----------------+-------------+-------------+
| **Source** | **DF**   | **Sum of Squares** | **Mean Square** | **F Value** | **Pr \> F** |
+:===========+=========:+===================:+================:+============:+============:+
| **Model**  | 3        | 7607.18810         | 2535.72937      | 3.28        | 0.0366      |
+------------+----------+--------------------+-----------------+-------------+-------------+

SNK/CONTRAST

```{r,eval=FALSE}
PROC GLM DATA=study_methods;
   TITLE "Jacob Richards - Analysis of Reading Data - Planned Comparisons";
   CLASS GROUP; 
   MODEL score = GROUP;         # A B C D 
   MEANS PROGRAM / SNK ALPHA=0.05;
   CONTRAST 'A&B to C&D' GROUP   1 1 -1 -1; 
   CONTRAST 'A & B & C to D' GROUP   1 1 1 -3;
#remember to set the coefficient's in the order corresponding to their group's input pattern
RUN;
```

**SNK results**

inconclusive

**contrast results**

+------------------------+----------+-----------------+-----------------+-------------+-------------+
| **Contrast**           | **DF**   | **Contrast SS** | **Mean Square** | **F Value** | **Pr \> F** |
+:======================:+:========:+:===============:+:===============:+:===========:+:===========:+
| **A and B to C and D** | 1        | 7225.152381     | 7225.152381     | 9.36        | 0.0051      |
+------------------------+----------+-----------------+-----------------+-------------+-------------+
| **D to A B C**         | 1        | 2413.012158     | 2413.012158     | 3.13        | 0.0888      |
+------------------------+----------+-----------------+-----------------+-------------+-------------+

The means of A and B to C and D are significantly different.

The means of A and B and C to D are not significantly different.

equivalently, if not specified that you cannot reformat the given data then.

```{r, eval=FALSE}
DATA EXAM;
INPUT PROGRAM $ SCORE @@;
DATALINES;
A 560 A 520 A 530 A 525 A 575 A 527 A 580 A 620
B 565 B 522 B 520 B 530 B 510 B 522 B 600 B 590
C 512 C 518 C 555 C 502 C 510 C 520 C 516
D 505 D 508 D 512 D 520 D 543 D 523 D 517
;
RUN;
PROC GLM DATA=EXAM;
CLASS PROGRAM;
MODEL SCORE = PROGRAM;
MEANS PROGRAM / SNK ALPHA=0.05;
CONTRAST 'A AND B VS. C AND D' PROGRAM -1 -1 1 1;
CONTRAST 'A B AND C VS. D' PROGRAM -1 -1 -1 3;
RUN;
```

## anova - SNK - contrast (reading scores)

DATA:

```{r,eval=FALSE}
DATA READING;
   INPUT GROUP $ WORDS @@;
DATALINES;
X 700   X 850   X 820   X 640   X 920
Y 480   Y 460   Y 500   Y 570   Y 580
Z 500   Z 550   Z 480   Z 600   Z 610
;
run;
```

**ANOVA TEST**

```{r,eval=FALSE}
PROC ANOVA DATA=READING;
   TITLE "Analysis of Reading Data";
   CLASS GROUP; /#The CLASS statement names the classification variables to be used in the model. Typical classification variables are TREATMENT, SEX, RACE, GROUP, and REPLICATION. The CLASS statement is required, and it must appear before the MODEL statement#/
   MODEL WORDS = GROUP; /# MODEL dependent variable = independent variables.#/
   MEANS GROUP / SNK ALPHA=0.05; /#Compute means of the dependent variables for any effect that appears on the right-hand side in the MODEL statement (GROUP).#/
/#SNK option performs the Student-Newman-Keuls multiple range test on all main effect means in the MEANS statement.#/
/#ALPHA= specifies the level of significance for comparisons among the means. By default, ALPHA=0.05. You can specify any value greater than 0 and less than 1.#/
RUN;
```

+------------+----------+--------------------+-----------------+-------------+-------------+
| **Source** | **DF**   | **Sum of Squares** | **Mean Square** | **F Value** | **Pr \> F** |
+:==========:+:========:+:==================:+:===============:+:===========:+:===========:+
| **Model**  | 2        | 215613.3333        | 107806.6667     | 16.78       | 0.0003      |
+------------+----------+--------------------+-----------------+-------------+-------------+

-   The F-value was 16.78 (df = 2, 12, p = 0.0003).

-   Reject the null hypothesis

-   There is at least one group mean different from one other group mean, but cannot be sure which one(s) are different.

-   The reading-instruction methods were not all equivalent

    **SNK TEST RESULTS**

    X \> Y & Z , Y = Z

**CONTRAST TEST**

suppose that x is a new method and y and z are traditional ones

we are testing x to y and z by H0: 2mu(x) - mu(y) - mu(z) = 0

and y and z the same way

so groups X Y Z –\> CONTRASTS: 2 -1 -1 and 0 -1 1

```{r,eval=FALSE}
PROC GLM DATA=READING;
   TITLE "Analysis of Reading Data - Planned Comparisons";
   CLASS GROUP; /#The CLASS statement names the classification variables to be used in the model.#/
   MODEL WORDS = GROUP;   
   CONTRAST 'X VS. Y AND Z' GROUP   -2 1 1; 
/#CONTRAST 'label' independent (CLASS) variable followed by a set of k coefficients (where k is the number of levels of the class variable)#/
   CONTRAST 'METHOD Y VS Z' GROUP   0 1 -1;
  #1. The sum of the coefficients must add to 0 
  #2. The order of the coefficients must match the alphanumeric order of the levels of the CLASS variable if it is not formatted.#/
RUN;
```

+-------------------+----------+-----------------+-----------------+-------------+-------------+
| **Contrast**      | **DF**   | **Contrast SS** | **Mean Square** | **F Value** | **Pr \> F** |
+:=================:+:========:+:===============:+:===============:+:===========:+:===========:+
| **X VS. Y AND Z** | 1        | 213363.3333     | 213363.3333     | 33.22       | \<.0001     |
+-------------------+----------+-----------------+-----------------+-------------+-------------+
| **METHOD Y VS Z** | 1        | 2250.0000       | 2250.0000       | 0.35        | 0.5649      |
+-------------------+----------+-----------------+-----------------+-------------+-------------+

Notice that the method X is shown to be significantly different from methods Y and Z combined, and there is no difference between methods Y and Z at the 0.05 level.

## Wilcox Rank sum test

```{r,eval=FALSE}
*# Nonparametric (distribution-free tests);
DATA TUMOR;
   INPUT GROUP $ MASS @@;
DATALINES;
A 3.1 A 2.2 A 1.7 A 2.7 A 2.5
B 0.0 B 0.0 B 1.0 B 2.3
;
PROC NPAR1WAY DATA=TUMOR WILCOXON MEDIAN; /#PROC NPAR1WAY performs the nonparametric tests#/
/#The option WILCOXON requests the Wilcoxon rank-sum test#/
/#MEDIAN option requests an analysis of median scores. When there are two classification levels, this option produces the two-sample median test.#/
   TITLE "Nonparametric Test to Compare Tumor Masses";
   CLASS GROUP; /#The CLASS statement, which is required, names one and only one classification variable. The variable can be character or numeric. The CLASS variable identifies groups (or samples) in the data#/
   VAR MASS; /#The VAR statement names the response or dependent variables to be included in the analysis. These variables must be numeric.#/
   EXACT WILCOXON; /#The EXACT statement computes exact p-values in addition to the asymptotic approximations usually computed. Used when we have relatively small sample sizes#/
RUN;
```

and the options for it

```{r,eval=FALSE}
PROC NPAR1WAY DATA=SAS-data-set WILCOXON MEDIAN; /*PROC NPAR1WAY performs the nonparametric tests; The option WILCOXON requests the Wilcoxon rank-sum test; The MEDIAN option requests the median test. */ 
  CLASS variable; /*The CLASS statement names one and only one classification variable.*/
  VAR variables; /*The VAR statement names the response variables to be included in the analysis*/ 
  EXACT test-keyword; /*The EXACT statement computes exact p-values in addition to the asymptotic approximations usually computed. Used when we have relatively small sample sizes*/ 
RUN;
```

# Module 7

-   Data points (x,y) are naturally paired samples as x and y come from the same observation

## Pearson Correlation Coefficient

### Proc corr - options

will produce matrix of pearson corr coef's with pvalue directly beneath it, all var variables will be the rows and columns so the main diagonal will be all 1's.

```{r,eval=FALSE}
PROC CORR DATA= fitness NOSIMPLE;  #change to PROC CORR DATA= fitness NOSIMPLE; to only get corr coef and p-value or replace "NOSIMPLE" with RANK
    VAR Oxygen_Consumption Runtime; #2x2 matrix produced by only var variables inputted
RUN;
```

### Proc corr - correlation coeffient matrix

will produce matrix of cooraltion coeficients, rows = with, columns = vards, with output in order of rank corr coef largest to least

```{r,eval=FALSE}
PROC CORR DATA= fitness RANK;
  VAR Runtime Age Weight Run_Pulse Rest_Pulse Maximum_Pulse Performance; # 1x7 corr coef matrix will also print simple statistics
  WITH Oxygen_Consumption;
RUN;

PROC CORR DATA= fitness NOSIMPLE;
  VAR Runtime Age Weight Run_Pulse Rest_Pulse Maximum_Pulse Performance; # 7x7 corr coef matrix, no statistics, only corr coef and p-values 
RUN;
```

### Pearson product-moment and Spearman rank-order correlation

```{r,eval=FALSE}
DATA spearcc;
  INPUT X Y XRank YRank;
  DATALINES;
   1  1  1 1
   3  2  2 2
   7  3  3 3
   9  6  4 4
  11  8  5 5
;
RUN;
PROC CORR DATA=spearcc 
PEARSON /*Requests Pearson product-moment correlation  */
SPEARMAN /*Requests Spearman rank-order correlation */    NOSIMPLE 
NOPROB /*Suppresses p-values */; 
RUN;
```

### partial cooralation option

```{r,eval=FALSE}
PROC CORR DATA=fitness;
  VAR Oxygen_Consumption Runtime Run_Pulse Rest_Pulse Maximum_Pulse Age Weight; 
  PARTIAL Performance; #*The PARTIAL statement lists variables to use in the calculation of partial correlation statistics.*/
RUN;        
```

output: the p-values are testing H0: population corr coef = 0

```{r,eval=FALSE}
Pearson Correlation Coefficients, N = 31
                        Prob > |r| under H0: Rho=0
Oxygen_Consumption   Performance        Runtime      Rest_Pulse
                         0.86377       -0.86219        -0.39935

         PVALUE          <.0001         <.0001          0.0260

Oxygen_Consumption     Run_Pulse          Age     Maximum_Pulse
                        -0.39808       -0.31162        -0.23677
        PVALUE           0.0266         0.0879          0.1997

Oxygen_Consumption        Weight
                        -0.16289
            PVALUE       0.3813

```

```{r,eval=FALSE}
± 1.00 	Perfect Correlation
± 0.80 	Strong Correlation
± 0.50 	Moderate Correlation
± 0.20 	Weak Correlation
± 0.00 	No Correlation
```

### R-square - aka coef of determination

is just the (pearson correlation coef)\^2 variables corresponding to signifigant R\^2 p-values have a significant linear correlation between each other.

## Regression

### conditions of regression analysis validity

assumptions for regression model

```{r,eval=FALSE}
The errors have a mean of 0 at each value of the independent variable.
The errors have the same variance at each value of the independent variable.
The errors are independent.
The errors are normally distributed (for confidence intervals and tests).
```

Plots to verify assumptions: Residuals vs. Predicted (predicted responce variable by the model) Residuals vs. (Observed Values of the Independent Variable, used to produce model) Studentized Residuals vs. (Observed Values of the IndependentVariable) (OBS) Residual by QQ plot

```{r,eval=FALSE}
GOPTIONS RESET=ALL;
PROC REG DATA= fitness;
MODEL Oxygen_Consumption=Performance;
 PLOT R.*(P. Performance); #R. will plot residual for *(P. Performance) [the P. is response estimate from the model, and Performance is actual predictor variable]
PLOT STUDENT.*OBS. / #(STUDENT. will divide residuals by standard errors and *OBS. is what it will be plotted by as number of observations
VREF=3 2 -2 -3 #refrence lines for studentized residuals
VAXIS=-4 TO 4 BY 1 #upper and lower bounds of studnized residual plot 
HAXIS=0 TO 32 BY 1; # left and right bounds of the studentized residual plot 
PLOT RESIDUAL.* NQQ.; # residual by qq plot 
SYMBOL V=DOT;
TITLE "Plots Of Diagnostic Statistics";
RUN;
QUIT;
```

Plot of residuals vs. predicted values of Oxygen_Consumption (dependent variable)

-   randomly scattered about the reference line at 0
-   no apparent trends or patterns
-   check

Plot of residuals vs. observed values of Performance (independent variable)

Since residuals scatter around 0, we can verify the assumption:

-   Have a mean of 0 at each value of the independent variable.
-   Because of no apparent trends or patterns in the residuals, we can verify the assumption:
-   Have the same variance at each value of the independent variable.
-   check

Studentized Residuals vs. actual predictor values given to produce regression line (observed values of Performance (independent variable))

-   produce studentship residuals vs observation number
-   no unusually large residuals -\> check
-   produce residuals vs normal quantiles
-   straight qq line --\> normal -\> check

### Proc Reg - with fitted model

don't forget to check the signifgance levels of intercept and coef of regression model

```{r,eval=FALSE}
GOPTIONS RESET=ALL FTEXT='ARIAL/BO' GUNIT=PCT HTEXT=3;
#The PROC REG statement is used when you want to fit a model to the data, you must also use a MODEL statement;
PROC REG DATA=fitness;  
  MODEL Oxygen_Consumption = Performance/ CLB ALPHA=0.01;
#MODEL dependent variable(s)= independent-variables/ options;
#CLB: computes 100(1-a) % confidence limits for the parameter estimates;
#ALPHA= option sets significance value for confidence and prediction intervals and tests;
  PLOT Oxygen_Consumption*Performance;
#The PLOT statement in PROC REG displays scatter plots with y variable on the vertical axis, x variable on the horizontal axis, and the regression line;
  SYMBOL V=DOT H=1.5 CV=RED W=2 CI=BLACK ;
RUN;
QUIT;
```

### Proc Reg - Need Predictions with confidence interval

```{r,eval=FALSE}
proc import datafile="/home/u63989204/Module 7/Module 7 Labs/muscle.txt"
dbms=dlm
out = work.muscle_and_age replace;
getnames=yes;
option validvarname=v7;
run;

proc contents data=work.muscle_and_age;
run;

data muscle;
set work.muscle_and_age;
run;

DATA NEEDEDPREDICTIONS;
INPUT AGE @@;
DATALINES;
66 40
;

RUN;
DATA PREDICT;
SET MUSCLE NEEDEDPREDICTIONS;
RUN;
PROC REG DATA=PREDICT;
MODEL MASS=AGE / P CLM ALPHA=0.02; 
#muscle mass of a 40 year old, that means all of them, not just an individual, think about it. the mean interval would 
#be tighter than the individual one 
RUN;
```

### Proc Reg - comparing goodness of fit for transformed variable linear model

```{r,eval=FALSE}
data dose_response;
input dose systolic diastolic;
log_dose = LOG(dose);
datalines;
4 180 110
4 190 108
4 178 100
8 170 100
8 180 98
8 168 88
16 160 80
16 172 86
16 170 86
32 140 80
32 130 72
32 128 70
;

PROC REG DATA = dose_response;
    MODEL systolic = log_dose;
    plot systolic*log_dose;
    MODEL diastolic = log_dose;
    plot diastolic*log_dose;
RUN;

PROC REG DATA=dose_response;
   MODEL systolic = log_dose;  #replace with MODEL systolic = log_dose / NOPRINT; if only want the plot
   PLOT RESIDUAL.*log_dose; 
   MODEL diastolic = log_dose;
   PLOT RESIDUAL.*log_dose; 
RUN;
```

in examination of the adjusted r-square, the non transformed predictor variable was better for Systolic blood pressure and the natural logarithim LOG() was better for Diastolic blood pressure

# Module 9

## base functions

```{r,eval=FALSE}
my_seq3 <- rep(2, times = 10)   # repeats 2, 10 times
my_seq3
##  [1] 2 2 2 2 2 2 2 2 2 2


my_seq7 <- rep(c(3, 1, 10, 7), each = 3) # repeats each 
                                         # element of the 
                                         # series 3 times
my_seq7
##  [1]  3  3  3  1  1  1 10 10 10  7  7  7


#match - finds the location of the arguments of the first input in the second input
# Define two vectors
x <- c("a", "b", "c", "d")
table <- c("c", "a", "b", "e")

# Match positions of elements of x in table
match(x, table)
# Output: [1] 2 3 1 NA

# Define a vector
x <- c(1, 2, 3, 4)

cumsum(x)
# Output: [1]  1  3  6 10

cumprod(x)
# Output: [1]  1  2  6 24

```

```{r,eval=FALSE}


####### Some basic math functions: ########

# abs 			     Absolute Value
# ceiling 		   Next Larger Integer
# floor 		     Next Smallest Integer
# cos, sin, tan  Trigonometric Functions
# exp(x) 	       e^x  [e = 2.71828 ?]
# log 			     Natural Logarithm
# log10 		     Logarithm Base 10
# sqrt 			     Square Root
# length 	       Length of Object, how many elements in the object
# prod 		       Product of Values
# sum 		       Sum
# rev 		       Put Values of Vectors in Reverse Order
# sort 		       Sort Values of Vector
# order 	       Permutation of Elements to Produce Sorted Order
# rank 		       Ranks of Values in Vector
# match 	       Detect Occurrences in a Vector x[match(x,y)] == x[x %in% y] == intersect(x,y)
# cumsum 	       Cumulative Sums of Values in Vector
# cumprod 	     Cumulative Products

sqrt(a*pi)
log(a,3)  # log( x, base)   computes the log of x with base b
log1p(a)  # computes log(1+a)
log2(a)   # computes the log base 2 of a
log(100, base=10)   # one easy way to do it

# There are more options
exp(a)    # computes the exponential of a
log(a,b)
logb(2,4)

# Tricky example dealing with rounding issues
(1/49)*49
(1/49)*49==1  # what is the problem?
(1/49)*49-1  # rounding errors
eps<- 1e-12# to set the maximum difference allowed
abs((1/49)*49-1)<=eps

#trig functions
x<-1
sin(x)   # finds the sine of each element of x
sin(x)^2
sin(x)^2+cos(x)^2


# Although there are lots of built in functions in R, sometimes, you might need to create your own
power<-function(x)
{
  power2<-x^2
  power3<-x^3
  return(list("sqr"=power2,"cub"= power3))
}
power(4)
power(4)$sqr

sqrt(-3)   # NaN is shown when value undefined or missing

# On inequalities 
sign <-(x<0) # returns a logical the values of x that are negative
sign

x <- c(-1:5, 1)
sign<-(x<0)
sign

```

```{r,eval=FALSE}
######## creating sequences and vectors #######

x<-1:10  # creates regular sequence between 1 and 10
length(x)
x[2] # captures the 2nd element of x
x[-2] # removes 2nd element from x

seq(1,10, by=1) # another method to do it
seq(0,2, length=10)  # creates a sequence of 10 numbers from 0 to 2
10:1 # is another way
-10:1
x<-c(x,11) # takes x, adds 11 to it, and replaces x by it
x<-c(x, c(12,13)) # takes x and adds another vector c(12, 13)
x[3]  # to obtain the 3rd element of x
x[-3]  # to delete the 3rd element of x
x[3:7] # to obtain the those elements with position from 3 to 7
x[-c(3:7)]  # delete all those positions, and give the others

x[x>3]
x[ x< -2 | x > 2] # values less than -2 or greater than 2 
# use & for and.

which(x == max(x)) # gives the largest value index for x

# Some useful functions associated with any vector
x  # let's thing of this as x(1), x(2), ..., x(15)
x/2
c(1,2,3,4)+c(5,6)
c(1,2,3,4)+c(5,6,7,8)
length(x)   # it will be 13
x[-1] # we know this one
x[-1]-x[-length(x)] #  delete x[-1] as a vector with x[-length(x)]
x[-length(x)]
a=6
b=6
a==b  # Do we have a equals b?
a<b
a>b
a<=b
rm(a)


y<-seq(1,3,0.1) # creates a sequence from 1 to 3 with a 0.1 increment

chardata=c("w", "a", "b", "c")
chardata
is.numeric(chardata)  # way to check if data is numeric or not
chardata>"a"     # alphabetic inequality
chardata[chardata>"a"]
length(chardata[chardata>"a"])
mode(chardata)

x<-1:10 # creates a set of numbers, not a vector yet 
x
dim(x)  # dimenson does not exist yet
dim(x)<-c(2,5)  # creates a vector or rather a 2 by 5 matrix
x # is a matrix filled by row, not by column
#  To do that, let's change x back to a sequence and use the matrix function
x<-1:10
x<-matrix(x,2,5)
x
z<-as.vector(x)
z
w<-matrix(x,2,5, byrow=T)
w
dim(w)<-c(3,5) # error message
x
x[2,3]
x[,3] # to access the 3rd column
x[,-3] # everything except the 3rd column
x[, c(2,5)]    # gives the 2nd and 5th column
x[2,c(2,5)]      # gives the intersection of 2nd row, and the 2nd and 5th column
x<-cbind(x,c(11:12))  # add another column, adjoin column
x
x<-rbind(x,13:18) # allows to adjoin row
x
t(x)  # to get the transpose of x
apply(x,1,mean) # gives mean of each row
apply(x,2,mean)  # gives the mean of each column


jsum<-function(x)
{
  jsum<-0
  for (i in 1:length(x)) {
  	jsum<-jsum+x[i]
  }
  return(jsum)
} 
x=c(2, 3, 50)
jsum(x)

######### MATRICES ###########

# matrices are like data frames but all entries must be same type

# create a matrix with 3 rows and 3 columns

matrix(0, 3, 3)
matrix(1, 3, 3)
diag(3)
diag(c(1, 2, 4), 3)

# store the matrix
x <- matrix(0, 5, 3)

# column major order by default
y <- matrix(0:8, 3, 3)
y
diag(y)
sum(diag(y))  #trace

z <- matrix(0:8, 3, 3, byrow=TRUE)
z

# scalar multiplication
2*y

# matrix multiplication vs elementwise multiplication
y * z    #elementwise multiplication
y %*% z  #matrix multiplication

# extract elements, rows or columns
z[3,2]
z[ ,2]   #second row
z[ ,2:3] #rows 2 and 3

a <- iris[1:5, 1:4]  #is it a matrix?
a
is.matrix(a)
colMeans(a)

matplot(iris[, 1:4])


######### Matrix Operations ##########

x <- matrix(rnorm(9) , 3, 3)
x
solve(x)   #inverse
det(x)     #determinant
eigen(x)   #eigen-decomposition
svd(x)     #singular value decomposition

e <- eigen(x)
e$values
v <- e$vectors
t(v) %*% x %*% v

y <- t(x) %*% x
qr(y)       #QR factorization
chol(y)     #Choleski factorization
svd(y)      #singular value decomposition

e <- eigen(y)
e
diag(e$values)

v <- e$vectors
t(v) %*% y %*% v
round(t(v) %*% y %*% v, 5)

# Let's look at more about matrices
a<-matrix(c(1,2,3,4),2,2)
b<-matrix(c(5,6,7,8),2,2)
det(a)
t(a) # gives the transpose of a
a%*%b # is the product
eigen(a)
?solve
solve(a) # gives the inverse of the matrix
# Choleski decomposition is obtained by taking 'chol(a)'
# The QR decomposition is obtained from 'qr(a)'
solve(a,b) # find a matrix c such that a*c=b
crossprod(a)
crossprod(a,b)
dim(a)
apply(a,1,sum)
apply(a,2,sum)
apply(a,c(1,2),sum)


```

**just a good indexing tip**

difference between %in% and == operators x %in% y the index within x of which somewhere in the vector why is the same value

x == y component by component x[i] == y[i]

therefore x[x %in% y] is equivalent to intersect(x,y)

```{r,eval=FALSE}
> x <- c('a','b','c')
> y <- c('c', 'b', 'a')
> x == y
[1] FALSE  TRUE FALSE

> x %in% y
[1] TRUE TRUE TRUE

> x %in% letters
[1] TRUE TRUE TRUE

> letters %in% x
 [1]  TRUE  TRUE  TRUE FALSE FALSE FALSE
 [7] FALSE FALSE FALSE FALSE FALSE FALSE
[13] FALSE FALSE FALSE FALSE FALSE FALSE
[19] FALSE FALSE FALSE FALSE FALSE FALSE
[25] FALSE FALSE
```

## 

# Module 10

## .txt read in

```{r}
flowers <- read.table(file = '/Users/jacobrichards/Desktop/Personal_save/Stat_405_Final_Exam/flower.txt', header = TRUE, sep = "\t", stringsAsFactors = TRUE) #only needs file name
```

## options for read.table

```{r,eval=FALSE}
dec = "," #if you're in europe 
na.strings = "*" #if they represent an empty element by somehing other than NA
sep = "\t" #tab delimeted 
sep = "," # for csv 
```

## flexible read in function

```{r}
library(readr)
# import white space delimited files

setwd("/Users/jacobrichards/Desktop/Personal_save/Stat_405_Final_Exam")

all_data <- read_table(file = 'seeiffwork.txt', col_names = TRUE) #be carefull about spaces in input
all_data #yes works
```

```{r,eval=FALSE}
# import comma delimited files
all_data <- read_csv(file = 'data/flower.txt')

# import tab delimited files
all_data <- read_delim(file = 'data/flower.txt', delim = "\t")

# or use
all_data <- read_tsv(file = 'data/flower.txt')
```

## factor - order

this will print the data frame rows ordered by low medium high of nitrogen

```{r}
flowers$nitrogen <- factor(flowers$nitrogen, 
                           levels = c("low", "medium", "high"))    
nit_ord <- flowers[order(flowers$nitrogen),]
nit_ord
```

## base table

```{r}
table(flowers$nitrogen, flowers$treat)
```

## stratified table-ing and functions applied to data frames

```{r}
# the right side of the ~ is all variables we would like to stratify and produce counts for 
xtabs(~ nitrogen + treat + block, data = flowers)
# will take care of data if not already factored 

ftable(xtabs(~ nitrogen + treat + block, data = flowers))

#evaluate the mean height of each category of nitrogen 
tapply(flowers$height, flowers$nitrogen, mean)
#the third argument is any function you would like, first is what it will be applied on, and second is the categories of division of the first variable 

tapply(flowers$height, flowers$nitrogen, sd)

# if there are NA's in the data and want to remove them 
tapply(flowers$height, flowers$nitrogen, summary, na.rm=TRUE)

# you can even pass lists of categories to stratify the variables to be evaluated by the function
tapply(flowers$height, list(flowers$nitrogen,flowers$treat), summary, na.rm=TRUE)
```

## aggregate

```{r,eval=FALSE}
#input variables for function - stratification categories - function to apply 
aggregate(flowers[, 4:7], by = list(nitrogen = flowers$nitrogen, treat = flowers$treat), FUN = mean)

# after formatting the dates column to be only year char you can do the rest of the problem in a single line 
means_by_year <- aggregate(mat[,2:23],by=list(year=mat[,1]),FUN=mean)

# and if we want to apply the previous to a subset we can do this 
aggregate(height ~ nitrogen + treat, FUN = mean, subset = flowers < 7, data = flowers)

aggregate(height ~ nitrogen + treat, FUN = mean, subset = block == "1", data = flowers)
```

# Module 11

## base histogram function

```{r,eval=FALSE}
hist(flowers$height)

max(flowers$height)
#if we want to set custom intervals for each frequency bars (more or less detailed)
max(flowers$height) #round up last point from the max 
intervals <- seq(0,18,1)
hist(flowers$height, breaks = intervals, main = "petunia height")

#proportion instead of frequency
hist(flowers$height, breaks = intervals, main = "petunia height",freq = FALSE)

#add a pdf approximation curve (density curve)
hist(flowers$height, breaks = intervals, main = "petunia height",freq = FALSE)
lines(density(flowers$height))
```

## boxplots (box and whiskers)

```{r,eval=FALSE}
boxplot(flowers$weight, ylab = "Weight")
#the thick black line in the middle is the median 
#upper 15 is the upper quartile (75% of data is less than 15)
#around 10 is the lower quartile (25% of the date is less than 10)
#the grey area of the box is the inter quartile range 
#the very upper line is the whiskers and points beyond that are potential unusual observations 
```

## box-plots stratified

```{r,eval=FALSE}
        #distribution of weight by levels of nitrogen, data = dataframe
boxplot(weight ~ nitrogen, data = flowers, ylab = "weight (g)", xlab = "nitrogen level")
#levels are stored in df as high,low,medium so that's the order in which the plots are printed

#change the order of the levels to this 
flowers$nitrogen <- factor(flowers$nitrogen, levels = c("low", "medium", "high"))
boxplot(weight ~ nitrogen, data = flowers, ylab = "weight (g)", xlab = "nitrogen level")
#and that's how they print now 

#we can also do combinations of stratification variables 
boxplot(weight ~ nitrogen * treat, data = flowers, 
         ylab = "weight (g)", xlab = "nitrogen level")

#reduce the font size of the group labels so they all fix on the x-axis 
boxplot(weight ~ nitrogen * treat, data = flowers, 
         ylab = "weight (g)", xlab = "nitrogen level", 
         cex.axis = 0.7)
```

## violin plots

```{r,eval=FALSE}
library(vioplot)
#weight stratified by levels of nitrogen
vioplot(weight ~ nitrogen, data = flowers, ylab = "weight (g)", xlab = "nitrogen level", col = "lightblue")
#the white circle is the median value 
```

## dotcharts

```{r,eval=FALSE}
#on the x-axis is values of height, the y-axis is order that those values are in data frame 
#(bottom is beginning of data frame, top is end of data frame)
dotchart(flowers$height)
```

## dot-charts stratified

```{r}
#dotchart(flowers$height, groups = flowers$nitrogen)
```

## pairs plots (correlation matrix)

Variable in the row is the y-axis variable and variable in the column is the x-axis

```{r}
pairs(flowers[, c("height", "weight", "leafarea", "shootarea", "flowers")],panel = panel.smooth)
#pairs(flowers[, 4:8]) #equivalently
#kernel smoother panel option which is the red line 
```

## pairs options, correlation, histogram, smooth

```{r}
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    usr <- par("usr")
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r)
}
pairs(flowers[, c("height", "weight", "leafarea", "shootarea", "flowers")],panel = panel.cor)

# or if we only want either the upper or lower diagonal to be corr coef's 


pairs(flowers[, c("height", "weight", "leafarea", "shootarea", "flowers")],lower.panel = panel.cor)
# or upper.panel

panel.hist <- function(x, ...)
{
    usr <- par("usr")
    par(usr = c(usr[1:2], 0, 1.5) )
    h <- hist(x, plot = FALSE)
    breaks <- h$breaks; nB <- length(breaks)
    y <- h$counts; y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)
}


# upper, lower, diagonal
pairs(flowers[, c("height", "weight", "leafarea", 
                "shootarea", "flowers")], 
                 lower.panel = panel.cor,
                 diag.panel = panel.hist,
                 upper.panel = panel.smooth)
```

## coplots

```{r}
flowers <- read.table(file = '/Users/jacobrichards/Desktop/Personal_save/Stat_405_Final_Exam/flower.txt', header = TRUE, sep = "\t", stringsAsFactors = TRUE) #only needs file name
#flowers as a function of weight, examined at different intervals of leaf area (stratify by quantatative)
coplot(flowers ~ weight|leafarea, data = flowers)
#panels from bottom left to top right correspond to bars of equivalent potion in the leaf area interval box 

#if you don't want any of the panels to overlap 
coplot(flowers ~ weight|leafarea, data = flowers, overlap = 0)

# can also condition by categorical variables 
coplot(flowers ~ weight|nitrogen, data = flowers)

# and even all combinations of more than 1 either categorical or numeric variables 
coplot(flowers ~ weight|nitrogen * treat, data = flowers)

```

## coplot with options

```{r}
coplot(flowers ~ weight|nitrogen * treat, data = flowers,
        panel = function(x, y, ...) {
        points(x, y, ...)
        abline(lm(y ~ x), col = "blue")})
```

## latice

```{r}
#everything in lattice uses response ~ predictor notation
library(lattice)
#histogram
histogram(~ height, type = "count", data = flowers)

#box plot (box and whiskers)
#weight stratified by nitrogen level
bwplot(weight ~ nitrogen, data = flowers)

#                lattice.                    R.  
#scatterplot    xyplot()	                  plot()
#frequency      histogram(type = "count")	hist()
#boxplot	       bwplot()	                  boxplot()
#Cleveland     dotplot	dotplot()	          dotchart()
#scatterplot   matrix	splom()	            pairs()
#conditioning  plot	xyplot(y ~ x | z)	    coplot()
```

## lattice plots in multiple panels, layout options

```{r}
library(lattice)
#height conditional on nitrogen level’. 
histogram(~ height | nitrogen, type = "count", data = flowers)

#layout = columns by rows 
histogram(~ height | nitrogen, type = "count", layout = c(1, 3), data = flowers)

#factor within the plot function call 
#will not change the values in the data frame 
bwplot(weight ~ nitrogen | factor(block), data = flowers)

#multiple categories of stratification
xyplot(height ~ weight | nitrogen * treat, data = flowers)

#stratify by multiple categories and display by another category 
xyplot(flowers ~ shootarea | nitrogen * treat, 
        groups = block, auto.key = TRUE, data = flowers)

```

## base r plot customization

```{r}
#margins par(mar = c(bottom, left, top, right)
par(mar = c(4.1, 4.4, 4.1, 1.9), xaxs = "i", yaxs = "i") #makes the axis better 
plot(flowers$weight, flowers$shootarea, 
       xlab = "weight (g)",
       ylab = expression(paste("shoot area (cm"^"2",")")), #superscript in the y label 
       xlim = c(0, 30), ylim = c(0, 200), bty = "l", # x-lim and y-lim # gets rid of box that surrounds entire plot 
       las = 1, cex.axis = 0.8, tcl = -0.2, #makes y axis labels horizontal, makes them smaller, make tic marks shorter
       pch = 16, col = "dodgerblue1", cex = 0.9) #plotting symbol used, color, and size of it 
text(x = 28, y = 190, label = "A", cex = 2) #coordiates of text, the text, and size of the text 
par(mar = c(4.1, 4.4, 4.1, 1.9), xaxs="i", yaxs="i")
points(x = flowers$weight[flowers$nitrogen == "low"],
       y = flowers$shootarea[flowers$nitrogen == "low"],
       pch = 16, col = "deepskyblue")
points(x = flowers$weight[flowers$nitrogen == "medium"],
       y = flowers$shootarea[flowers$nitrogen == "medium"],
       pch = 16, col = "yellowgreen")
points(x = flowers$weight[flowers$nitrogen == "high"],
       y = flowers$shootarea[flowers$nitrogen == "high"],
       pch = 16, col = "deeppink3")
text(x = 28, y = 190, label = "A", cex = 2)
leg_cols <- c("deepskyblue", "yellowgreen", "deeppink3")
leg_sym <- c(16, 16, 16)
leg_lab <- c("low", "medium", "high")

legend(x = 1, y = 200, col = leg_cols, pch = leg_sym, 
        legend = leg_lab, bty = "n", 
        title = "Nitrogen level")
```

## base R layout for multiple plots customized

```{r}
# matrix(c(topleft,topright,bottomleft,bottomright))

layout_mat <- matrix(c(2, 0, 1, 3), nrow = 2, ncol = 2,byrow = TRUE) 
layout_mat

#2x2 panels with the second plot 
#first plot in lower left 
#second plot in upper left 
#third plot in bottom right and the top right panel is empty 

#not we can change the aspect ratio of 
my_lay <- layout(mat = layout_mat, 
                 heights = c(1, 3),  #first row is one unit tall , second row is 3 units tall 
                 widths = c(3, 1), respect =TRUE)  #first column is 3 units wide, second column is 1 unit wide 
layout.show(my_lay)

par(mar = c(4, 4, 0, 0)) # expands margins when text gets cut off

plot(flowers$weight, flowers$shootarea, xlab = "weight (g)", ylab = "shoot area (cm2)")

par(mar = c(0, 4, 0, 0)) # expands margins when text gets cut off

boxplot(flowers$weight, horizontal = TRUE, frame = FALSE, axes =FALSE)

par(mar = c(4, 0, 0, 0)) # expands margins when text gets cut off

boxplot(flowers$shootarea, frame = FALSE, axes = FALSE)
```

## export base plots

```{r}
pdf(file = "my_plot.pdf")
plot(flowers$weight, flowers$shootarea)
dev.off()

png("my_plot.png")
plot(flowers$weight, flowers$shootarea)
dev.off()
```

# Module 12

### Histograms

```{r,eval=FALSE}
#remember histogram by default will calculate the count itself so you would pass the raw data for it to process

ggplot(data = diamonds, aes(x = carat)) + geom_histogram(bins=100)

ggplot(data = diamonds, aes(x = carat)) + geom_histogram(binwidth=0.001)


#geom_histogram produces the following variables from inputed data

    #count, the number of observations in each bin
    #density, the density of observations in each bin (percentage of total/bar width)
    #x, the center of the bin

#to map these produced variables to aesthetic dimension surround the variable with ..var.. as such 

ggplot(diamonds, aes(carat)) + geom_histogram(aes(y = ..density..), binwidth = 0.1) 

```

### bar plots

```{r,eval=FALSE}
mpg
s <- ggplot(mpg, aes(fl, fill = drv)) #count of fl stratified by category of drv 

s + geom_bar()

s + geom_bar(position = "stack") # counts the total of fl levels, and just shows proportion of each stratification wihtin that totall 

s + geom_bar(position = "dodge") #counts each stratification individually

s + geom_bar(position = "fill") #purely demon straights the proportion of the statification categoires within fl
```

### **box and whiskers**

```{r}
library(ggplot2)
data(iris)
ggplot(data = iris, aes(x = Species, y = Petal.Length, fill = Species)) + 
  geom_boxplot(position = "dodge") + 
  labs(y = "Petal Length", title = "Box Plot") 
```

### **violin plot**

```{r}
ggplot(data = iris, aes(x = Species, y = Petal.Length, fill = Species)) + 
  geom_violin(position = "dodge") + 
  labs(y = "Petal Length", title = "Violin Plot")
```

### **violin plot with box and whiskers inside it**

```{r}
ggplot(data = iris, aes(x = Species, y = Petal.Length, fill = Species)) + 
  geom_violin(trim = FALSE, position = "dodge") + 
  geom_boxplot(width = 0.20, position = position_dodge(width = 20), fill = "orange") +
  labs(y = "Petal Length", title = "Violin/Box") 
```

### **Ridge line**

```{r}
library(ggridges)
iris$Species <- factor(iris$Species,levels=c("setosa","versicolor","virginica"))
ggplot(data = iris, aes(x = Petal.Length, y = Species, fill = Species)) + 
  geom_hline(yintercept = c(1, 2, 3), color = "black", size = .6) +  
  geom_density_ridges(scale = 1.75, rel_min_height = 0.01) + 
  labs(x = "Petal Length",y="frequency", title = "Ridgeline Plot of Petal Length by Species") + 
  theme_bw() + theme(plot.title = element_text(face = "bold", size = 12),
  axis.title = element_text(vjust = 1, hjust = 1), 
  legend.background = element_rect(fill = "white", linewidth = 2, colour = "white"),
  panel.grid.minor = element_blank()) + expand_limits(y = c(0.5, 3.75))
```

### **pie chart**

```{r}
iris$Species <- factor(iris$Species,levels=c("versicolor","virginica","setosa"))
species_counts <- as.data.frame(table(iris$Species))
colnames(species_counts) <- c("Species", "Count")
ggplot(species_counts, aes(x = "", y = -Count, fill = Species)) +  
  geom_bar(stat = "identity", width = 1, color = "black") +  
  coord_polar("y") + theme_void()
```

## faceting

new plot by row for each color

```{r}
ggplot(data = diamonds, aes(x = carat)) + geom_histogram(bins=30) + facet_grid(rows = vars(color)) 
```

new plot by column for each color

```{r}
ggplot(data = diamonds, aes(x = carat)) + geom_histogram(bins=30) + facet_grid(cols = vars(color))
```

stratify by further variables, one category by row and other by columns

```{r}
ggplot(data = diamonds, aes(x = carat)) + geom_histogram(bins=30) + facet_grid(rows = vars(cut),cols = vars(color)) 
```

OPTION SCALES = FREE will make the scales fit the data of all facets, and all facets will have the same scales

#FOR FACET GRID, SCALES = FREE will completely decouple the facets scales from each other and each scale will fit only that individual facet.

```{r}
facet_grid(cols = vars(Species), scales="free")
```

**facet_wrap**

wrap will just make a new plot for each category and will wrap them like text to save space, it's just a list there isn't a dimensionality

it automatically printed them in order of trend line least to greatest

```{r}
library(nlme)
data(Oxboys)
ggplot(Oxboys, aes(age, height)) + geom_line() + facet_wrap(vars(Subject))
```

## grouping

new plot for each group within the same panel

```{r}
library(nlme)

# boys heights by age, plotted for each individual boy, otherwise the lines would connect different boys heights 
ggplot(Oxboys, aes(age, height, group = Subject)) + geom_line() 

ggplot(Oxboys, aes(age, height, group = Subject)) + geom_smooth() 
```

## customization

```{r}

library(ggplot2)
base <- ggplot(mpg, aes(cty, hwy, color = factor(cyl))) +
  geom_jitter() + 
  geom_abline(color = "grey50", linewidth = 2)

base


labelled <- base +
  labs(x = "City mileage/gallon",
       y = "Highway mileage/gallon",
       color = "Cylinders",
       title = "Highway and city mileage are highly correlated") +
       scale_color_brewer(type = "seq", palette = "Spectral")

labelled


styled <- labelled +
  theme_bw() + 
  theme(plot.title = element_text(face = "bold", size = 12),
    legend.background = element_rect(fill = "white", linewidth = 4, color = "white"),
    legend.justification = c(0, 1),
    legend.position = c(0, 1),
    axis.ticks = element_line(color = "grey70", linewidth = 0.2),
    panel.grid.major = element_line(color = "grey70", linewidth = 0.2),
    panel.grid.minor = element_blank())

```

# Module 13

## select() - selecting variables (columns)

```{r}
library(nycflights13)
library(tidyverse)
names(flights)
#you can subset variables like this
select(flights,year,month,day)

#you can selected ranges of variables (columns) without using the column number 
select(flights,year:day)


#and just do this for the compliment
select(flights,!(year:day))


```

selection functions for subsetting exactly what you want

```{r, eval=FALSE}

#starts_with(): Starts with a prefix.

#ends_with(): Ends with a suffix.

#contains(): Contains a literal string.

#matches(): Matches a regular expression.

#num_range(): Matches a numerical range like x01, x02, x03.

```

examples of how to use these selection functions

```{r}
select(flights, starts_with("dep"))

select(flights, ends_with("time"))

select(flights, contains("time"))

# this is or 
select(flights, starts_with("dep"), contains("time"), year:day)

# this is just the column numbers 
select(flights, c(1,3,5))

#select columns that are numbered but also have a prefix 
select(billboard, num_range("wk", 20:23))


```

## filter() - selecting observations of variables (rows)

```{r}
# use
# & and 
# | or 
# ! not 
```

```{r}
#filter flights departed on january 1st 
#all rows (observations) that satisfy the criteria
filter(flights, month == 1 & day == 1)

#flights that depart on November or December 
filter(flights, month == 11 | month == 12)

# all flights where the destination is IAH 
filter(flights, dest == "IAH")

# all flights with arrival delay less than or equal to 2 hours 
# or departure delay greater than 2 hours 

#only one condition needs to be satisfied to be included
filter(flights, arr_delay <= 120 | dep_delay > 120)
```

## mutate() - apply a transformation to a variable or make new one

```{r}
# from left to right future variables can use previous ones within the same statement 
mutate(flights, gain = arr_delay - dep_delay, hours = air_time/60, gain_per_hour = gain/hours)

```

### transmute()

```{r}
# same thing as filter but all variables other than the new ones just created will be removed
transmute(flights, gain = arr_delay - dep_delay, hours = air_time/60, gain_per_hour = gain/hours)

```

### variants of mutate() and transmute()

```{r}
#mutate_all() and transmute_all() affect every variable

#mutate_at() and transmute_at() affect variables selected with a character vector

#mutate_if() and transmute_if() affect variables selected with a predicate function (a function that returns TRUE or FALSE)
```

```{r}
head(iris,5)
#take the log() of all columns that include "Sepal" in their name and then assaign that to new tibble
mutated <- mutate_at(iris, vars(contains("Sepal")), log)

#you can specify to apply the function only to columns of specified data type 
mutated <- mutate_if(iris, is.numeric, log)
head(mutated)

#transmute would do the exact same thing just removal all other columns other than the ones that were transformed


#across() will apply a function to multiple different selection criteria 

```

## arrange() - reorder rows

```{r}
#reorder rows by numeric aescending value by default 
arrange(flights, year, month, day)

#or descending order
arrange(flights, desc(month))
```

## summarize() and group_by()

```{r}
#this will output a tibble with delay_mean which is the mean of the variable 
# dep_delay aswell output in the tibble delay_sd the standard dev of dep_delay
flights 

summarize(flights, delay_mean = mean(dep_delay, na.rm = TRUE), delay_sd = sd(dep_delay, na.rm = TRUE))
```

group_by() and summarize() together

```{r}
# will produce a group for each unique combination of the variables
by_day <- group_by(flights, year, month, day)

# takes that tibble of groups and evaluates the mean of variable dep_delay within each group 
summarize(by_day, delay = mean(dep_delay, na.rm = TRUE))

```

summarize functions

```{r}
# mean(), median(), sum(), min(), max(), sd(), var(), ... 
```

## pipe-line operator %\>%

```{r}
flights %>% 
  group_by(dest) %>% 
    summarize(count = n(), dist = mean(distance, na.rm = TRUE), delay = mean(arr_delay, na.rm = TRUE)) %>%
      filter(count > 20, dest != "HNL") %>% 
        ggplot(mapping = aes(x = dist, y = delay)) + 
          geom_point(aes(size = count), alpha = 1/3) + 
            geom_smooth(se = FALSE)
```

## more pipe-lines %\>%

```{r}
iris #remember you need to declare it as a tibble 
as_tibble(iris)

iris_grouped <- group_by(iris, Species) 
summarise_all(iris_grouped, mean)

#or you can just 

iris %>%
  group_by(Species) %>%
    summarise_all(mean)
    
#will output a summary of the observation groups where the summary is the mean of all columns within each group that start with Sepal in the variable name 
iris %>% 
  group_by(Species) %>%
    summarise(across(starts_with("Sepal"), mean))

# if you want the output from this for input to the next function in the pipeline to not be subject to previous grouping constraints for whatever reason. 
iris %>%
  group_by(Species) %>%
    summarise(across(starts_with("Sepal"), mean), .groups = "drop")


```

```{r,eval=FALSE}

data(HSB)
# convert data frame to tibble 
HSB <- as_tibble(HSB)
HSB

# summary of data types 
str(HSB)

# the function n() will give you the number of observations of each of the groups passed from the previous function in the pipe-line
HSB %>%
  group_by(race) %>%
    summarise(n=n())

# count() will tell you how many observations there are of each of the possible unique values within the variable
HSB %>% 
  count(race)

# how many observations of each unique combination of variable values 
HSB %>%
  count(race, gender)

```

example of filtering and then summarizing

```{r eval=FALSE, include=FALSE}
#means of all of these variables for all students
HSB %>% 
  summarise(readm = mean(read), writem = mean(write), mathm = mean(math), ssm = mean(ss))

# for each gender 
HSB %>% 
  group_by(gender) %>%
    summarise(readm = mean(read), writem = mean(write), mathm = mean(math), ssm = mean(ss))

# for each gender and race 
HSB %>% 
  group_by(gender,race) %>%
    summarise(readm = mean(read), writem = mean(write), mathm = mean(math), ssm = mean(ss))

# for each gender evaluate the mean of all numeric variables
HSB %>% 
  group_by(gender) %>%
    summarise(across(where(is.numeric), mean, na.rm = TRUE))


# the means of all numeric variables by ses for only asian and hispanic
HSB %>%
  filter(race == "hispanic" | race == "asian") %>%
    group_by(ses) %>%
      summarise(across(where(is.numeric), mean, na.rm = TRUE))

```

```{r}
library(dplyr)
data(starwars)
glimpse(starwars) # similar to str() in Base R

#a. How many humans are contained in starwars overall? (Hint. use count())
starwars %>%
  filter(species == "Human") %>%
    summarise(observations = n())

#b. How many feminine humans are contained in starwars?
starwars %>%
  filter(species == "Human" & gender == "feminine") %>%
    summarise(observations = n())


#c. From which homeworld do the most individuals come from?
starwars %>%
  group_by(homeworld) %>%
    summarise(observations = n()) %>%
      filter(observations == max(observations))

  
#d. What is the mean height of all individuals with orange eyes from the most popular homeworld?
starwars %>%
  filter(eye_color == "orange" & homeworld == "Naboo") %>%
    summarise(mean_height = mean(height))

```

## tibble joins

```{r}
library(tidyverse)
# tribble() creates a tibble
x <- tribble(
  ~key, ~val_x,
     1, "x1",
     2, "x2",
     3, "x3"
)

y <- tribble(
  ~key, ~val_y,
     1, "y1",
     2, "y2",
     4, "y3"
)
```

```{r}

  #  inner_join() keeps observations that appear in both data frames
  #  left_join() keeps all observations in x
  #  right_join() keeps all observations in y
  #  full_join() keeps all observations in x and y

```

inner join - retains all variables which correspond to matching keys of both sets

```{r}
inner_join_1<- x %>% 
  inner_join(y, by = "key")

inner_join_1

#equivalently 
inner_join_2 <- inner_join(x, y, by = "key")
```

left join - retains all variables which correspond to the keys of x

```{r}
left_join <- x %>% 
  left_join(y, by = "key")
left_join

#equivalently 
left_join(x, y, by = "key")
```

right join - retains all variables which correspond to the keys of y

```{r}
right_join <- x %>%
  right_join(y,by = "key")
right_join

#equivalently
right_join(x, y, by = "key")
```

full join - retains all variables which correspond to all keys present in both sets

```{r}
full_join <- x %>%
   full_join(y,by="key")
full_join

#equivalently
full_join(x, y, by = "key")
```

## tibble reshape and separate()

```{r}

 #   pivot_longer() lengthens data, increasing the number of rows and decreasing the #number of columns (i.e., turning columns into rows)
    
    
 #   pivot_wider() widens data, increasing the number of columns and decreasing the #number of rows (i.e., turning rows into columns)
    
    
 #   separate() separates a character column into multiple columns with a regular #expression or numeric locations
# as in a column containing ab-01 becomes two columns ab   01 
   
    
  #   unite() unites multiple columns into one by pasting strings together
      # the reverse of the previous 

```

```{r}
iris <- as_tibble(iris)

iris

# wide to long 
iris %>%
     pivot_longer(cols = -Species, names_to = "Measurements", values_to = "Values")


# seperate 
iris %>%
     pivot_longer(cols = -Species, names_to = "Measurements", values_to = "Values") %>%
          separate(col = Measurements, into = c("Part", "Measure")) #it registered the . as the seperator

# now that you've organized the data this way you can easily produce this complex plot 
iris %>%
  pivot_longer(cols = -Species, names_to = "Measures", values_to = "Values") %>%
  separate(col = Measures, into = c("Part", "Measure")) %>%
  ggplot(aes(x = Species, y = Values, color = Part)) + 
    geom_jitter() + 
    facet_grid(cols = vars(Measure)) + 
    theme_minimal()

```

wide to long

```{r}
table <- as_tibble(table4a)
table

long <- table %>%
  pivot_longer(cols=c("1999","2000"), names_to = "Year", values_to = "cases" )

long
```

```{r}
billboard

long <- billboard %>%
  pivot_longer(cols = wk1:wk76, names_to = "Week", values_to = "Rank", values_drop_na = TRUE)
long
```

long to wide

```{r}
table2 <- as_tibble(table2)

table2

#pivot_wider(data, names_from, values_from)

  #  names_from = A string specifying the name of the column to get the name of the output column
  #  values_from = A string specifying the name of the column to get the cell values from


wide <- table2 %>%
  pivot_wider(names_from = c("type"),values_from = "count")

wide
```

separate

```{r}
table3

#by default any non alphanumeric char will be registered as the seperator

table3 %>%
  separate(rate, into = c("cases","population"))

# or explicitly sepcify the seperator
table3 %>%  separate(rate, into = c("cases", "population"), sep = "/")

#convert from char to the data type it's best suited 
table3 %>%   
  separate(rate, into = c("cases", "population"), convert = TRUE)

# or you can specify by location, posative is left to right and sep = -2 is right to left 
table3 %>%   
  separate(year, into = c("century", "year"), sep = 2)

```

unite

```{r}
table5

table5 %>%
  unite(combined_column, century, year) # wil use _ by default 


table5 %>%
  unite(combined_column, century, year, sep = "") #specify none
```

## module 13 homework

```{r}
#setwd("C:/Users/jake pc/Desktop/Personal_save/Stat_405_Final_Exam")
setwd("/Users/jacobrichards/Desktop/Personal_save/Stat_405_Final_Exam")
bike <- read.csv(file="Bike_Lanes.csv",header=TRUE)
road <- read.csv(file="roads.csv",header=TRUE)
crash <- read.csv(file="crashes.csv",header=TRUE)
wide <- read.csv(file="Bike_Lanes_Wide.csv",header=TRUE)
```

Reshape wide using pivot_longer. Call this data long. Make the key lanetype, and the value\
the_length. Make sure we gather all columns but name, using -name. Note the NAs here

```{r}
wide
library(tidyverse)

long <- wide %>%
    pivot_longer(cols = -name, names_to = "lanytype", values_to = "the_length",values_drop_na = TRUE)
long
```

## str_replace()

Replace (using str_replace) any hyphens (-) with a space in crash\$Road. Call this data crash2.\
Table the Road variable.

```{r}
# reassaigns the Road variable with the output of str_replace
crash2 <- crash %>% mutate(Road = str_replace(Road, "-", " "))
table(crash2$Road)
```

Separate the Road column (using separate) into (type and number) in crash2.

Reassign this to\
crash2. Table crash2\$type. Then create a new variable calling it road_hyphen using the unite\
function. Unite the type and number columns using a hyphen (-) and then table road_hyphen.

```{r}
crash2 <- crash2 %>%
     separate(col=Road, into = c("type", "number"))

table(crash2$type)



crash2 <- crash2 %>%
          unite(road_hyphen, type, number, sep = "-")

table(crash2$road_hyphen)
```

Keep rows where the record is not missing type and not missing name and re-assign the output to bike.

```{r}
#in a tibble empty spaces you can index as (" ") 


bike <- bike %>%
          filter(name != " " & type != " ")

bike <- bike %>%
          filter(!is.na(name) & !is.na(type))

```

Summarize and group the data by grouping name and type (i.e, for each type within each name) and take the sum of the length (reassign the sum of the lengths to the length variable). Call this data set sub.

```{r}
sub <- bike %>%
  group_by(name,type) %>%
    summarise(length = sum(length)) #mutate cannot change dimensionality of the tibble, summarize will reduce 
                                    #the tibble to one row for each group 
sub
```

Reshape sub using pivot_wider. Spread the data where the key is type and we want the value in the new columns to be length - the bike lane length. Call this wide2. Look at the column names of wide2 - what are they? (they also have spaces).

```{r}
#pivot_wider(data, names_from, values_from)
#in her problems when she says "key" she means the variable, the thing we are measuring  
wide2 <- sub %>%
            pivot_wider(names_from = type, values_from = length)
```

Join data to retain only complete data, (using an inner_join) e.g., those observations with road lengths and districts. Merge without using by argument, then merge using by = "Road". call the output merged. How many observations are there?

```{r}
merged <- inner_join(road,crash)
merged
```

```{r}
full <- full_join(road,crash)
(nrow(full))
```

```{r}
left <- left_join(road,crash)
(nrow(left))
```

```{r}
right <- right_join(road,crash, by="Road") #in case there are more than 1 common variables but you want to use one as the key instead of the other. 
(nrow(right))
```

# Module 14

## CHECK NORMALITY FIRST

## t-tests, Wilcox - one sample

remember, you always have to verify normality to know weither to continue with the t-test or if you have to resort to Wilcox

```{r}
qqnorm(trees$Height)
qqline(trees$Height, lty = 2)
```

this looks a little suspicious, lets check the Shapiro test

```{r}
shapiro.test(trees$Height)
```

fail to reject the null hypothesis that it's normal.

one sample - two sided

```{r}
data(trees)
t.test(trees$Height, mu = 70)
```

one sample - one sided

```{r}
t.test(trees$Height, mu = 70,alternative = "greater") # sample is greater than mu
t.test(trees$Height, mu = 70,alternative = "less")  # sample is less than mu 
```

change the confidence level on the mean confidence interval

```{r}
t.test(trees$Height, mu = 70,conf.level = 0.99)
```

```         
99 percent confidence interval:
 72.85287     79.14713
```

whenever normality cannot be assumed use Wilcox rank sum test

the function call is exactly the same as for t-tests

```{r}
wilcox.test(trees$Height, mu = 70)
wilcox.test(trees$Height, mu = 70, alternative = "greater") # sample is greater than mu
wilcox.test(trees$Height, mu = 70, alternative = "less")  # sample is less than mu 
```

## t-tests, Wilcox - 2 sample independent - Variance test

A two-sample t-test is used to test the null hypothesis that the two samples come from distributions with the same mean (i.e., the means are not different).

**DATA HAS TO BE IN LONG FORMAT - might have to clean data**

```{r}
setwd("/Users/jacobrichards/Desktop/Personal_save/Stat_405_Module_14/Lab_14.1")
test <- read.csv(file="hypothesis.csv",header=TRUE)

test$id <- seq(1:nrow(test))
test
```

check normality

```{r}
qqnorm(test$Aspirin)
qqline(test$Aspirin, lty = 2)

qqnorm(test$Tylenol)
qqline(test$Tylenol, lty = 2)

#they're normal proceed with t-test
```

```{r}
library(reshape2)
long_data <- melt(data = test, id.vars = c("id"),
                   measured.vars = c("Aspirin", "Tylenol"),
                   variable.name = "Brand", value.name = "Time")
long_data
```

```{r}
var.test(long_data$Time ~ long_data$Brand)
#H0: variances are equal
#H1:  true ratio of variances is not equal to 1
# p-value = 0.46 
```

```{r}
t.test(long_data$Time ~ long_data$Brand, var.equal=TRUE) #default variances are not equal
```

notice that the 95% confidence interval for difference of means from the test contains zero verifying the result of fail to reject null hypothesis

```         
95 percent confidence interval:
 -1.529204 21.195871
```

## **2 sample independent t-test - Another example**

H0: the two samples come from distributions with the same mean

Example:

For example, a study was conducted to test whether ‘seeding’ clouds with dimethylsulphate alters the moisture content of clouds. Ten random clouds were ‘seeded’ with a further ten ‘unseeded.’ The dataset can be found in the `atmosphere.txt` data file located in Canvas.

```{r}
#setwd(file="~/Desktop/Personal_save/Stat_405_Final_Exam")
atmos <- read.table(file="atmosphere.txt",header=TRUE)
atmos
t.test(atmos$moisture ~ atmos$treatment)
```

check variances and normality

```{r}
var.test(atmos$moisture ~ atmos$treatment)
#H0: the varainces are equal, p-value = 0.4282 --> fail to reject null --> the variances are equal 
```

if you happen to be sure that the variances of the samples are equal

```{r}
t.test(atmos$moisture ~ atmos$treatment, var.equal = TRUE) #so in this context 
```

just the same you can use Wilcox for 2 sample tests for data you cant assume normality on

```{r}
wilcox.test(atmos$moisture ~ atmos$treatment)
```

## 2 sample independent t-test and Wilcox upper tailed

3.) Two friends play a computer game and each of them repeats the same level 10 times.

this is an **independent** two-sample t-test because the thing we are measuring and testing for difference of mean is two different people. The experimental unit is distinct.

```{r}
setwd("~/Desktop/Personal_save/Stat_405_Module_14/Module_14_Homework")
scores <- read.table(file="scores.txt",header=TRUE)

library(tidyverse)
long <- scores %>%
            pivot_longer(cols = X1:X10, names_to = "trials", values_to = "scores") %>%
                select(-trials)

long$ID <- factor(long$ID, levels = c("Player1","Player2"), labels = c("Player1","Player2"))
```

two sided t-test, unpaired, variance not equal

check normality and variance beforehand

```{r}
scores <- t(as.matrix(scores))
colnames(scores) <- scores[1,]
scores <- as_tibble(scores[-1,])
library(dplyr)

scores <- scores %>%
  mutate(across(everything(), as.numeric))


shapiro.test(scores$Player1)
shapiro.test(scores$Player2)

t.test(long$scores ~ long$ID, var.equal=FALSE)
```

Fail to reject null —\> players means are no different.

Upper tailed Wilcox

```{r}
wilcox.test(long$scores ~ long$ID, alternative = "greater")
```

Reject null —\> Player 1 is better than Player 2

## t-test, Wilcox paired

**DATA HAS TO BE IN WIDE FORMAT**

**DO NOT NEED TO CHECK IF VARIANCES ARE EQUAL**

Paired data are where there are two measurements on the same experimental unit (individual, site, etc.) For example, the `pollution` dataset gives the biodiversity score of aquatic invertebrates collected using kick samples in 17 different rivers. These data are paired because two samples were taken on each river, one upstream of a paper mill and one downstream. Each individual observation pair originated from the same place.

example:

```{r}
library(tidyverse)
pollution <- read.table('pollution.txt', header = TRUE)
pollution <- as_tibble(pollution)
pollution

t.test(pollution$down, pollution$up, paired = TRUE)

wilcox.test(pollution$down, pollution$up, paired = TRUE)
```

example:

" is there a difference in the test scores " —\> two-sided

```{r}
setwd("~/Desktop/Personal_save/Stat_405_Module_14/Module_14_Homework")
HW_6 <- read.csv(file="Homework_6.csv",header=TRUE)
HW_6

t.test(HW_6$Test.1, HW_6$Test.2, paired = TRUE)
```

Was there improvement over the course of the semester. H0: Test 2 - Test 1 \> 0

```{r}
t.test(HW_6$Test.2, HW_6$Test.1, paired = TRUE, alternative = "greater")
```

## Wilcoxon test

```{r}
library(coin)
wilcox_test(Time ~ Brand, data = long_data, distribution = "exact")
```

You use an Exact Wilcoxon-Mann-Whitney Test, also known as the "Exact Mann-Whitney U test," **when comparing two independent groups on a continuous or ordinal variable, especially when the data is not normally distributed, has a small sample size, or when you want a precise calculation of the p-value, particularly when dealing with very small sample sizes where the normal approximation might not be reliable**. 

## Proportion test

advertising campaign counts of those would and would not buy are taken before and after - this is a proportions test - they applied a treatment

if they had ran a campaign, then sampled people and tabulated who did and did not by and of which who did or did not see the advertisement, then it would be a contingency table (odds ratio)

Are the proportions significantly different? H0: P1 - P2 = 0

+:-----------------------------:+:-------------------------:+:---------------------:+
|                               | before = **no treatment** | after = **treatment** |
+-------------------------------+---------------------------+-----------------------+
| would buy = **+ outcome**     | 45                        | 71                    |
+-------------------------------+---------------------------+-----------------------+
| would not buy = **- outcome** | 35                        | 32                    |
+-------------------------------+---------------------------+-----------------------+

```{r}
buy <- c(45,71)                     # creates a vector of positive outcomes
total <-c((45 + 35), (71 + 32))   # creates a vector of total numbers
prop.test(buy, total)   
```

There is no evidence to support that the advertising campaign has changed cat owners' opinions of cat food (*p* = 0.107)

## Chi-square

Test whether the number of cat owners buying the cat food is independent of the advertising campaign. This is not a contingency table, the company ran the campaign.

same thing but perform a chi-square test for independence rather than difference of proportion, in this when 2x2 those are the same thing

```{r}
buyers <- matrix(c(45, 71, 35, 32), nrow = 2, byrow=TRUE)
chisq.test(buyers)
```

you can also use the chisq.test function on raw un-tabulated data

```{r}
data <- read.csv(file="homework_5_data.csv",header=TRUE)
data <- as_tibble(data)
data <- table(data)
data
chisq.test(data)
```

Fail to reject the null hypothesis that the variables are independent.

## Contingency Table (odds ratio)

**set continuity correct = false** (correct = FALSE)

A random sample of 90 adults is classified according to gender and the number of hours of television watched during a week:

They did not apply any treatment —\> contingency table (odds ratio)

Use a 0.01 level of significance and test the hypothesis that the time spent watching television is independent of whether the viewer is male or female.

```{r}
table <- matrix(data=c(15,29,27,19),nrow=2,ncol=2,byrow=TRUE,dimnames = list(c("Over 25 hours", "Under 25 hours"),c("Male", "Female")))
table <- t(table)
table
chisq.test(table, correct = FALSE)
```

The p-value obtained is 0.01934 which is greater than 0.01, we therefore fail to reject the null hypothesis that time spent watching television is independent of whether the viewer is male or female.

## correlation coefficients and plots

Pearson’s product-moment correlation coefficient between two continuous variables is the default correlation coefficient in the function

these values are only meaningful under the assumption of linear relationships

```{r}
cor(trees$Height, trees$Volume) #select specific variables 
cor.test(trees$Height, trees$Volume) #test that they are statistically signifignat

cor(trees) #or produce a matrix of the coef's 

cor(trees, use = "complete.obs") #if theres is an NA 

setwd("~/Desktop/Personal_save/Stat_405_Module_14/Lab_14.2")
times <- read.csv(file="Game_Times.csv",header=TRUE)

# when you need to plot it 
pairs(times[,c(4,5,6,11)])
pairs(times[,c(7,8,9,11)])
pairs(times[,c(10,12,13,11)])
pairs(times[,c(14,11)])

cor(times[,4:14]) #or can just do this and then check that there is actually a linear relationship between the variables
```

### p-value of correlation coefficient

to test if these correlation coefficients are statistically significant you need to use.

```{r}
cor.test(trees$Height, trees$Volume)
```

non parametric options

```{r}
cor.test(trees$Height, trees$Volume, method = "spearman")
cor.test(trees$Height, trees$Volume, method = "kendall")
```

## Linear Models

```{r eval=FALSE, include=FALSE}

Bivariate regression	    Y ~ X (continuous)	                lm(Y ~ X) #regular linear regression

One-way ANOVA	            Y ~ X (categorical)	                lm(Y ~ X) #difference between group means 

Two-way ANOVA	            Y ~ X1 (cat) + X2(cat)            	lm(Y ~ X1 + X2) 

ANCOVA	                  Y ~ X1 (cat) + X2(cont)           	lm(Y ~ X1 + X2)

Multiple regression     	Y ~ X1 (cont) + X2(cont)          	lm(Y ~ X1 + X2)

Factorial ANOVA	          Y ~ X1 (cat) * X2(cat)	            lm(Y ~ X1 * X2) or lm(Y ~ X1 + X2 + X1:X2)

```

## linear regression

produce model

```{r}
setwd("~/Desktop/Personal_save/Stat_405_Module_14/Module_14_Homework")
smoke <- read.table(file="smoking.txt",header=TRUE) 
smoke_lm <- lm(mortality ~ smoking, data=smoke)
summary(smoke_lm) #check for significance of coeficient's 
anova(smoke_lm) #this is a condensed form of an anova table, signifigant p-value of F statistic - model is signifigant
```

```         
          Pr(>|t|)  
Intercept  0.901  --> fail to reject = 0 
smoking   5.66e-05 *** --> reject -> signifigant relationship


R^2 0.4918 
F-statistic:  p-value: 5.658e-05 --> the variable are not independent
```

plot with data and correlation coefficient

```{r}
slope_intercept <- coefficients(smoke_lm)
plot(mortality ~ smoking,data=smoke)
abline(a=slope_intercept[1],slope_intercept[2])
cor(y=smoke$mortality, x=smoke$smoking)
```

### prediction and confidence interval

```{r}
predicted_smoke_120 <- predict(smoke_lm, newdata = data.frame(smoking = 120))
predicted_smoke_120

CI_predicted_smoke_120 <- predict(smoke_lm, newdata = data.frame(smoking = 120), interval = "confidence", level = 0.98)
CI_predicted_smoke_120
```

### Verify Model

**Plot the model**

```{r}
library(ggplot2)
ggplot(mapping = aes(x = smoking, y = mortality), data = smoke) + geom_point() + geom_smooth(method = "lm", se = TRUE)
```

**1.) residuals by the response values produced by the model (fitted values)**

known as equal variances (homogeneity of variance)

```{r}
smoke_res <- resid(smoke_lm) 
smoke_fit <- fitted(smoke_lm) # this will extract the response values produced by the model

ggplot(mapping = aes(x = smoke_fit, y = smoke_res)) +
    geom_point() +
    geom_hline(yintercept = 0, colour = "red", linetype = "dashed")
```

**2.) residuals by known predictor values**

```{r}
ggplot(mapping = aes(x = smoke$smoking, y = smoke_res)) +
    geom_point() +
    geom_hline(yintercept = 0, colour = "red", linetype = "dashed")
```

No patterns, looks randomly and and evenly distributed. **check**

**3.) normality of residuals**

```{r}
qqnorm(smoke_res)
qqline(smoke_res) 
```

This actually does look kind of suspicious.

**4.) Studentized Residuals vs. Predictor observation number**

```{r}
plot(rstudent(smoke_lm) ~ seq(1,length(smoke$smoking),by=1))
```

more diagnostic plots

```{r}
par(mfrow = c(2,2))
plot(smoke_lm)
```

the first 2 are same as before

bottom left is first one but by a different scale

bottom right plots the points which have the greatest impact on the linear models coefficients

We may see that there is an extreme outlier point that weakens out model for the normal range of values that we would like to predict, it would benefit our model to just remove the point from the data points we produce the model from

```{r}
library(ggfortify)
autoplot(smoke_lm, which = 1:6, ncol = 2, label.size = 3)
```

Cook's distance is how much influence that point has on the coefficients of the model.

we notice that point 2 has more than double the influence than any other point, maybe it's and outlier and the the model would be more representative of the relationship of the variables without it.

### Update model from verification plots

so this one is probably hurting the model because it's far outside the data center (less accurate) and has high impact

```{r}
smoke_lm2 <- update(smoke_lm, subset = -2)
summary(smoke_lm2)
slope_intercept_2 <- coefficients(smoke_lm2)
plot(mortality ~ smoking, data=smoke)
abline(a=slope_intercept_2[1],b=slope_intercept_2[2],col="red")

slope_intercept_1 <- coefficients(smoke_lm)
abline(a=slope_intercept_1[1],b=slope_intercept_1[2],col="blue")


#R^2 before 0.4918 ---> after: 0.5622
```

### diagnostic plot for how much a point effects the model relative to it's error

```{r}
matplot(dfbetas(smoke_lm), type = "l", col = "black")

lines(sqrt(cooks.distance(smoke_lm)), lwd = 2)

axis(1, at = 1:nrow(dfbetas(smoke_lm)), labels = 1:nrow(dfbetas(smoke_lm)))
```

dfbetas() gives the change in the estimated parameters if an observation is excluded, relative to its standard error (intercept is the solid line, and slope is the dashed line in the example below). The solid bold line in the same graph represents the Cook’s distance. Examples of how to use these functions are given below.

## anova

linear model with categorical variable as predictor will compute anova test

Example: test is significant —\> reject null, the groups are different

```{r}
# this will evaluate an anova test
smoke_anova_lm <- lm(mortality ~ risk.group, data=smoke)

# this will print the anova table of the test 
anova(smoke_anova_lm)
```

reject the null hypothesis that the group means are equal.

this will give us more information on the results of this anova test.

```{r}
summary(smoke_anova_lm)
```

```{r eval=FALSE, include=FALSE}
Coefficients:
  
  Coefficients:
                 Estimate Std. Error t value Pr(>|t|)    
(Intercept)        135.00       5.25  25.713  < 2e-16 *** #reject null --> mean of high group != zero
risk.grouplow      -57.83       8.02  -7.211 3.16e-07 *** #reject null --> mean of low != high group
risk.groupmedium   -27.55       6.90  -3.992 0.000615 *** #reject null --> mean of medium != high group
  
  
  
                 Estimate Std. Error t value Pr(>|t|)    
(Intercept)        135.00       5.25  25.713  < 2e-16 *** #this is the first level of the factor, in this case risk.grouphigh (alpahebtical)
  # estimated mean mortality index for risk group high is 135,
  
   < 2e-16 *** #this is testing the null hypotheis that the mean mortality index of the high risk group is equal to zero. --> reject h0, not zero
  
  
risk.grouplow      -57.83       8.02  -7.211 3.16e-07 ***
# these estimate values are the estimated mean differences of it and the high risk group (whaterver one gets assaigned to the intercept) 
  # as in risk.grouplow - risk.grouphigh = -57.83 ---> 135 - 57.83 = 77.17  
  
  3.16e-07 *** # this is testing  H0: abs(risk.grouplow - risk.grouphigh) = 0 , 2 sided --> reject h0: they are different
  
  
risk.groupmedium   -27.55       6.90  -3.992 0.000615 *** # risk.groupmedium - risk.grouphigh = -27.55 --> 135 - 27.55 = 107.45
  
  0.000615 *** # this is testing  H0: abs(risk.groupmedium - risk.grouphigh) = 0 , 2 sided --> reject h0: they are different
  

```

Conclusion: the group means are in-equal, specifically, none of the groups are equal with any other group.

Example where the test is not significant

```{r, warning=FALSE}
setwd("~/Desktop/Personal_save/Stat_405_Module_14/Module_14_Homework")
plasma <- read.csv(file="plasma.csv",header=TRUE)
plasma$time <- factor(plasma$time,levels=c("8am", "11am", "2pm", "5pm", "8pm"), 
                      labels = c("8am", "11am", "2pm", "5pm", "8pm"))
plasma_model <- lm(plasma ~ time, data = plasma)
```

```{r}
anova(plasma_model)
```

Pr(\>F) = 0.1132 —\> fail to reject the null hypothesis that the group means are NOT different from each other.

```{r}
summary(plasma_model)
```

```         
(Intercept)  118.500      5.944  19.935   <2e-16 *** ---> reject null, 8AM group mean does NOT equal zero. 
```

```         
time11am       9.400      8.407   1.118    0.269 ---> fail to reject null --> conclude that 11 AM group mean is not different than 8AM
```

```         
time2pm        1.800      8.407   0.214    0.831 ---> fail to reject null --> conclude that 2 pm group mean is not different than 8AM
```

```         
time5pm      -13.700      8.407  -1.630    0.110 ---> fail to reject null --> conclude that 5 pm group mean is not different than 8AM
```

```         
time8pm        1.200      8.407   0.143    0.887 ---> fail to reject null --> conclude that 8 pm group mean is not different than 8AM
```

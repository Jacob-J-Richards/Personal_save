---
title: "Untitled"
output: html_document
date: "2024-12-03"
---

# MODULE 2

## data inputting

```{r eval=FALSE}
DATA jansales;
INPUT @1 Item $10. @18 Amount comma6.; 
DATALINES;
trucks           1,382
jeeps            1,235
Landrovers       2,391
;
RUN;
```

has \@ for start position, does not need colon

```{r, eval=FALSE}
DATA january_sales; 
  INPUT Item : $12. Amount : COMMA5.; 
  DATALINES; 
    Trucks 1,382 
    Vans 1,235 
    Sedans 2,391 
    SportUtility 987 
; 
RUN;
```

does not have starting postion, and is of variable length, stop once 12 chars read or until blank reached. Same for second variable

```{r, eval=FALSE}
DATA wghtclub;
  INPUT IDno comma5. Name $ 6-24 Team $ Strtwght Endwght;
  Loss = Strtwght-Endwght; /#Create a variable called Loss by computing the difference of Strtwght and Endwght*/
  CARDS;
1,023 David Shaw         red 189 165
1,049 Amelia Serrano     yellow 145 124
1,219 Alan Nance         red 210 192
1,246 Ravi Sinha         yellow 194 177
1,078 Ashley McKnight    red 127 118
;
RUN;
```

of same length, does not need colon

## delimited input data

```{r, eval=FALSE}
DATA SCORES1;   
  LENGTH Team $ 14;   
  ***INFILE DATALINES DLM=',';***
INPUT ...
DATALINES;
      Joe,11,32,76,Red Racers
; 
```

if datalines are delimited by something

## complex data inputting

```{r, eval=FALSE}
data factory;
input @1 factory_number 2. state $ 6-7 ID $ 1-7 @8 quantity 2. @10 price dollar7.;

datalines;
13AB2NY44 $123
22XXXCT88 $1,033
37123TX11$22,999
;

proc print data=factory; 
title "Jacob Richards Factory Data";
```

```{r, eval=FALSE}
data scores; 
infile datalines missover;
input SSN $ 1-11 score_1-score_8 2. @37 score_9 score_10;


datalines;
123-45-6789 100 98 96 95 92 88 95 98100 90
344-56-7234 69 79 82 65 88 78 78 92 66 77
898-23-1234 80 82 86 92 78 88 84 85 83
;

proc print data=scores; 
title "Jacob Richards Scores";
VAR SSN Score_1-Score_8 score_9 score_10;  
RUN; 
```

the 2. after score_1-score_8 is only have the last input variable have the 2. stopping condition

then because of the missover it reads a blank for the missing score when the score 10 was formatted as list input

## missing values in data input

```{r, eval=FALSE}
  DATA NEW;
  INFILE DATALINES MISSOVER;
  INPUT ID INIT $ GENDER : $1. AGE VAULT P_BAR;
  DATALINES;
3 LJ . . .  6.7
5 MK . 15 8.1 7.2
7 FR F  
9 BV M 11
;
RUN;
```

will assign missing values to missing variables at the end of datelines such as this example it assigns missing to missing values in last 2 lines

## dates

unless specifies start and length, use colon before the date format

```{r, eval=FALSE}
data mice_contagion;
input number date_of_birth : date9. date_of_contraction : date9. date_of_death : date9. group $;
...
datalines;
1 23MAY1990 23JUN1990 28JUN1990 A
2 21MAY1990 27JUN1990 05JUL1990 A
;
```

the colons are required for date informants in list input because dates are inherently variable length data types.

05JUL1990 would read in the same as 5JUL1990

```{r, eval=FALSE}
DATA HOSPITAL;
   INPUT @1   ID            $3.     /*This is called formatted input */
         @4   DOB     MMDDYY10.
         @14  ADMIT   MMDDYY10.
         @24  DISCHRG MMDDYY10.
         @34  DX             1.
         @35  FEE            5.;
/#A column pointer (@)first tells the program which column to start reading, 
follow by the variable name and a specification of what type of data we are reading, 
called an informat. All of our formats and informats end with periods#/

   LENGTH_STAY = DISCHRG-ADMIT + 1;
   AGE = ADMIT - DOB;
DATALINES;
00110/21/194612/12/200412/14/20048 8000
00205/01/198007/08/200408/08/2004412000
00301/01/196001/01/200401/04/20043 9000
00406/23/199811/11/200412/25/2004715123
;
```

## Basic Programming

```{r, eval=FALSE}
DATA IQ_AND_TEST_SCORES;
	INPUT ID 1-3 IQ 4-6 MATH 7-9 SCIENCE 10-12;
	overall=((IQ + MATH + SCIENCE)/3)/500;
	if IQ ge 0 and IQ lt 100 then group='1';
	else if IQ ge 101 and IQ le 140 then group='2';
	else if IQ gt 140 then group='3';
	
	DATALINES;
001128550590
002102490501
003140670690
004115510510
;

proc sort data=IQ_AND_TEST_SCORES;
	by IQ;

proc freq data=iq_and_test_scores;
	tables group;

proc print data=iq_and_test_scores;
	title "Jacob Richards data set in IQ order";
	var IQ;
run;
```

basic programming

```{r, eval=FALSE}
data labor_expenses;
	input employeeID 1-3 @4 salary job_class;
	bonus_jobclass1_calculation=salary*.10;
	bonus_jobclass2_calculation=salary*.15;
	bonus_jobclass3_calculation=salary*.20;

	if job_class=1 then
		bonus=bonus_jobclass1_calculation;
	else if job_class=2 then
		bonus=bonus_jobclass2_calculation;
	else if job_class=3 then
		bonus=bonus_jobclass3_calculation;
	new_salary=salary + bonus;
	datalines;
137  28000  1
214  98000    3
199 150000  3
355   57000   2
;

proc print data=labor_expenses;
	title "Jacob Richards Labor Expenses";
	var employeeID salary job_class bonus new_salary;

```

# Module 3

## reading in txt and setting variables manually

```{r, eval=FALSE}
data adults;
infile '/home/u63989204/Module 3/Module_3_labs/adults(1).txt' firstobs=2; #starts read at second line of file
input weight bloodp; 
run;

proc print data=adults; 
```

### delimeter option for txt file read in

```{r, eval=FALSE}
DATA HW4_2;
Infile '/home/u63989204/Module 4/Module_4_Homework/exchange rates.csv' DLM=',' FIRSTOBS=2;
Input Date : MMDDYY10. USD JPY EUR GBP AUD BRL CAD CNY DKK
HKD INR MYR MXN NOK NZD KRW SEK SGD LKR ZAR TWD THB;
Year=Year(Date);
run;

PROC Means data=HW4_2;
BY Year;
output out=AVG mean=; #prevents any other statistics from being calculated;
run;
```

## reading in data from spreadsheet

```{r, eval=FALSE}
proc import datafile = "/home/u63989204/Midterm 1/Module 3/Module_3_labs/Game_Times (1).xlsx" 
DBMS=XLSX  
out = imported_data_sheet replace;  
getnames=yes;  
options validvarname=v7;  
run;

proc sort data=imported_data_sheet; 
by descending Time_of_Game;


proc print data=imported_data_sheet (obs=1);  
format Date worddate.14;
var Date HomeTeam AwayTeam Time_of_Game; 
```

## reading in data from csv

```{r, eval=FALSE}
PROC IMPORT DATAFILE='/home/u63989204/Midterm 1/Module 4/Module_4_Homework/NBAPPG2008.csv'
 OUT=NBA
 DBMS=CSV REPLACE;
 getnames=yes;  
 options validvarname=v7;
run;
 
proc univariate data=NBA normaltest;
var G MIN PTS FGM FGA FGP FTM FTA FTP _3PM _3PA _3PP ORB DRB TRB AST STL BLK TO PF;
ods output TestsForNormality=normaltest;
run;
```

## programming lag (like diff() in R)

```{r, eval=FALSE}

DATA PATIENTS;
   INPUT @1  ID          $3.
         @4  DATE   MMDDYY8.
         @12 HR           3.
         @15 SBP          3.
         @18 DBP          3.
         @21 DX           3.
         @24 DOCFEE       4.
         @28 LABFEE       4.;
   FORMAT DATE MMDDYY10.;
DATALINES;
0071021198307012008001400400150
0071201198307213009002000500200
0090903198306611007013700300000
0050705198307414008201300900000
0050115198208018009601402001500
0050618198207017008401400800400
0050703198306414008401400800200
;

PROC SORT DATA=PATIENTS;
   BY ID DATE;   #Sort the data set in patient-date order
RUN;

DATA DIFFERENCE;
   SET PATIENTS;
   BY ID;
   DIFF_HR = HR - LAG(HR); 
   DIFF_SBP = SBP - LAG(SBP);
   DIFF_DBP = DBP - LAG(DBP);

   IF NOT FIRST.ID THEN OUTPUT; 
RUN;

PROC PRINT DATA = DIFFERENCE; 
RUN;
```

# Module 4

## means table, evaluating mean of variable by numerical value category

```{r, eval=FALSE}
Infile '/home/u63989204/Module 4/Module_4_Homework/exchange rates.csv' DLM=',' FIRSTOBS=2;
Input Date : MMDDYY10. USD JPY EUR GBP AUD BRL CAD CNY DKK
HKD INR MYR MXN NOK NZD KRW SEK SGD LKR ZAR TWD THB;
Year=Year(Date);
run;
PROC Means data=HW4_2;
BY Year;
output out=AVG mean=; #prevents any other statistics from being calculated;
run;
proc print data=AVG;
var year USD JPY EUR GBP AUD BRL CAD CNY DKK HKD INR
MYR MXN NOK NZD KRW SEK SGD LKR ZAR TWD THB;
run;
```

## normality test, inputted data

```{r, eval=FALSE}
data test_one;
input score;
datalines;
82
91
100
68
87
73
78
80
65
84
116
76
97
100
105
77
;

proc univariate data=test_one normal;
var score;
run;

##for all 4 normality tests, the p-value is above 0.05 and we therefore fail
#to reject the null hypothesis that the data set is normally distributed;
```

Null hypothesis: variable is normally distributed —-\> (p-value \< 0.05) —\> reject null hypothesis —\> variable is not normally distributed.

## Normality test - csv read in

```{r, eval=FALSE}
PROC IMPORT DATAFILE='/home/u63989204/Module 4/Module_4_Homework/NBAPPG2008.csv'
 OUT=NBA
 DBMS=CSV REPLACE;
 getnames=yes;  
 options validvarname=v7;
run;

#Test for normality;
proc univariate data=NBA normaltest;
var G MIN PTS FGM FGA FGP FTM FTA FTP _3PM _3PA _3PP ORB DRB TRB AST STL BLK TO PF;
ods output TestsForNormality=normaltest;
run;

#create 0/1 variable for rejection of hypothesis;
data normaltest;
set normaltest;
reject=(pValue<0.05);
run;

proc print data = normaltest;
where reject=0;
run;
```

scroll all the way down to the table of the variable table, reject = 0 (fail to reject) —-\> normally distributed

Null hypothesis: variable is normally distributed —\> (p-value \< 0.05) —\> reject null hypothesis —\> variable is not normally distributed.

```{r, eval=FALSE}
proc univariate data=NBA Normal;
var _3PM _3PA DRB;
run;
```

these variables have at least one test with a p-value less than 0.05 —\> reject null hypothesis —\> variables are not normally distributed

# Module 5 - SAS

## odds ratio

CASE VS CONTROL

both cause and effect already occurred independently, now we are just analyzing what happened.

Question: Is there evidence that the results of the entrance exam are related to race? Analysis: Perform a statistical test of association between row and column variables. Null Hypothesis: Exam results and race of examinee are independent. Alternative Hypothesis: Exam results and race of examinee are not independent.

the first row and the first column are the exposure and outcome categories we are testing for association.

```{r,eval=FALSE}
data entrance_exam;
input race $ RESULTS $ count;
datalines;
2-white PASS  5
2-white FAIL 3
1-other PASS 4
1-other FAIL 6
;

PROC FREQ DATA=entrance_exam;
     TITLE "Mantel-Haenszel Chi-square Test";
     TABLES race*RESULTS/all; /#ALL option use with the TABLES statement 
#requests tests and measures of association produced by CHISQ, MEASURES, and CMH options#/
  WEIGHT count;
RUN;

```

1.) check Chi-square for overall association between predictor and response

+--------------------------------------------------------+---------+-----------+----------+
| **Statistic**                                          | **DF**  | **Value** | **Prob** |
+:=======================================================+========:+==========:+=========:+
| **WARNING: 50% of the cells have expected counts less\ |         |           |          |
| than 5. Chi-Square may not be a valid test.**          |         |           |          |
+--------------------------------------------------------+---------+-----------+----------+
| **Chi-Square**                                         | 1       | 0.9000    | 0.3428   |
+--------------------------------------------------------+---------+-----------+----------+

p-value = 0.34 —\> fail to reject null that the variables are independent

2.) check odds ratio and confidence interval for odds ratio

```{r,eval=FALSE}

Odds Ratio and Relative Risks
Statistic               Value       95% Confidence Limits
Odds Ratio              2.5000      0.3701    16.8884
Relative Risk (Col 1)   1.6000      0.5725    4.4719
Relative Risk (Col 2)   0.6400      0.2526    1.6216

```

odds ratio contains 1 within it's upper and lower bounds —\> odds ratio is not greater than 1 with 95% confidence —\> fail to reject null hypothesis that the variables are independent.

## odds ratio with set confidence level

(Use SAS) A random sample of 90 adults is classified according to gender and the number of hours of television watched during a week: Use a 0.01 level of significance and test the hypothesis that the time spent watching television is independent of whether the viewer is male or female.

```{r,eval=FALSE}
data tv;
input gender $ GT_or_LT $ count; 
datalines; 
2-M 1-More 15   # we think that it will be higher for females so we put a 1 in front of the variable so that it will print in the first
2-M 2-Less 27   # row of the contigency table and align with the first column 1-More
1-F 1-More 29
1-F 2-Less 19
;

PROC FREQ DATA=tv; 
    TITLE "Jacob Richards - Mantel-Haenszel Chi-square Test with Odds Ratio"; 
    TABLES gender*GT_or_LT / CHISQ CMH OR CL ALPHA=0.01;  
    WEIGHT count;
RUN;
```

| **Statistic**  | **DF** | **Value** | **Prob** |
|:---------------|-------:|----------:|---------:|
| **Chi-Square** |      1 |    5.4702 |   0.0193 |

fail to reject the null hypothesis that the variables are independent

```{r,eval=FALSE}
Odds Ratio and Relative Risks
Statistic               Value       99% Confidence Limits
Odds Ratio              2.7474      0.8918    8.4641
Relative Risk (Col 1)   1.6917      0.9171    3.1206
Relative Risk (Col 2)   0.6157      0.3565    1.0636

```

odds ratio confidence interval at alpha = 0.01 contains 1 —\> insufficient evidence to conclude that odds ratio does not equal 1 —\> fail to reject null that variables are independent.

## Risk Ratio

*A randomized clinical trial compared aspirin to placebo for the prevention of heart attacks (Mis) and strokes. Out of a total of 1,000 subjects on aspirin, there were 80 heart attacks and 65 strokes;* out of a total of 2,000 subjects on placebo, there were 240 heart attacks and 165 strokes. is there a significant benefit for aspirin therapy for heart attacks and strokes? What is the RR for aspirin use for each of these two outcomes? (Careful here, you are given the total number of subjects in each group and the number of complications.);

rather than analyse the relative risk of having a heart complications or not analyse the risk ratio of treatment (yes or no) to having a heart attack (Y/N) and risk ratio of treatment (yes or no) to having a stroke (Y/N)

HEART ATTACK RR EVALUATION

```{r,eval=FALSE}
DATA heart_attacks;
	INPUT treatment $ heart_attack $ COUNT;
	DATALINES;
asprin YES 80 
asprin NO 920
placebo YES 240
placebo NO 1760
;

PROC FREQ DATA=heart_attacks;
	TITLE "realtive risk: treatment on heart attacks";
	TABLES treatment*heart_attack /CMH;
	WEIGHT COUNT;
RUN;
```

HEART ATTACKS - results

```{r,eval=FALSE}
Common Odds Ratio and Relative Risks
Statistic               Method            Value       95% Confidence Limits
Odds Ratio              Mantel-Haenszel   1.5682      1.2028    2.0446
                        Logit             1.5682      1.2028    2.0446
Relative Risk (Col 1)   Mantel-Haenszel   1.0455      1.0202    1.0713
                        Logit             1.0455      1.0202    1.0713
Relative Risk (Col 2)   Mantel-Haenszel   0.6667      0.5237    0.8487
                        Logit             0.6667      0.5237    0.8487


```

Column 1 given you are in row 1 (Asprin) your risk of not having a heart attack is greater than 1 for both upper and lower bounds

Column 2 given you are in row 1 (Asprin) your risk of having a heart attack is less than 1 for both upper and lower bounds

Therefore, there is a signifigant benefit of asprin thearapy for heart attacks

STROKES – RESULTS

```{r,eval=FALSE}
Common Odds Ratio and Relative Risks
Statistic               Method            Value       95% Confidence Limits
Odds Ratio              Mantel-Haenszel   1.2934      0.9605    1.7418
                        Logit             1.2934      0.9605    1.7418
Relative Risk (Col 1)   Mantel-Haenszel   1.0191      0.9979    1.0407
                        Logit             1.0191      0.9979    1.0407
Relative Risk (Col 2)   Mantel-Haenszel   0.7879      0.5974    1.0391
                        Logit             0.7879      0.5974    1.0391

```

Column 1 given you are in row 1 (Asprin) your risk of not having a stroke is not greater than 1 for the lower bound

Column 2 given you are in row 1 (Asprin) your risk of having a heart attack is not less than 1 for the upper bound

Therefore, there is a not signifigant benefit of asprin thearapy for strokes

## stratified odds ratio

```{r,eval=FALSE}
data colds;
	input smoker $ cold $ Tempature $ count;
	datalines;
yes 1-yes 1-poor 20
yes 1-yes 2-good 15
yes 2-no 1-poor 100
yes 2-no 2-good 150 
no 1-yes 1-poor 30 
no 1-yes 2-good 25
no 2-no 1-poor 100
no 2-no 2-good 200
;

PROC FREQ DATA=colds;
	TITLE "Jacob Richards - Mantel-Haenszel Chi-square Test";
	TABLES smoker*tempature*cold/ALL;
	WEIGHT COUNT;
RUN;
```

first check if the odds ratio for the stratification categories are significantly different

```{r,eval=FALSE}
Breslow-Day Test for Homogeneity of Odds Ratios
Statistic         Value
Chi-Square        0.1501
Degrees of Freedom (DF)   1
P-value (Pr > ChiSq)      0.6985

```

fail to reject the null hypothesis that the odds ratios for the categories are not different

so we can analyse the table all together

test if the odds ratios for both categories are equal to 1

```{r,eval=FALSE}
Cochran-Mantel-Haenszel Statistics (Based on Table Scores)
Statistic                         Alternative Hypothesis         DF    Value     Prob
1                                 Nonzero Correlation            1     12.4770   0.0004
2                                 Row Mean Scores Differ         1     12.4770   0.0004
3                                 General Association            1     12.4770   0.0004

```

reject null —\> the odds ratio for both categories is not 1.

check the value of the common odds ratio

```{r,eval=FALSE}
Common Odds Ratio and Relative Risks
Statistic               Method            Value       95% Confidence Limits
Odds Ratio              Mantel-Haenszel   2.2289      1.4185    3.5024
                        Logit             2.2318      1.4205    3.5064
Relative Risk (Col 1)   Mantel-Haenszel   1.9775      1.3474    2.9021
                        Logit             1.9822      1.3508    2.9087
Relative Risk (Col 2)   Mantel-Haenszel   0.8891      0.8283    0.9544
                        Logit             0.8936      0.8334    0.9582
```

upper and lower bounds are greater than 1 —\> with 95% the risk ratio is greater than 1 -\> there is association between poor temp and cold

## Chi-Square given counts

```{r,eval=FALSE}
data crimes; 
input district $ crime $ count; 
datalines; 
1 1-assault 162
1 2-Burglary 118
1 3-Larceny 451
1 4-Homicide 18
2 1-assault 310
2 2-Burglary 196
2 3-Larceny 996
2 4-Homicide 25
3 1-assault 258
3 2-Burglary 193
3 3-Larceny 458
3 4-Homicide 10
4 1-assault 280
4 2-Burglary 175
4 3-Larceny 390
4 4-Homicide 19
;

PROC FREQ DATA=crimes;
      TITLE "Jacob Richards -  Chi-square Test";
      TABLES district*crime/all; 
   WEIGHT COUNT;
RUN;
```

| **Statistic**  | **DF** | **Value** | **Prob** |
|:--------------:|:------:|:---------:|:--------:|
| **Chi-Square** |   9    | 124.5297  | \<.0001  |

reject null hypothesis that the variables are independent.

## Chi square given raw data

Ex:

```{r,eval=FALSE}
4 categories of predictor, 3 categories of response --> chi square test 
data structure:
  col1               col2
predictor_1       responce_2
predictor_2       responce_1
....
```

if given the raw data, use proc freq to find the frequencies of the variable category occurrences and run the chi square from that

```{r,eval=FALSE}
proc import datafile="/home/u63989204/Module 5/Module_5_Homework/Homework 5 data.xlsx"
dbms=xlsx 
out = work.hw_five replace;
getnames=yes;
option validvarname=v7;
run;

proc contents data=work.hw_five; 
run;  

data opinion;
set work.hw_five;
run;

proc freq data=opinion;
    tables GL*opinion/all; 
run;
```

## chi square for trend

```{r,eval=FALSE}
DATA TREND;
INPUT Alcohol $ Mal $ COUNT;
DATALINES;
0 N 17066
1 N 14464
2 N 788
3 N 126
4 N 37
0 Y 48
1 Y 38
2 Y 5
3 Y 1
4 Y 1
;
PROC FREQ DATA=TREND;
      TITLE "Chi-square Test for Trend";
   TABLES Alcohol*Mal / CHISQ;
   WEIGHT COUNT;
RUN;
```

RESULTS:

```{r,eval=FALSE}
# to test if there is a significant linear trend in proportions, use the “Mantel-Haenszel Chi-square."

Statistics for Table of Alcohol by Mal
  Statistic 	             DF 	Value   	Prob
Mantel-Haenszel Chi-Square 	1 	1.8278 	0.1764
```

## McNemar Test For Paired Data

Paired design: the same subject responds to a question under two different conditions (a dichotomous characteristic)

Ex: a subject is asked their opinion of smoking before and after an anti smoking ad

100 people, asked before and after given data table

```{r,eval=FALSE}
         	After
 	        -	  +
Before	-	32	15
 	      +	30	23
```

such that, 32 people were negative before and negative after.

```{r,eval=FALSE}
DATA MCNEMAR;
LENGTH AFTER BEFORE $ 1;
INPUT AFTER $ BEFORE $ COUNT;
FORMAT BEFORE AFTER $OPINION.;
DATALINES;
N N 32
N P 30
P N 15
P P 23
;
PROC FREQ DATA=MCNEMAR;
TITLE "McNemar's Test for Paired Samples";
TABLES BEFORE*AFTER / AGREE ;
WEIGHT COUNT;
RUN;
```

```{r,eval=FALSE}
      McNemar's Test
Chi-Square	DF	Pr > ChiSq
5.0000.  	1	     0.0253

therefore reject the null hypothesis that the variables are independent. 

```

# Module 6

```{r,eval=FALSE}
data balls;
input brand_age $ bounces;
datalines;
WNew 67 
WNew 72
WNew 74
WNew 82
WNew 81 
WOld 46
WOld 44
WOld 45
WOld 51
WOld 43 
PNEW 75
PNEW 76
PNEW 80
PNEW 72 
PNEW 73
POld 63
POld 62
POld 66
POld 62
POld 60
;
run;

PROC ANOVA DATA=balls;
  TITLE "Analysis of balls data";
  CLASS brand_age; 
  MODEL bounces = brand_age;
  MEANS brand_age / SNK ALPHA=0.05;
RUN;
```

ANOVA RESULTS:

+------------+----------+--------------------+-----------------+-------------+-------------+
| **Source** | **DF**   | **Sum of Squares** | **Mean Square** | **F Value** | **Pr \> F** |
+:==========:+:========:+:==================:+:===============:+:===========:+:===========:+
| **Model**  | 3        | 2910.600000        | 970.200000      | 60.73       | \<.0001     |
+------------+----------+--------------------+-----------------+-------------+-------------+

reject the null hypothesis that all of the means are equal

at least one of the group means is different from the others

**SNK RESULTS:**

the means:

P_new and W_new \> P_old \> W_old

and the means of P_new and W_new are not significantly different

## ANOVA formatted data input (cholesterol medication)

\*Two cholesterol-lowering medications (statins) and a placebo were given to each of 10 volunteers with total cholesterol readings of 240 or higher. After 6 weeks, the foUowing total cholesterol values were recorded Create a SAS data set by reading these data.;

the data is given such that each line is the data samples for each of the three groups

```{r,eval=FALSE}
data blood_med;
do group = 'Statin A','Statin B','Placebo';
	do subject = 1 to 10;
		input cholesterol @;
		output;
	end;
end;
datalines;
220 190 180 185 210 170 178 200 177 189
160 168 178 200 172 155 159 167 185 199
240 220 246 244 198 238 277 255 190 188
;

PROC ANOVA DATA=blood_med;
   TITLE "Jacob Richards - Analysis of Cholesterol Data";
   CLASS group; 
   MODEL cholesterol = group;
   MEANS group / SNK ALPHA=0.05; 
RUN;

```

**Anova test results**

+------------+----------+--------------------+-----------------+-------------+-------------+
| **Source** | **DF**   | **Sum of Squares** | **Mean Square** | **F Value** | **Pr \> F** |
+:===========+=========:+===================:+================:+============:+============:+
| **Model**  | 2        | 16258.46667        | 8129.23333      | 17.58       | \<.0001     |
+------------+----------+--------------------+-----------------+-------------+-------------+

H0 = The means are equal.

h1 = Not all the means are equal.

The test statistic obtained has a p value of less than 0.0001 which is less than 0.05 and is therefore statistically significant. We therefore reject the null hypothesis and tentatively conclude that at least one of the group means of cholesterol is significantly different than the others.

**SNK test results**

the means:

Placebo \> Statin_A and Statin_B

aswell that the means for statin A and Statin B are not significantly different.

## anova - SNK - contrast (study methods)

DATA SET:

```{r,eval=FALSE}
data study_methods;
do group = 'A','B';
	do student = 1 to 8;
		input score @;
		output;
	end;
end;
do group = 'C','D';
	do student = 1 to 7;
		input score @;
		output;
	end;
end;

datalines;
560 520 530 525 575 527 580 620
565 522 520 530 510 522 600 590 
512 518 555 502 510 520 516 
505 508 512 520 543 523 517
;
```

anova test

```{r, eval=FALSE}
PROC ANOVA DATA=study_methods;
  TITLE "Jacob Richards - Analysis of Reading Data";
  CLASS GROUP;  #this names the classification variables to be used in the model, in this case (A,B,C,D)
  MODEL score = GROUP; # dependent variable = independent variable 
  MEANS GROUP / SNK ALPHA=0.05; #runs test of the means of groups dependent variables which are the scores of each group
RUN;
```

anova results

+------------+----------+--------------------+-----------------+-------------+-------------+
| **Source** | **DF**   | **Sum of Squares** | **Mean Square** | **F Value** | **Pr \> F** |
+:===========+=========:+===================:+================:+============:+============:+
| **Model**  | 3        | 7607.18810         | 2535.72937      | 3.28        | 0.0366      |
+------------+----------+--------------------+-----------------+-------------+-------------+

SNK/CONTRAST

```{r,eval=FALSE}
PROC GLM DATA=study_methods;
   TITLE "Jacob Richards - Analysis of Reading Data - Planned Comparisons";
   CLASS GROUP; 
   MODEL score = GROUP;         # A B C D 
   MEANS PROGRAM / SNK ALPHA=0.05;
   CONTRAST 'A&B to C&D' GROUP   1 1 -1 -1; 
   CONTRAST 'A & B & C to D' GROUP   1 1 1 -3;
#remember to set the coefficient's in the order corresponding to their group's input pattern
RUN;
```

**SNK results**

inconclusive

**contrast results**

+------------------------+----------+-----------------+-----------------+-------------+-------------+
| **Contrast**           | **DF**   | **Contrast SS** | **Mean Square** | **F Value** | **Pr \> F** |
+:======================:+:========:+:===============:+:===============:+:===========:+:===========:+
| **A and B to C and D** | 1        | 7225.152381     | 7225.152381     | 9.36        | 0.0051      |
+------------------------+----------+-----------------+-----------------+-------------+-------------+
| **D to A B C**         | 1        | 2413.012158     | 2413.012158     | 3.13        | 0.0888      |
+------------------------+----------+-----------------+-----------------+-------------+-------------+

The means of A and B to C and D are significantly different.

The means of A and B and C to D are not significantly different.

equivalently, if not specified that you cannot reformat the given data then.

```{r, eval=FALSE}
DATA EXAM;
INPUT PROGRAM $ SCORE @@;
DATALINES;
A 560 A 520 A 530 A 525 A 575 A 527 A 580 A 620
B 565 B 522 B 520 B 530 B 510 B 522 B 600 B 590
C 512 C 518 C 555 C 502 C 510 C 520 C 516
D 505 D 508 D 512 D 520 D 543 D 523 D 517
;
RUN;
PROC GLM DATA=EXAM;
CLASS PROGRAM;
MODEL SCORE = PROGRAM;
MEANS PROGRAM / SNK ALPHA=0.05;
CONTRAST 'A AND B VS. C AND D' PROGRAM -1 -1 1 1;
CONTRAST 'A B AND C VS. D' PROGRAM -1 -1 -1 3;
RUN;
```

## anova - SNK - contrast : formatted data input (reading scores)

DATA:

```{r,eval=FALSE}
DATA READING;
   INPUT GROUP $ WORDS @@;
DATALINES;
X 700   X 850   X 820   X 640   X 920
Y 480   Y 460   Y 500   Y 570   Y 580
Z 500   Z 550   Z 480   Z 600   Z 610
;
```

**ANOVA TEST**

```{r,eval=FALSE}
PROC ANOVA DATA=READING;
   TITLE "Analysis of Reading Data";
   CLASS GROUP; /#The CLASS statement names the classification variables to be used in the model. Typical classification variables are TREATMENT, SEX, RACE, GROUP, and REPLICATION. The CLASS statement is required, and it must appear before the MODEL statement#/
   MODEL WORDS = GROUP; /# MODEL dependent variable = independent variables.#/
   MEANS GROUP / SNK ALPHA=0.05; /#Compute means of the dependent variables for any effect that appears on the right-hand side in the MODEL statement (GROUP).#/
/#SNK option performs the Student-Newman-Keuls multiple range test on all main effect means in the MEANS statement.#/
/#ALPHA= specifies the level of significance for comparisons among the means. By default, ALPHA=0.05. You can specify any value greater than 0 and less than 1.#/
RUN;
```

+------------+----------+--------------------+-----------------+-------------+-------------+
| **Source** | **DF**   | **Sum of Squares** | **Mean Square** | **F Value** | **Pr \> F** |
+:==========:+:========:+:==================:+:===============:+:===========:+:===========:+
| **Model**  | 2        | 215613.3333        | 107806.6667     | 16.78       | 0.0003      |
+------------+----------+--------------------+-----------------+-------------+-------------+

**CONTRAST TEST**

```{r,eval=FALSE}
PROC GLM DATA=READING;
   TITLE "Analysis of Reading Data - Planned Comparisons";
   CLASS GROUP; /#The CLASS statement names the classification variables to be used in the model.#/
   MODEL WORDS = GROUP;   
   CONTRAST 'X VS. Y AND Z' GROUP   -2 1 1; 
/#CONTRAST 'label' independent (CLASS) variable followed by a set of k coefficients (where k is the number of levels of the class variable)#/
   CONTRAST 'METHOD Y VS Z' GROUP   0 1 -1;
  #1. The sum of the coefficients must add to 0 
  #2. The order of the coefficients must match the alphanumeric order of the levels of the CLASS variable if it is not formatted.#/
RUN;
```

+-------------------+----------+-----------------+-----------------+-------------+-------------+
| **Contrast**      | **DF**   | **Contrast SS** | **Mean Square** | **F Value** | **Pr \> F** |
+:=================:+:========:+:===============:+:===============:+:===========:+:===========:+
| **X VS. Y AND Z** | 1        | 213363.3333     | 213363.3333     | 33.22       | \<.0001     |
+-------------------+----------+-----------------+-----------------+-------------+-------------+
| **METHOD Y VS Z** | 1        | 2250.0000       | 2250.0000       | 0.35        | 0.5649      |
+-------------------+----------+-----------------+-----------------+-------------+-------------+

# Module 9

## base functions

```{r,eval=FALSE}
my_seq3 <- rep(2, times = 10)   # repeats 2, 10 times
my_seq3
##  [1] 2 2 2 2 2 2 2 2 2 2


my_seq7 <- rep(c(3, 1, 10, 7), each = 3) # repeats each 
                                         # element of the 
                                         # series 3 times
my_seq7
##  [1]  3  3  3  1  1  1 10 10 10  7  7  7


#match - finds the location of the arguments of the first input in the second input
# Define two vectors
x <- c("a", "b", "c", "d")
table <- c("c", "a", "b", "e")

# Match positions of elements of x in table
match(x, table)
# Output: [1] 2 3 1 NA

# Define a vector
x <- c(1, 2, 3, 4)

cumsum(x)
# Output: [1]  1  3  6 10

cumprod(x)
# Output: [1]  1  2  6 24

```

# Module 10

## .txt read in

```{r}
flowers <- read.table(file = '/Users/jacobrichards/Desktop/Personal_save/Stat_405_Final_Exam/flower.txt', header = TRUE, sep = "\t", stringsAsFactors = TRUE) #only needs file name
```

## options for read.table

```{r}
dec = "," #if you're in europe 
na.strings = "*" #if they represent an empty element by somehing other than NA
sep = "\t" #tab delimeted 
sep = "," # for csv 
```

## flexible read in function

```{r}
library(readr)
# import white space delimited files

setwd("/Users/jacobrichards/Desktop/Personal_save/Stat_405_Final_Exam")

all_data <- read_table(file = 'seeiffwork.txt', col_names = TRUE) #be carefull about spaces in input
all_data #yes works
```

```{r,eval=FALSE}
# import comma delimited files
all_data <- read_csv(file = 'data/flower.txt')

# import tab delimited files
all_data <- read_delim(file = 'data/flower.txt', delim = "\t")

# or use
all_data <- read_tsv(file = 'data/flower.txt')
```

## factor - order

this will print the data frame rows ordered by low medium high of nitrogen

```{r}
flowers$nitrogen <- factor(flowers$nitrogen, 
                           levels = c("low", "medium", "high"))    
nit_ord <- flowers[order(flowers$nitrogen),]
nit_ord
```

## base table

```{r}
table(flowers$nitrogen, flowers$treat)
```

## stratified table-ing and functions applied to data frames

```{r}
# the right side of the ~ is all variables we would like to stratify and produce counts for 
xtabs(~ nitrogen + treat + block, data = flowers)
# will take care of data if not already factored 

ftable(xtabs(~ nitrogen + treat + block, data = flowers))

#evaluate the mean height of each category of nitrogen 
tapply(flowers$height, flowers$nitrogen, mean)
#the third argument is any function you would like, first is what it will be applied on, and second is the categories of division of the first variable 

tapply(flowers$height, flowers$nitrogen, sd)

# if there are NA's in the data and want to remove them 
tapply(flowers$height, flowers$nitrogen, summary, na.rm=TRUE)

# you can even pass lists of categories to stratify the variables to be evaluated by the function
tapply(flowers$height, list(flowers$nitrogen,flowers$treat), summary, na.rm=TRUE)
```

## aggregate

```{r,eval=FALSE}
#input variables for function - stratification categories - function to apply 
aggregate(flowers[, 4:7], by = list(nitrogen = flowers$nitrogen, treat = flowers$treat), FUN = mean)

# after formatting the dates column to be only year char you can do the rest of the problem in a single line 
means_by_year <- aggregate(mat[,2:23],by=list(year=mat[,1]),FUN=mean)

# and if we want to apply the previous to a subset we can do this 
aggregate(height ~ nitrogen + treat, FUN = mean, subset = flowers < 7, data = flowers)

aggregate(height ~ nitrogen + treat, FUN = mean, subset = block == "1", data = flowers)
```

# Module 11

## base histogram function

```{r,eval=FALSE}
hist(flowers$height)

max(flowers$height)
#if we want to set custom intervals for each frequency bars (more or less detailed)
max(flowers$height) #round up last point from the max 
intervals <- seq(0,18,1)
hist(flowers$height, breaks = intervals, main = "petunia height")

#proportion instead of frequency
hist(flowers$height, breaks = intervals, main = "petunia height",freq = FALSE)

#add a pdf approximation curve (density curve)
hist(flowers$height, breaks = intervals, main = "petunia height",freq = FALSE)
lines(density(flowers$height))
```

## boxplots (box and whiskers)

```{r,eval=FALSE}
boxplot(flowers$weight, ylab = "Weight")
#the thick black line in the middle is the median 
#upper 15 is the upper quartile (75% of data is less than 15)
#around 10 is the lower quartile (25% of the date is less than 10)
#the grey area of the box is the inter quartile range 
#the very upper line is the whiskers and points beyond that are potential unusual observations 
```

## box-plots stratified

```{r,eval=FALSE}
        #distribution of weight by levels of nitrogen, data = dataframe
boxplot(weight ~ nitrogen, data = flowers, ylab = "weight (g)", xlab = "nitrogen level")
#levels are stored in df as high,low,medium so that's the order in which the plots are printed

#change the order of the levels to this 
flowers$nitrogen <- factor(flowers$nitrogen, levels = c("low", "medium", "high"))
boxplot(weight ~ nitrogen, data = flowers, ylab = "weight (g)", xlab = "nitrogen level")
#and that's how they print now 

#we can also do combinations of stratification variables 
boxplot(weight ~ nitrogen * treat, data = flowers, 
         ylab = "weight (g)", xlab = "nitrogen level")

#reduce the font size of the group labels so they all fix on the x-axis 
boxplot(weight ~ nitrogen * treat, data = flowers, 
         ylab = "weight (g)", xlab = "nitrogen level", 
         cex.axis = 0.7)
```

## violin plots

```{r,eval=FALSE}
library(vioplot)
#weight stratified by levels of nitrogen
vioplot(weight ~ nitrogen, data = flowers, ylab = "weight (g)", xlab = "nitrogen level", col = "lightblue")
#the white circle is the median value 
```

## dotcharts

```{r,eval=FALSE}
#on the x-axis is values of height, the y-axis is order that those values are in data frame 
#(bottom is beginning of data frame, top is end of data frame)
dotchart(flowers$height)
```

## dot-charts stratified

```{r}
#dotchart(flowers$height, groups = flowers$nitrogen)
```

## pairs plots (correlation matrix)

Variable in the row is the y-axis variable and variable in the column is the x-axis

```{r}
pairs(flowers[, c("height", "weight", "leafarea", "shootarea", "flowers")],panel = panel.smooth)
#pairs(flowers[, 4:8]) #equivalently
#kernel smoother panel option which is the red line 
```

## pairs options, correlation, histogram, smooth

```{r}
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    usr <- par("usr")
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r)
}
pairs(flowers[, c("height", "weight", "leafarea", "shootarea", "flowers")],panel = panel.cor)

# or if we only want either the upper or lower diagonal to be corr coef's 


pairs(flowers[, c("height", "weight", "leafarea", "shootarea", "flowers")],lower.panel = panel.cor)
# or upper.panel

panel.hist <- function(x, ...)
{
    usr <- par("usr")
    par(usr = c(usr[1:2], 0, 1.5) )
    h <- hist(x, plot = FALSE)
    breaks <- h$breaks; nB <- length(breaks)
    y <- h$counts; y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)
}


# upper, lower, diagonal
pairs(flowers[, c("height", "weight", "leafarea", 
                "shootarea", "flowers")], 
                 lower.panel = panel.cor,
                 diag.panel = panel.hist,
                 upper.panel = panel.smooth)
```

## coplots

```{r}
#flowers as a function of weight, examined at different intervals of leaf area (stratify by quantatative)
coplot(flowers ~ weight|leafarea, data = flowers)
#panels from bottom left to top right correspond to bars of equivalent potion in the leaf area interval box 

#if you don't want any of the panels to overlap 
coplot(flowers ~ weight|leafarea, data = flowers, overlap = 0)

# can also condition by categorical variables 
coplot(flowers ~ weight|nitrogen, data = flowers)

# and even all combinations of more than 1 either categorical or numeric variables 
coplot(flowers ~ weight|nitrogen * treat, data = flowers)

```

## coplot with options

```{r}
coplot(flowers ~ weight|nitrogen * treat, data = flowers,
        panel = function(x, y, ...) {
        points(x, y, ...)
        abline(lm(y ~ x), col = "blue")})
```

## latice

```{r}
#everything in lattice uses response ~ predictor notation
library(lattice)
#histogram
histogram(~ height, type = "count", data = flowers)

#box plot (box and whiskers)
#weight stratified by nitrogen level
bwplot(weight ~ nitrogen, data = flowers)

#                lattice.                    R.  
#scatterplot    xyplot()	                  plot()
#frequency      histogram(type = "count")	hist()
#boxplot	       bwplot()	                  boxplot()
#Cleveland     dotplot	dotplot()	          dotchart()
#scatterplot   matrix	splom()	            pairs()
#conditioning  plot	xyplot(y ~ x | z)	    coplot()
```

## lattice plots in multiple panels, layout options

```{r}
library(lattice)
#height conditional on nitrogen level’. 
histogram(~ height | nitrogen, type = "count", data = flowers)

#layout = columns by rows 
histogram(~ height | nitrogen, type = "count", layout = c(1, 3), data = flowers)

#factor within the plot function call 
#will not change the values in the data frame 
bwplot(weight ~ nitrogen | factor(block), data = flowers)

#multiple categories of stratification
xyplot(height ~ weight | nitrogen * treat, data = flowers)

#stratify by multiple categories and display by another category 
xyplot(flowers ~ shootarea | nitrogen * treat, 
        groups = block, auto.key = TRUE, data = flowers)

```

## base r plot customization

```{r}
#margins par(mar = c(bottom, left, top, right)
par(mar = c(4.1, 4.4, 4.1, 1.9), xaxs = "i", yaxs = "i") #makes the axis better 
plot(flowers$weight, flowers$shootarea, 
       xlab = "weight (g)",
       ylab = expression(paste("shoot area (cm"^"2",")")), #superscript in the y label 
       xlim = c(0, 30), ylim = c(0, 200), bty = "l", # x-lim and y-lim # gets rid of box that surrounds entire plot 
       las = 1, cex.axis = 0.8, tcl = -0.2, #makes y axis labels horizontal, makes them smaller, make tic marks shorter
       pch = 16, col = "dodgerblue1", cex = 0.9) #plotting symbol used, color, and size of it 
text(x = 28, y = 190, label = "A", cex = 2) #coordiates of text, the text, and size of the text 
par(mar = c(4.1, 4.4, 4.1, 1.9), xaxs="i", yaxs="i")
points(x = flowers$weight[flowers$nitrogen == "low"],
       y = flowers$shootarea[flowers$nitrogen == "low"],
       pch = 16, col = "deepskyblue")
points(x = flowers$weight[flowers$nitrogen == "medium"],
       y = flowers$shootarea[flowers$nitrogen == "medium"],
       pch = 16, col = "yellowgreen")
points(x = flowers$weight[flowers$nitrogen == "high"],
       y = flowers$shootarea[flowers$nitrogen == "high"],
       pch = 16, col = "deeppink3")
text(x = 28, y = 190, label = "A", cex = 2)
leg_cols <- c("deepskyblue", "yellowgreen", "deeppink3")
leg_sym <- c(16, 16, 16)
leg_lab <- c("low", "medium", "high")

legend(x = 1, y = 200, col = leg_cols, pch = leg_sym, 
        legend = leg_lab, bty = "n", 
        title = "Nitrogen level")
```

## base R layout for multiple plots customized

```{r}
# matrix(c(topleft,topright,bottomleft,bottomright))

layout_mat <- matrix(c(2, 0, 1, 3), nrow = 2, ncol = 2,byrow = TRUE) 
layout_mat

#2x2 panels with the second plot 
#first plot in lower left 
#second plot in upper left 
#third plot in bottom right and the top right panel is empty 

#not we can change the aspect ratio of 
my_lay <- layout(mat = layout_mat, 
                 heights = c(1, 3),  #first row is one unit tall , second row is 3 units tall 
                 widths = c(3, 1), respect =TRUE)  #first column is 3 units wide, second column is 1 unit wide 
layout.show(my_lay)

par(mar = c(4, 4, 0, 0)) # expands margins when text gets cut off

plot(flowers$weight, flowers$shootarea, xlab = "weight (g)", ylab = "shoot area (cm2)")

par(mar = c(0, 4, 0, 0)) # expands margins when text gets cut off

boxplot(flowers$weight, horizontal = TRUE, frame = FALSE, axes =FALSE)

par(mar = c(4, 0, 0, 0)) # expands margins when text gets cut off

boxplot(flowers$shootarea, frame = FALSE, axes = FALSE)
```

## export base plots

```{r}
pdf(file = "my_plot.pdf")
plot(flowers$weight, flowers$shootarea)
dev.off()

png("my_plot.png")
plot(flowers$weight, flowers$shootarea)
dev.off()
```

# Module 12

### Histograms

```{r,eval=FALSE}
#remember histogram by default will calculate the count itself so you would pass the raw data for it to process

ggplot(data = diamonds, aes(x = carat)) + geom_histogram(bins=100)

ggplot(data = diamonds, aes(x = carat)) + geom_histogram(binwidth=0.001)


#geom_histogram produces the following variables from inputed data

    #count, the number of observations in each bin
    #density, the density of observations in each bin (percentage of total/bar width)
    #x, the center of the bin

#to map these produced variables to aesthetic dimension surround the variable with ..var.. as such 

ggplot(diamonds, aes(carat)) + geom_histogram(aes(y = ..density..), binwidth = 0.1) 

```

### bar plots

```{r,eval=FALSE}
mpg
s <- ggplot(mpg, aes(fl, fill = drv)) #count of fl stratified by category of drv 

s + geom_bar()

s + geom_bar(position = "stack") # counts the total of fl levels, and just shows proportion of each stratification wihtin that totall 

s + geom_bar(position = "dodge") #counts each stratification individually

s + geom_bar(position = "fill") #purely demon straights the proportion of the statification categoires within fl
```

### **box and whiskers**

```{r}
library(ggplot2)
data(iris)
ggplot(data = iris, aes(x = Species, y = Petal.Length, fill = Species)) + 
  geom_boxplot(position = "dodge") + 
  labs(y = "Petal Length", title = "Box Plot") 
```

### **violin plot**

```{r}
ggplot(data = iris, aes(x = Species, y = Petal.Length, fill = Species)) + 
  geom_violin(position = "dodge") + 
  labs(y = "Petal Length", title = "Violin Plot")
```

### **violin plot with box and whiskers inside it**

```{r}
ggplot(data = iris, aes(x = Species, y = Petal.Length, fill = Species)) + 
  geom_violin(trim = FALSE, position = "dodge") + 
  geom_boxplot(width = 0.20, position = position_dodge(width = 20), fill = "orange") +
  labs(y = "Petal Length", title = "Violin/Box") 
```

### **Ridge line**

```{r}
library(ggridges)
iris$Species <- factor(iris$Species,levels=c("setosa","versicolor","virginica"))
ggplot(data = iris, aes(x = Petal.Length, y = Species, fill = Species)) + 
  geom_hline(yintercept = c(1, 2, 3), color = "black", size = .6) +  
  geom_density_ridges(scale = 1.75, rel_min_height = 0.01) + 
  labs(x = "Petal Length",y="frequency", title = "Ridgeline Plot of Petal Length by Species") + 
  theme_bw() + theme(plot.title = element_text(face = "bold", size = 12),
  axis.title = element_text(vjust = 1, hjust = 1), 
  legend.background = element_rect(fill = "white", linewidth = 2, colour = "white"),
  panel.grid.minor = element_blank()) + expand_limits(y = c(0.5, 3.75))
```

### **pie chart**

```{r}
iris$Species <- factor(iris$Species,levels=c("versicolor","virginica","setosa"))
species_counts <- as.data.frame(table(iris$Species))
colnames(species_counts) <- c("Species", "Count")
ggplot(species_counts, aes(x = "", y = -Count, fill = Species)) +  
  geom_bar(stat = "identity", width = 1, color = "black") +  
  coord_polar("y") + theme_void()
```

## faceting

new plot by row for each color

```{r}
ggplot(data = diamonds, aes(x = carat)) + geom_histogram(bins=30) + facet_grid(rows = vars(color)) 
```

new plot by column for each color

```{r}
ggplot(data = diamonds, aes(x = carat)) + geom_histogram(bins=30) + facet_grid(cols = vars(color))
```

stratify by further variables, one category by row and other by columns

```{r}
ggplot(data = diamonds, aes(x = carat)) + geom_histogram(bins=30) + facet_grid(rows = vars(cut),cols = vars(color)) 
```

OPTION SCALES = FREE will make the scales fit the data of all facets, and all facets will have the same scales

#FOR FACET GRID, SCALES = FREE will completely decouple the facets scales from each other and each scale will fit only that individual facet.

```{r}
facet_grid(cols = vars(Species), scales="free")
```

**facet_wrap**

wrap will just make a new plot for each category and will wrap them like text to save space, it's just a list there isn't a dimensionality

it automatically printed them in order of trend line least to greatest

```{r}
library(nlme)
data(Oxboys)
ggplot(Oxboys, aes(age, height)) + geom_line() + facet_wrap(vars(Subject))
```

## grouping

new plot for each group within the same panel

```{r}
library(nlme)

# boys heights by age, plotted for each individual boy, otherwise the lines would connect different boys heights 
ggplot(Oxboys, aes(age, height, group = Subject)) + geom_line() 

ggplot(Oxboys, aes(age, height, group = Subject)) + geom_smooth() 
```

## customization

```{r}

library(ggplot2)
base <- ggplot(mpg, aes(cty, hwy, color = factor(cyl))) +
  geom_jitter() + 
  geom_abline(color = "grey50", linewidth = 2)

base


labelled <- base +
  labs(x = "City mileage/gallon",
       y = "Highway mileage/gallon",
       color = "Cylinders",
       title = "Highway and city mileage are highly correlated") +
       scale_color_brewer(type = "seq", palette = "Spectral")

labelled


styled <- labelled +
  theme_bw() + 
  theme(plot.title = element_text(face = "bold", size = 12),
    legend.background = element_rect(fill = "white", linewidth = 4, color = "white"),
    legend.justification = c(0, 1),
    legend.position = c(0, 1),
    axis.ticks = element_line(color = "grey70", linewidth = 0.2),
    panel.grid.major = element_line(color = "grey70", linewidth = 0.2),
    panel.grid.minor = element_blank())

```

# Module 13

## select() - selecting variables (columns)

```{r}
library(nycflights13)
library(tidyverse)
names(flights)
#you can subset variables like this
select(flights,year,month,day)

#you can selected ranges of variables (columns) without using the column number 
select(flights,year:day)


#and just do this for the compliment
select(flights,!(year:day))


```

selection functions for subsetting exactly what you want

```{r, eval=FALSE}

#starts_with(): Starts with a prefix.

#ends_with(): Ends with a suffix.

#contains(): Contains a literal string.

#matches(): Matches a regular expression.

#num_range(): Matches a numerical range like x01, x02, x03.

```

examples of how to use these selection functions

```{r}
select(flights, starts_with("dep"))

select(flights, ends_with("time"))

select(flights, contains("time"))

# this is or 
select(flights, starts_with("dep"), contains("time"), year:day)

# this is just the column numbers 
select(flights, c(1,3,5))

#select columns that are numbered but also have a prefix 
select(billboard, num_range("wk", 20:23))


```

## filter() - selecting observations of variables (rows)

```{r}
# use
# & and 
# | or 
# ! not 
```

```{r}
#filter flights departed on january 1st 
#all rows (observations) that satisfy the criteria
filter(flights, month == 1 & day == 1)

#flights that depart on November or December 
filter(flights, month == 11 | month == 12)

# all flights where the destination is IAH 
filter(flights, dest == "IAH")

# all flights with arrival delay less than or equal to 2 hours 
# or departure delay greater than 2 hours 

#only one condition needs to be satisfied to be included
filter(flights, arr_delay <= 120 | dep_delay > 120)
```

## mutate() - apply a transformation to a variable or make new one

```{r}
# from left to right future variables can use previous ones within the same statement 
mutate(flights, gain = arr_delay - dep_delay, hours = air_time/60, gain_per_hour = gain/hours)

```

### transmute()

```{r}
# same thing as filter but all variables other than the new ones just created will be removed
transmute(flights, gain = arr_delay - dep_delay, hours = air_time/60, gain_per_hour = gain/hours)

```

### variants of mutate() and transmute()

```{r}
#mutate_all() and transmute_all() affect every variable

#mutate_at() and transmute_at() affect variables selected with a character vector

#mutate_if() and transmute_if() affect variables selected with a predicate function (a function that returns TRUE or FALSE)
```

```{r}
head(iris,5)
#take the log() of all columns that include "Sepal" in their name and then assaign that to new tibble
mutated <- mutate_at(iris, vars(contains("Sepal")), log)

#you can specify to apply the function only to columns of specified data type 
mutated <- mutate_if(iris, is.numeric, log)
head(mutated)

#transmute would do the exact same thing just removal all other columns other than the ones that were transformed


#across() will apply a function to multiple different selection criteria 

```

## arrange() - reorder rows

```{r}
#reorder rows by numeric aescending value by default 
arrange(flights, year, month, day)

#or descending order
arrange(flights, desc(month))
```

## summarize() and group_by()

```{r}
#this will output a tibble with delay_mean which is the mean of the variable 
# dep_delay aswell output in the tibble delay_sd the standard dev of dep_delay
flights 

summarize(flights, delay_mean = mean(dep_delay, na.rm = TRUE), delay_sd = sd(dep_delay, na.rm = TRUE))
```

group_by() and summarize() together

```{r}
# will produce a group for each unique combination of the variables
by_day <- group_by(flights, year, month, day)

# takes that tibble of groups and evaluates the mean of variable dep_delay within each group 
summarize(by_day, delay = mean(dep_delay, na.rm = TRUE))

```

summarize functions

```{r}
# mean(), median(), sum(), min(), max(), sd(), var(), ... 
```

## pipe-line operator %\>%

```{r}
flights %>% 
  group_by(dest) %>% 
    summarize(count = n(), dist = mean(distance, na.rm = TRUE), delay = mean(arr_delay, na.rm = TRUE)) %>%
      filter(count > 20, dest != "HNL") %>% 
        ggplot(mapping = aes(x = dist, y = delay)) + 
          geom_point(aes(size = count), alpha = 1/3) + 
            geom_smooth(se = FALSE)
```

## more pipe-lines %\>%

```{r}
iris #remember you need to declare it as a tibble 
as_tibble(iris)

iris_grouped <- group_by(iris, Species) 
summarise_all(iris_grouped, mean)

#or you can just 

iris %>%
  group_by(Species) %>%
    summarise_all(mean)
    
#will output a summary of the observation groups where the summary is the mean of all columns within each group that start with Sepal in the variable name 
iris %>% 
  group_by(Species) %>%
    summarise(across(starts_with("Sepal"), mean))

# if you want the output from this for input to the next function in the pipeline to not be subject to previous grouping constraints for whatever reason. 
iris %>%
  group_by(Species) %>%
    summarise(across(starts_with("Sepal"), mean), .groups = "drop")


```

```{r,eval=false eval=FALSE, include=FALSE, r,eval=false}

library(candisc)

# convert data frame to tibble 
HSB <- as_tibble(HSB)
HSB

# summary of data types 
str(HSB)

# the function n() will give you the number of observations of each of the groups passed from the previous function in the pipe-line
HSB %>%
  group_by(race) %>%
    summarise(n=n())

# count() will tell you how many observations there are of each of the possible unique values within the variable
HSB %>% 
  count(race)

# how many observations of each unique combination of variable values 
HSB %>%
  count(race, gender)

```

example of filtering and then summarizing

```{r eval=FALSE, include=FALSE}
#means of all of these variables for all students
HSB %>% 
  summarise(readm = mean(read), writem = mean(write), mathm = mean(math), ssm = mean(ss))

# for each gender 
HSB %>% 
  group_by(gender) %>%
    summarise(readm = mean(read), writem = mean(write), mathm = mean(math), ssm = mean(ss))

# for each gender and race 
HSB %>% 
  group_by(gender,race) %>%
    summarise(readm = mean(read), writem = mean(write), mathm = mean(math), ssm = mean(ss))

# for each gender evaluate the mean of all numeric variables
HSB %>% 
  group_by(gender) %>%
    summarise(across(where(is.numeric), mean, na.rm = TRUE))


# the means of all numeric variables by ses for only asian and hispanic
HSB %>%
  filter(race == "hispanic" | race == "asian") %>%
    group_by(ses) %>%
      summarise(across(where(is.numeric), mean, na.rm = TRUE))

```

```{r}
library(dplyr)
data(starwars)
glimpse(starwars) # similar to str() in Base R

#a. How many humans are contained in starwars overall? (Hint. use count())
starwars %>%
  filter(species == "Human") %>%
    summarise(observations = n())

#b. How many feminine humans are contained in starwars?
starwars %>%
  filter(species == "Human" & gender == "feminine") %>%
    summarise(observations = n())


#c. From which homeworld do the most individuals come from?
starwars %>%
  group_by(homeworld) %>%
    summarise(observations = n()) %>%
      filter(observations == max(observations))

  
#d. What is the mean height of all individuals with orange eyes from the most popular homeworld?
starwars %>%
  filter(eye_color == "orange" & homeworld == "Naboo") %>%
    summarise(mean_height = mean(height))

```

## tibble joins

```{r}
library(tidyverse)
# tribble() creates a tibble
x <- tribble(
  ~key, ~val_x,
     1, "x1",
     2, "x2",
     3, "x3"
)

y <- tribble(
  ~key, ~val_y,
     1, "y1",
     2, "y2",
     4, "y3"
)
```

```{r}

  #  inner_join() keeps observations that appear in both data frames
  #  left_join() keeps all observations in x
  #  right_join() keeps all observations in y
  #  full_join() keeps all observations in x and y

```

inner join - retains all variables which correspond to matching keys of both sets

```{r}
inner_join_1<- x %>% 
  inner_join(y, by = "key")

inner_join_1

#equivalently 
inner_join_2 <- inner_join(x, y, by = "key")
```

left join - retains all variables which correspond to the keys of x

```{r}
left_join <- x %>% 
  left_join(y, by = "key")
left_join

#equivalently 
left_join(x, y, by = "key")
```

right join - retains all variables which correspond to the keys of y

```{r}
right_join <- x %>%
  right_join(y,by = "key")
right_join

#equivalently
right_join(x, y, by = "key")
```

full join - retains all variables which correspond to all keys present in both sets

```{r}
full_join <- x %>%
   full_join(y,by="key")
full_join

#equivalently
full_join(x, y, by = "key")
```

## tibble reshape and separate()

```{r}

 #   pivot_longer() lengthens data, increasing the number of rows and decreasing the #number of columns (i.e., turning columns into rows)
    
    
 #   pivot_wider() widens data, increasing the number of columns and decreasing the #number of rows (i.e., turning rows into columns)
    
    
 #   separate() separates a character column into multiple columns with a regular #expression or numeric locations
# as in a column containing ab-01 becomes two columns ab   01 
   
    
  #   unite() unites multiple columns into one by pasting strings together
      # the reverse of the previous 

```

```{r}
iris <- as_tibble(iris)

iris

# wide to long 
iris %>%
     pivot_longer(cols = -Species, names_to = "Measurements", values_to = "Values")


# seperate 
iris %>%
     pivot_longer(cols = -Species, names_to = "Measurements", values_to = "Values") %>%
          separate(col = Measurements, into = c("Part", "Measure")) #it registered the . as the seperator

# now that you've organized the data this way you can easily produce this complex plot 
iris %>%
  pivot_longer(cols = -Species, names_to = "Measures", values_to = "Values") %>%
  separate(col = Measures, into = c("Part", "Measure")) %>%
  ggplot(aes(x = Species, y = Values, color = Part)) + 
    geom_jitter() + 
    facet_grid(cols = vars(Measure)) + 
    theme_minimal()

```

wide to long

```{r}
table <- as_tibble(table4a)
table

long <- table %>%
  pivot_longer(cols=c("1999","2000"), names_to = "Year", values_to = "cases" )

long
```

```{r}
billboard

long <- billboard %>%
  pivot_longer(cols = wk1:wk76, names_to = "Week", values_to = "Rank", values_drop_na = TRUE)
long
```

long to wide

```{r}
table2 <- as_tibble(table2)

table2

#pivot_wider(data, names_from, values_from)

  #  names_from = A string specifying the name of the column to get the name of the output column
  #  values_from = A string specifying the name of the column to get the cell values from


wide <- table2 %>%
  pivot_wider(names_from = c("type"),values_from = "count")

wide
```

separate

```{r}
table3

#by default any non alphanumeric char will be registered as the seperator

table3 %>%
  separate(rate, into = c("cases","population"))

# or explicitly sepcify the seperator
table3 %>%  separate(rate, into = c("cases", "population"), sep = "/")

#convert from char to the data type it's best suited 
table3 %>%   
  separate(rate, into = c("cases", "population"), convert = TRUE)

# or you can specify by location, posative is left to right and sep = -2 is right to left 
table3 %>%   
  separate(year, into = c("century", "year"), sep = 2)

```

unite

```{r}
table5

table5 %>%
  unite(combined_column, century, year) # wil use _ by default 


table5 %>%
  unite(combined_column, century, year, sep = "") #specify none
```

## module 13 homework

```{r}
#setwd("C:/Users/jake pc/Desktop/Personal_save/Stat_405_Final_Exam")
setwd("/Users/jacobrichards/Desktop/Personal_save/Stat_405_Final_Exam")
bike <- read.csv(file="Bike_Lanes.csv",header=TRUE)
road <- read.csv(file="roads.csv",header=TRUE)
crash <- read.csv(file="crashes.csv",header=TRUE)
wide <- read.csv(file="Bike_Lanes_Wide.csv",header=TRUE)
```

Reshape wide using pivot_longer. Call this data long. Make the key lanetype, and the value\
the_length. Make sure we gather all columns but name, using -name. Note the NAs here

```{r}
wide

long <- wide %>%
    pivot_longer(cols = -name, names_to = "lanytype", values_to = "the_length",values_drop_na = TRUE)
long
```

## str_replace()

Replace (using str_replace) any hyphens (-) with a space in crash\$Road. Call this data crash2.\
Table the Road variable.

```{r}
# reassaigns the Road variable with the output of str_replace
crash2 <- crash %>% mutate(Road = str_replace(Road, "-", " "))
table(crash2$Road)
```

Separate the Road column (using separate) into (type and number) in crash2.

Reassign this to\
crash2. Table crash2\$type. Then create a new variable calling it road_hyphen using the unite\
function. Unite the type and number columns using a hyphen (-) and then table road_hyphen.

```{r}
crash2 <- crash2 %>%
     separate(col=Road, into = c("type", "number"))

table(crash2$type)

crash2 <- crash2 %>%
          unite(road_hyphen, type, number, sep = "-")

table(crash2$road_hyphen)
```

Keep rows where the record is not missing type and not missing name and re-assign the output to bike.

```{r}
#in a tibble empty spaces you can index as (" ") 


bike <- bike %>%
          filter(name != " " & type != " ")

bike <- bike %>%
          filter(!is.na(name) & !is.na(type))

```

Summarize and group the data by grouping name and type (i.e, for each type within each name) and take the sum of the length (reassign the sum of the lengths to the length variable). Call this data set sub.

```{r}
sub <- bike %>%
  group_by(name,type) %>%
    summarise(length = sum(length)) #mutate cannot change dimensionality of the tibble, summarize will reduce 
                                    #the tibble to one row for each group 
sub
```

Reshape sub using pivot_wider. Spread the data where the key is type and we want the value in the new columns to be length - the bike lane length. Call this wide2. Look at the column names of wide2 - what are they? (they also have spaces).

```{r}
#pivot_wider(data, names_from, values_from)
#in her problems when she says "key" she means the variable, the thing we are measuring  
wide2 <- sub %>%
            pivot_wider(names_from = type, values_from = length)
```

Join data to retain only complete data, (using an inner_join) e.g., those observations with road lengths and districts. Merge without using by argument, then merge using by = "Road". call the output merged. How many observations are there?

```{r}
merged <- inner_join(road,crash)
merged
```

```{r}
full <- full_join(road,crash)
full
```

```{r}
left <- left_join(road,crash)
left
```

```{r}
right <- right_join(road,crash, by="Road") #in case there are more than 1 common variables but you want to use one as the key instead of the other. 
right
```

# Module 14

## t-tests, Wilcox (one sample)

one sample - two sided

```{r}
data(trees)
t.test(trees$Height, mu = 70)
```

one sample - one sided

```{r}
t.test(trees$Height, mu = 70,alternative = "greater") # sample is greater than mu
t.test(trees$Height, mu = 70,alternative = "less")  # sample is less than mu 
```

change the confidence level on the mean confidence interval

```{r}
t.test(trees$Height, mu = 70,conf.level = 0.99)
```

whenever normality cannot be assumed use wilcox rank sum test

the function call is exactly the same as for t-tests

```{r}
wilcox.test(trees$Height, mu = 70)
wilcox.test(trees$Height, mu = 70, alternative = "greater") # sample is greater than mu
wilcox.test(trees$Height, mu = 70, alternative = "less")  # sample is less than mu 
```

## hypothesis test normality assumption verification

```{r}
qqnorm(trees$Height)
qqline(trees$Height, lty = 2)
```

this looks a little suspicious, lets check the Shapiro test

```{r}
shapiro.test(trees$Height)
```

fail to reject the null hypothesis that it's normal.

## t-tests - Wilcox - 2 sample independent - Variance test

**DATA HAS TO BE IN LONG FORMAT - might have to clean data**

```{r}
setwd("/Users/jacobrichards/Desktop/Personal_save/Stat_405_Module_14/Lab_14.1")
test <- read.csv(file="hypothesis.csv",header=TRUE)

test$id <- seq(1:nrow(test))
test
library(reshape2)
long_data <- melt(data = test, id.vars = c("id"),
                   measured.vars = c("Aspirin", "Tylenol"),
                   variable.name = "Brand", value.name = "Time")
long_data
t.test(long_data$Time ~ long_data$Brand)
```

H0: the two samples come from distributions with the same mean

Example:

For example, a study was conducted to test whether ‘seeding’ clouds with dimethylsulphate alters the moisture content of clouds. Ten random clouds were ‘seeded’ with a further ten ‘unseeded.’ The dataset can be found in the `atmosphere.txt` data file located in Canvas.

```{r}
#setwd(file="~/Desktop/Personal_save/Stat_405_Final_Exam")
atmos <- read.table(file="atmosphere.txt",header=TRUE)
atmos
t.test(atmos$moisture ~ atmos$treatment)
```

if you happen to be sure that the variances of the samples are equal

```{r}
t.test(atmos$moisture ~ atmos$treatment, var.equal = TRUE) #so in this context 
```

to verify assumption that variances are unequal - cannot use this with data that is not normal.

```{r}
var.test(atmos$moisture ~ atmos$treatment)
#H0: the varainces are equal, p-value = 0.4282 --> fail to reject null --> the variances are equal 
```

just the same you can use Wilcox for 2 sample tests for data you cant assume normality on

```{r}
wilcox.test(atmos$moisture ~ atmos$treatment)
```

## 2 sample independent t-test and Wilcox upper tailed

3.) Two friends play a computer game and each of them repeats the same level 10 times.

this is an **independent** two-sample t-test because the thing we are measuring and testing for difference of mean is two different people. The experimental unit is distinct.

```{r}
setwd("~/Desktop/Personal_save/Stat_405_Module_14/Module_14_Homework")
scores <- read.table(file="scores.txt",header=TRUE)

library(tidyverse)
long <- scores %>%
            pivot_longer(cols = X1:X10, names_to = "trials", values_to = "scores") %>%
                select(-trials)

long$ID <- factor(long$ID, levels = c("Player1","Player2"), labels = c("Player1","Player2"))
```

two sided t-test, unpaired, variance not equal

check normality and variance beforehand

```{r}
scores <- t(as.matrix(scores))
colnames(scores) <- scores[1,]
scores <- as_tibble(scores[-1,])
library(dplyr)

scores <- scores %>%
  mutate(across(everything(), as.numeric))


shapiro.test(scores$Player1)
shapiro.test(scores$Player2)

t.test(long$scores ~ long$ID, var.equal=FALSE)
```

Fail to reject null —\> players means are no different.

Upper tailed Wilcox

```{r}
wilcox.test(long$scores ~ long$ID, alternative = "greater")
```

Reject null —\> Player 1 is better than Player 2

## t-test Wilcox paired

**DATA HAS TO BE IN WIDE FORMAT**

Paired data are where there are two measurements on the same experimental unit (individual, site, etc.) For example, the `pollution` dataset gives the biodiversity score of aquatic invertebrates collected using kick samples in 17 different rivers. These data are paired because two samples were taken on each river, one upstream of a paper mill and one downstream. Each individual observation pair originated from the same place.

example:

```{r}
library(tidyverse)
pollution <- read.table('pollution.txt', header = TRUE)
pollution <- as_tibble(pollution)
pollution

t.test(pollution$down, pollution$up, paired = TRUE)

wilcox.test(pollution$down, pollution$up, paired = TRUE)
```

example:

" is there a difference in the test scores " —\> two-sided

```{r}
setwd("~/Desktop/Personal_save/Stat_405_Module_14/Module_14_Homework")
HW_6 <- read.csv(file="Homework_6.csv",header=TRUE)
HW_6

t.test(HW_6$Test.1, HW_6$Test.2, paired = TRUE)
```

Was there improvement over the course of the semester. H0: Test 2 - Test 1 \> 0

```{r}
t.test(HW_6$Test.2, HW_6$Test.1, paired = TRUE, alternative = "greater")
```

## Proportion test

advertising campaign counts of those would and would not buy are taken before and after - this is a proportions test - they applied a treatment

if they had ran a campaign, then sampled people and tabulated who did and did not by and of which who did or did not see the advertisement, then it would be a contingency table (odds ratio)

Are the proportions significantly different? H0: P1 - P2 = 0

+:-----------------------------:+:-------------------------:+:---------------------:+
|                               | before = **no treatment** | after = **treatment** |
+-------------------------------+---------------------------+-----------------------+
| would buy = **+ outcome**     | 45                        | 71                    |
+-------------------------------+---------------------------+-----------------------+
| would not buy = **- outcome** | 35                        | 32                    |
+-------------------------------+---------------------------+-----------------------+

```{r}
buy <- c(45,71)                     # creates a vector of positive outcomes
total <-c((45 + 35), (71 + 32))   # creates a vector of total numbers
prop.test(buy, total)   
```

There is no evidence to support that the advertising campaign has changed cat owners' opinions of cat food (*p* = 0.107)

## Chi-square

Test whether the number of cat owners buying the cat food is independent of the advertising campaign.

same thing but perform a chi-square test for independence rather than difference of proportion, in this context those are the same thing

```{r}
buyers <- matrix(c(45, 35, 71, 32), nrow = 2)
chisq.test(buyers)
```

you can also use the chisq.test function on raw un-tabulated data

```{r}
data <- read.csv(file="homework_5_data.csv",header=TRUE)
data <- as_tibble(data)
data <- table(data)
data
chisq.test(data)
```

Fail to reject the null hypothesis that the variables are independent.

## Contingency Table (odds ratio)

**set continuity correct = false** (correct = FALSE)

A random sample of 90 adults is classified according to gender and the number of hours of television watched during a week:

They did not apply any treatment —\> contingency table (odds ratio)

Use a 0.01 level of significance and test the hypothesis that the time spent watching television is independent of whether the viewer is male or female.

```{r}
table <- matrix(data=c(15,29,27,19),nrow=2,ncol=2,byrow=TRUE,dimnames = list(c("Over 25 hours", "Under 25 hours"),c("Male", "Female")))
table <- t(table)
table
chisq.test(table, correct = FALSE)
```

The p-value obtained is 0.01934 which is greater than 0.01, we therefore fail to reject the null hypothesis that time spent watching television is independent of whether the viewer is male or female.

## correlation coefficients and plots

Pearson’s product-moment correlation coefficient between two continuous variables is the default correlation coefficient in the function

these values are only meaningful under the assumption of linear relationships

```{r}
cor(trees$Height, trees$Volume) #select specific variables 

cor(trees) #or produce a matrix of the coef's 

cor(trees, use = "complete.obs") #if theres is an NA 

setwd("~/Desktop/Personal_save/Stat_405_Module_14/Lab_14.2")
times <- read.csv(file="Game_Times.csv",header=TRUE)

# when you need to plot it 
pairs(times[,c(4,5,6,11)])
pairs(times[,c(7,8,9,11)])
pairs(times[,c(10,12,13,11)])
pairs(times[,c(14,11)])

cor(times[,4:14]) #or can just do this and then check that there is actually a linear relationship between the variables
```

### p-value of correlation coefficient

to test if these correlation coefficients are statistically significant you need to use.

```{r}
cor.test(trees$Height, trees$Volume)
```

non parametric options

```{r}
cor.test(trees$Height, trees$Volume, method = "spearman")
cor.test(trees$Height, trees$Volume, method = "kendall")
```

## Linear Models

```{r eval=FALSE, include=FALSE}

Bivariate regression	    Y ~ X (continuous)	                lm(Y ~ X) #regular linear regression

One-way ANOVA	            Y ~ X (categorical)	                lm(Y ~ X) #difference between group means 

Two-way ANOVA	            Y ~ X1 (cat) + X2(cat)            	lm(Y ~ X1 + X2) 

ANCOVA	                  Y ~ X1 (cat) + X2(cont)           	lm(Y ~ X1 + X2)

Multiple regression     	Y ~ X1 (cont) + X2(cont)          	lm(Y ~ X1 + X2)

Factorial ANOVA	          Y ~ X1 (cat) * X2(cat)	            lm(Y ~ X1 * X2) or lm(Y ~ X1 + X2 + X1:X2)

```

## linear regression

produce model

```{r}
setwd("~/Desktop/Personal_save/Stat_405_Module_14/Module_14_Homework")
smoke <- read.table(file="smoking.txt",header=TRUE) 
smoke_lm <- lm(mortality ~ smoking, data=smoke)
summary(smoke_lm)
```

plot with data and correlation coefficient

```{r}
slope_intercept <- coefficients(smoke_lm)
plot(mortality ~ smoking,data=smoke)
abline(a=slope_intercept[1],slope_intercept[2])
cor(y=smoke$mortality, x=smoke$smoking)
```

summary of model

```{r}
summary(smoke_lm) #check for significance of coeficient's 
anova(smoke_lm) #this is a condensed form of an anova table, signifigant p-value of F statistic - model is signifigant
```

### prediction and confidence interval

```{r}
predicted_smoke_120 <- predict(smoke_lm, newdata = data.frame(smoking = 120))
predicted_smoke_120

CI_predicted_smoke_120 <- predict(smoke_lm, newdata = data.frame(smoking = 120), interval = "confidence", level = 0.98)
CI_predicted_smoke_120
```

### Verify Model

**Plot the model**

```{r}
ggplot(mapping = aes(x = smoking, y = mortality), data = smoke) + geom_point() + geom_smooth(method = "lm", se = TRUE)
```

**1.) residuals by the response values produced by the model (fitted values)**

```{r}
smoke_res <- resid(smoke_lm) 
smoke_fit <- fitted(smoke_lm) # this will extract the response values produced by the model

ggplot(mapping = aes(x = smoke_fit, y = smoke_res)) +
    geom_point() +
    geom_hline(yintercept = 0, colour = "red", linetype = "dashed")
```

**2.) residuals by known predictor values**

```{r}
ggplot(mapping = aes(x = smoke$smoking, y = smoke_res)) +
    geom_point() +
    geom_hline(yintercept = 0, colour = "red", linetype = "dashed")
```

No patterns, looks randomly and and evenly distributed. **check**

**3.) normality of residuals**

```{r}
qqnorm(smoke_res)
qqline(smoke_res) 
```

This actually does look kind of suspicious.

**4.) Studentized Residuals vs. Predictor observation number**

```{r}
plot(rstudent(smoke_lm) ~ seq(1,length(smoke$smoking),by=1))
```

more diagnostic plots

```{r}
par(mfrow = c(2,2))
plot(smoke_lm)
```

the first 2 are same as before

bottom left is first one but by a different scale

bottom right plots the points which have the greatest impact on the linear models coefficients

We may see that there is an extreme outlier point that weakens out model for the normal range of values that we would like to predict, it would benefit our model to just remove the point from the data points we produce the model from

```{r}
library(ggfortify)
autoplot(smoke_lm, which = 1:6, ncol = 2, label.size = 3)
```

Cook's distance is how much influence that point has on the coefficients of the model.

we notice that point 2 has more than double the influence than any other point, maybe it's and outlier and the the model would be more representative of the relationship of the variables without it.

### Update model from verification plots

so this one is probably hurting the model because it's far outside the data center (less accurate) and has high impact

```{r}
smoke_lm2 <- update(smoke_lm, subset = -2)
summary(smoke_lm2)
slope_intercept_2 <- coefficients(smoke_lm2)
plot(mortality ~ smoking, data=smoke)
abline(a=slope_intercept_2[1],b=slope_intercept_2[2],col="red")

slope_intercept_1 <- coefficients(smoke_lm)
abline(a=slope_intercept_1[1],b=slope_intercept_1[2],col="blue")


#R^2 before 0.4918 ---> after: 0.5622
```

### diagnostic plot for how much a point effects the model relative to it's error

```{r}
matplot(dfbetas(smoke_lm), type = "l", col = "black")

lines(sqrt(cooks.distance(smoke_lm)), lwd = 2)

axis(1, at = 1:nrow(dfbetas(smoke_lm)), labels = 1:nrow(dfbetas(smoke_lm)))
```

dfbetas() gives the change in the estimated parameters if an observation is excluded, relative to its standard error (intercept is the solid line, and slope is the dashed line in the example below). The solid bold line in the same graph represents the Cook’s distance. Examples of how to use these functions are given below.

## anova

linear model with categorical variable as predictor will compute anova test

Example: test is significant —\> reject null, the groups are different

```{r}
# this will evaluate an anova test 
smoke_anova_lm <- lm(mortality ~ risk.group, data=smoke)

# this will print the anova table of the test 
anova(smoke_anova_lm)
```

reject the null hypothesis that the group means are equal.

this will give us more information on the results of this anova test.

```{r}
summary(smoke_anova_lm)
```

```{r eval=FALSE, include=FALSE}
Coefficients:
  
  Coefficients:
                 Estimate Std. Error t value Pr(>|t|)    
(Intercept)        135.00       5.25  25.713  < 2e-16 *** #reject null --> mean of high group != zero
risk.grouplow      -57.83       8.02  -7.211 3.16e-07 *** #reject null --> mean of low != high group
risk.groupmedium   -27.55       6.90  -3.992 0.000615 *** #reject null --> mean of medium != high group
  
  
  
                 Estimate Std. Error t value Pr(>|t|)    
(Intercept)        135.00       5.25  25.713  < 2e-16 *** #this is the first level of the factor, in this case risk.grouphigh (alpahebtical)
  # estimated mean mortality index for risk group high is 135,
  
   < 2e-16 *** #this is testing the null hypotheis that the mean mortality index of the high risk group is equal to zero. --> reject h0, not zero
  
  
risk.grouplow      -57.83       8.02  -7.211 3.16e-07 ***
# these estimate values are the estimated mean differences of it and the high risk group (whaterver one gets assaigned to the intercept) 
  # as in risk.grouplow - risk.grouphigh = -57.83 ---> 135 - 57.83 = 77.17  
  
  3.16e-07 *** # this is testing  H0: abs(risk.grouplow - risk.grouphigh) = 0 , 2 sided --> reject h0: they are different
  
  
risk.groupmedium   -27.55       6.90  -3.992 0.000615 *** # risk.groupmedium - risk.grouphigh = -27.55 --> 135 - 27.55 = 107.45
  
  0.000615 *** # this is testing  H0: abs(risk.groupmedium - risk.grouphigh) = 0 , 2 sided --> reject h0: they are different
  

```

Example where the test is not significant

```{r, warning=FALSE}
setwd("~/Desktop/Personal_save/Stat_405_Module_14/Module_14_Homework")
plasma <- read.csv(file="plasma.csv",header=TRUE)
plasma$time <- factor(plasma$time,levels=c("8am", "11am", "2pm", "5pm", "8pm"), 
                      labels = c("8am", "11am", "2pm", "5pm", "8pm"))
plasma_model <- lm(plasma ~ time, data = plasma)

```

```{r}
anova(plasma_model)
```

Pr(\>F) = 0.1132 —\> fail to reject the null hypothesis that the group means are NOT different from each other.

```{r}
summary(plasma_model)
```

```         
(Intercept)  118.500      5.944  19.935   <2e-16 *** ---> reject null, 8AM group mean does NOT equal zero. 
```

```         
time11am       9.400      8.407   1.118    0.269 ---> fail to reject null --> conclude that 11 AM group mean is not different than 8AM
```

```         
time2pm        1.800      8.407   0.214    0.831 ---> fail to reject null --> conclude that 2 pm group mean is not different than 8AM
```

```         
time5pm      -13.700      8.407  -1.630    0.110 ---> fail to reject null --> conclude that 5 pm group mean is not different than 8AM
```

```         
time8pm        1.200      8.407   0.143    0.887 ---> fail to reject null --> conclude that 8 pm group mean is not different than 8AM
```
